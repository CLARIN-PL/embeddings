{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEPISZCZE\n",
    "\n",
    "> The use cases and examples how to train and submit models to the [LEPISZCZE](https://lepiszcze.ml/). \n",
    "\n",
    "- bibliography: references.bib\n",
    "- title-block-banner: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp lepiszcze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48643/2764131430.py:4: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { max-width:1800px !important;width:auto; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { max-width:1800px !important;width:auto; }</style>\"))\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We recommend to read our NeurIPS paper [@augustyniak2022this] where you can find our lessons learned from the process of designing and compiling LEPISZCZE benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export \n",
    "from pathlib import Path\n",
    "\n",
    "from embeddings.config.lightning_config import LightningBasicConfig, LightningAdvancedConfig\n",
    "from embeddings.pipeline.lightning_classification import LightningClassificationPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with training a text classifier using `embeddings.pipeline.lightning_classification.LightningClassificationPipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr/>\n",
       "<h3>LightningClassificationPipeline</h3>\n",
       "<blockquote><pre><code>LightningClassificationPipeline(embedding_name_or_path:Union[str,pathlib.Path], dataset_name_or_path:Union[str,pathlib.Path], input_column_name:Union[str,Sequence[str]], target_column_name:str, output_path:Union[str,pathlib.Path], evaluation_filename:str='evaluation.json', config:Union[embeddings.config.lightning_config.LightningBasicConfig,embeddings.config.lightning_config.LightningAdvancedConfig]=LightningBasicConfig(use_scheduler=True, optimizer='Adam', warmup_steps=100, learning_rate=0.0001, adam_epsilon=1e-08, weight_decay=0.0, finetune_last_n_layers=-1, classifier_dropout=None, max_seq_length=None, batch_size=32, max_epochs=None, early_stopping_monitor='val/Loss', early_stopping_mode='min', early_stopping_patience=3, tokenizer_kwargs={}, batch_encoding_kwargs={}, dataloader_kwargs={}), devices:Union[List[int],str,int,NoneType]='auto', accelerator:Union[str,pytorch_lightning.accelerators.accelerator.Accelerator,NoneType]='auto', logging_config:embeddings.utils.loggers.LightningLoggingConfig=LightningLoggingConfig(loggers_names=[], tracking_project_name=None, wandb_entity=None, wandb_logger_kwargs={}), tokenizer_name_or_path:Union[str,pathlib.Path,NoneType]=None, predict_subset:embeddings.data.dataset.LightingDataModuleSubset=<LightingDataModuleSubset.TEST: 'test'>, load_dataset_kwargs:Optional[Dict[str,Any]]=None, model_checkpoint_kwargs:Optional[Dict[str,Any]]=None)</code></pre></blockquote><p>Helper class that provides a standard way to create an ABC using\n",
       "inheritance.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc(LightningClassificationPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "LEPISZCZE_SUBMISSIONS = Path(\"../lepiszcze-submissions\")\n",
    "LEPISZCZE_SUBMISSIONS.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "config = LightningBasicConfig(\n",
    "    learning_rate=0.01, max_epochs=1, max_seq_length=128, finetune_last_n_layers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 4 required positional arguments: 'model_config_kwargs', 'early_stopping_kwargs', 'tokenizer_kwargs', and 'batch_encoding_kwargs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m advanced_config \u001b[39m=\u001b[39m LightningAdvancedConfig(\n\u001b[1;32m      2\u001b[0m     finetune_last_n_layers\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     task_train_kwargs\u001b[39m=\u001b[39;49m{\n\u001b[1;32m      4\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmax_epochs\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m1\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mdevices\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39maccelerator\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mdeterministic\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      8\u001b[0m     },\n\u001b[1;32m      9\u001b[0m     task_model_kwargs\u001b[39m=\u001b[39;49m{\n\u001b[1;32m     10\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mlearning_rate\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m5e-4\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39muse_scheduler\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     12\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39moptimizer\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mAdamW\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39madam_epsilon\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m1e-8\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mwarmup_steps\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m100\u001b[39;49m,\n\u001b[1;32m     15\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0.0\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m     },\n\u001b[1;32m     17\u001b[0m     datamodule_kwargs\u001b[39m=\u001b[39;49m{\n\u001b[1;32m     18\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mdownsample_train\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0.01\u001b[39;49m,\n\u001b[1;32m     19\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mdownsample_val\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0.01\u001b[39;49m,\n\u001b[1;32m     20\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mdownsample_test\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0.05\u001b[39;49m,\n\u001b[1;32m     21\u001b[0m     },\n\u001b[1;32m     22\u001b[0m     dataloader_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mnum_workers\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0\u001b[39;49m},\n\u001b[1;32m     23\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 4 required positional arguments: 'model_config_kwargs', 'early_stopping_kwargs', 'tokenizer_kwargs', and 'batch_encoding_kwargs'"
     ]
    }
   ],
   "source": [
    "advanced_config = LightningAdvancedConfig(\n",
    "    finetune_last_n_layers=0,\n",
    "    task_train_kwargs={\n",
    "        \"max_epochs\": 1,\n",
    "        \"devices\": \"auto\",\n",
    "        \"accelerator\": \"cpu\",\n",
    "        \"deterministic\": True,\n",
    "    },\n",
    "    task_model_kwargs={\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"use_scheduler\": False,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "    datamodule_kwargs={\n",
    "        \"downsample_train\": 0.01,\n",
    "        \"downsample_val\": 0.01,\n",
    "        \"downsample_test\": 0.05,\n",
    "    },\n",
    "    dataloader_kwargs={\"num_workers\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: polemo2-official/all_text\n",
      "Found cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n",
      "100%|██████████| 3/3 [00:00<00:00, 39.80it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-df7f6639fbb755c8.arrow\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-56499cf86dfad548.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-e46c214a0dfb2649.arrow\n",
      "Casting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-dc4c3193ad68f234.arrow\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "pipeline = LightningClassificationPipeline(\n",
    "    dataset_name_or_path=\"clarin-pl/polemo2-official\",\n",
    "    embedding_name_or_path=\"allegro/herbert-base-cased\",\n",
    "    input_column_name=\"text\",\n",
    "    target_column_name=\"target\",\n",
    "    output_path=\".\",\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took a couple of seconds but finally we have a pipeline objects ready and we need only run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.config.lightning_config import LightningAdvancedConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.sso.sso_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.sso.sso_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No config specified, defaulting to: polemo2-official/all_text\n",
      "Found cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n",
      "100%|██████████| 3/3 [00:00<00:00, 61.72it/s]\n",
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:849: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(strategy=\"dp\"|\"ddp\"|\"ddp2\")`. Setting `strategy=\"ddp_spawn\"` for you.\n",
      "  rank_zero_warn(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#| export\n",
    "results = pipeline.run()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "import typer\n",
    "\n",
    "from embeddings.defaults import RESULTS_PATH\n",
    "from embeddings.pipeline.flair_classification import FlairClassificationPipeline\n",
    "from embeddings.utils.utils import build_output_path, format_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \".\"\n",
    "embedding_name_or_path = \"clarin-pl/word2vec-kgr10\"\n",
    "dataset_name = \"clarin-pl/polemo2-official\"\n",
    "input_column_name = \"text\"\n",
    "target_column_name = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 22:35:55,493 - embeddings.utils.utils - WARNING - String 'clarin-pl/word2vec-kgr10' contains '/'. Replacing it with '__'. Cleaned_text: clarin-pl__word2vec-kgr10.\n",
      "2022-11-13 22:35:55,496 - embeddings.utils.utils - WARNING - String 'clarin-pl/polemo2-official' contains '/'. Replacing it with '__'. Cleaned_text: clarin-pl__polemo2-official.\n",
      "2022-11-13 22:35:56,661 - embeddings.embedding.auto_flair - INFO - clarin-pl/word2vec-kgr10 not compatible with Transformers, trying to initialise as static embedding.\n",
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/huggingface_hub/file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n",
      "Downloading: 100%|██████████| 76.0/76.0 [00:00<00:00, 39.7kB/s]\n",
      "Downloading: 100%|██████████| 72.0/72.0 [00:00<00:00, 42.1kB/s]\n",
      "Downloading: 100%|██████████| 139M/139M [01:01<00:00, 2.27MB/s] \n",
      "Downloading:  11%|█         | 289M/2.74G [02:10<18:57, 2.16MB/s]  "
     ]
    }
   ],
   "source": [
    "output_path = build_output_path(root, embedding_name_or_path, dataset_name)\n",
    "pipeline = FlairClassificationPipeline(\n",
    "    embedding_name=embedding_name_or_path,\n",
    "    dataset_name=dataset_name,\n",
    "    input_column_name=input_column_name,\n",
    "    target_column_name=target_column_name,\n",
    "    output_path=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('embeddings')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "86b40992624b6ecf125385760a49d2b554d653d5c84d942a6f4a5512888cc722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
