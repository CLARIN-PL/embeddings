{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc85c5f",
   "metadata": {},
   "source": [
    "# LM-based models inference\n",
    "\n",
    "> Inference for LM-based models\n",
    "\n",
    "- title-block-banner: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3ac2b5-06e8-46bc-a626-9384a35920e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1019b750-cebe-438b-b1ab-434d6f756864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "import os\n",
    "\n",
    "os.chdir(\"..\")\n",
    "from typing import Any, Dict\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from embeddings.config.lightning_config import LightningAdvancedConfig\n",
    "from embeddings.defaults import DATASET_PATH, RESULTS_PATH\n",
    "from embeddings.model.lightning_module.text_classification import (\n",
    "    TextClassificationModule,\n",
    ")\n",
    "from embeddings.pipeline.hf_preprocessing_pipeline import (\n",
    "    HuggingFacePreprocessingPipeline,\n",
    ")\n",
    "from embeddings.pipeline.lightning_classification import LightningClassificationPipeline\n",
    "from embeddings.task.lightning_task.text_classification import TextClassificationTask\n",
    "from embeddings.utils.utils import build_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0e06e2-3c5a-420b-b065-31d5ccd6b255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 23:05:26,246 - embeddings.utils.utils - WARNING - String 'hf-internal-testing/tiny-albert' contains '/'. Replacing it with '__'. Cleaned_text: hf-internal-testing__tiny-albert.\n",
      "2023-02-13 23:05:26,247 - embeddings.utils.utils - WARNING - String 'clarin-pl/polemo2-official' contains '/'. Replacing it with '__'. Cleaned_text: clarin-pl__polemo2-official.\n",
      "2023-02-13 23:05:26,254 - embeddings.utils.utils - WARNING - String 'hf-internal-testing/tiny-albert' contains '/'. Replacing it with '__'. Cleaned_text: hf-internal-testing__tiny-albert.\n",
      "2023-02-13 23:05:26,256 - embeddings.utils.utils - WARNING - String 'clarin-pl/polemo2-official' contains '/'. Replacing it with '__'. Cleaned_text: clarin-pl__polemo2-official.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "embedding_name_or_path = \"hf-internal-testing/tiny-albert\"\n",
    "dataset_name = \"clarin-pl/polemo2-official\"\n",
    "\n",
    "dataset_path = build_output_path(DATASET_PATH, embedding_name_or_path, dataset_name)\n",
    "output_path = build_output_path(RESULTS_PATH, embedding_name_or_path, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d0098c-41ec-473a-954a-709f7fb05922",
   "metadata": {},
   "source": [
    "### Preprocess and downsample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "095d1c88-900f-4275-a879-f9efdb73265a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e0c1ce6ddfd81769\n",
      "Found cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/default-e0c1ce6ddfd81769/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 817.23it/s]\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/default-e0c1ce6ddfd81769/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-a54edce9681df8b7.arrow and /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/default-e0c1ce6ddfd81769/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-09cf731207f31628.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/default-e0c1ce6ddfd81769/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-c48721732fabb729.arrow and /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/default-e0c1ce6ddfd81769/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-f9d782422a65c7e6.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/default-e0c1ce6ddfd81769/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-0db6321193feb3ec.arrow and /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/default-e0c1ce6ddfd81769/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-4e6c26839c3e4adf.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/default-e0c1ce6ddfd81769/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-e32b75da1d28bfd0.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/default-e0c1ce6ddfd81769/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-98cbedcc70a23855.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/default-e0c1ce6ddfd81769/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-b2cbb8ab856bac0f.arrow\n",
      "                                                                                         \r"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "def preprocess_data(path: str) -> Dict[str, Any]:\n",
    "    pipeline = HuggingFacePreprocessingPipeline(\n",
    "        dataset_name=dataset_name,\n",
    "        load_dataset_kwargs={\n",
    "            \"train_domains\": [\"hotels\", \"medicine\"],\n",
    "            \"dev_domains\": [\"hotels\", \"medicine\"],\n",
    "            \"test_domains\": [\"hotels\", \"medicine\"],\n",
    "            \"text_cfg\": \"text\",\n",
    "        },\n",
    "        persist_path=path,\n",
    "        sample_missing_splits=None,\n",
    "        ignore_test_subset=False,\n",
    "        downsample_splits=(0.01, 0.01, 0.05),\n",
    "        seed=441,\n",
    "    )\n",
    "    pipeline.run()\n",
    "\n",
    "    return {\n",
    "        \"dataset_name_or_path\": path,\n",
    "        \"input_column_name\": [\"text\"],\n",
    "        \"target_column_name\": \"target\",\n",
    "    }\n",
    "\n",
    "\n",
    "dataset_kwargs = preprocess_data(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159445cd-fb59-4964-aca2-ce9c18a8cf5e",
   "metadata": {},
   "source": [
    "### Train simple downsampled pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cb7ebd4-182c-4797-b5de-a7069313a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "config = LightningAdvancedConfig(\n",
    "    finetune_last_n_layers=0,\n",
    "    task_train_kwargs={\"max_epochs\": 1, \"deterministic\": True,},\n",
    "    task_model_kwargs={\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"train_batch_size\": 32,\n",
    "        \"eval_batch_size\": 32,\n",
    "        \"use_scheduler\": True,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "    datamodule_kwargs={\"max_seq_length\": 64,},\n",
    "    early_stopping_kwargs={\"monitor\": \"val/Loss\", \"mode\": \"min\", \"patience\": 3,},\n",
    "    tokenizer_kwargs={},\n",
    "    batch_encoding_kwargs={},\n",
    "    dataloader_kwargs={},\n",
    "    model_config_kwargs={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "148a0089-f461-4948-93fa-04f2e34ac9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.73ba/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 25.69ba/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 26.51ba/s]\n",
      "Casting the dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 113.84ba/s]\n",
      "Casting the dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 68.70ba/s]\n",
      "Casting the dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 103.15ba/s]\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-albert were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at hf-internal-testing/tiny-albert and are newly initialized: ['classifier.bias', 'albert.pooler.weight', 'albert.pooler.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1579: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name          | Type                            | Params\n",
      "------------------------------------------------------------------\n",
      "0 | model         | AlbertForSequenceClassification | 352 K \n",
      "1 | metrics       | MetricCollection                | 0     \n",
      "2 | train_metrics | MetricCollection                | 0     \n",
      "3 | val_metrics   | MetricCollection                | 0     \n",
      "4 | test_metrics  | MetricCollection                | 0     \n",
      "------------------------------------------------------------------\n",
      "132       Trainable params\n",
      "352 K     Non-trainable params\n",
      "352 K     Total params\n",
      "1.410     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:407: UserWarning: The number of training samples (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.07it/s, loss=1.39, v_num=, train/BaseLR=5e-6, train/LambdaLR=5e-6, val/MulticlassAccuracy=0.375, val/MulticlassPrecision=0.0938, val/MulticlassRecall=0.250, val/MulticlassF1Score=0.136]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.02s/it]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test/Loss': 1.3823001384735107,\n",
      " 'test/MulticlassAccuracy': 0.4054054021835327,\n",
      " 'test/MulticlassF1Score': 0.14423076808452606,\n",
      " 'test/MulticlassPrecision': 0.10135135054588318,\n",
      " 'test/MulticlassRecall': 0.25}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /app/resources/results/hf-internal-testing__tiny-albert/clarin-pl__polemo2-official/20230213_230526/checkpoints/epoch=0-step=1.ckpt\n",
      "Loaded model weights from checkpoint at /app/resources/results/hf-internal-testing__tiny-albert/clarin-pl__polemo2-official/20230213_230526/checkpoints/epoch=0-step=1.ckpt\n",
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/embeddings/metric/hugging_face_metric.py:27: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  datasets.load_metric(metric, **init_kwargs) if isinstance(metric, str) else metric\n",
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "pipeline = LightningClassificationPipeline(\n",
    "    embedding_name_or_path=embedding_name_or_path,\n",
    "    output_path=output_path,\n",
    "    config=config,\n",
    "    devices=\"auto\",\n",
    "    accelerator=\"cpu\",\n",
    "    **dataset_kwargs\n",
    ")\n",
    "result = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491215dc-9960-4ad0-bc14-6d61d1fafac8",
   "metadata": {},
   "source": [
    "### Load model from chechpoint automatically generated with Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e824c-00f1-45b0-9e32-1bd33f364f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = output_path / \"checkpoints\" / \"last.ckpt\"\n",
    "ckpt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2785fcbc-1c95-4d23-807f-a14569992354",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_from_ckpt = TextClassificationTask.from_checkpoint(\n",
    "    checkpoint_path=ckpt_path, output_path=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13272a49-8ef5-41af-80a3-5cf3b7b677c7",
   "metadata": {},
   "source": [
    "#### Alternatively we can load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b32fd93-e43d-4c42-961e-53232bf9e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_ckpt = TextClassificationModule.load_from_checkpoint(str(ckpt_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e7972-c386-4c44-9b58-0385213f20f8",
   "metadata": {},
   "source": [
    "The warning appears when loading the model, however, it was validated that the loaded weights are the same as the weights that are being saved. The reason for this is that when the model_state_dict keys are loaded from the cached huggingface model some of them (cls.(...)) do not match the keys from the state_dict of the model weights that are saved.\n",
    "\n",
    "https://github.com/CLARIN-PL/embeddings/issues/225"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e7a6c7-449f-4d0c-9042-a5f98aebc14b",
   "metadata": {},
   "source": [
    "### Use task from checkpoint for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eeab69-e13c-4ba4-b0ea-2473555915d9",
   "metadata": {},
   "source": [
    "`return_names` needs to be set to False since it uses the `datamodule` to retrieves the names while the datamodule is not loaded to `Trainer` in the `LightningTask` since we have not fitted it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad7b9b0-823a-4c8e-aac5-61a333558ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = pipeline.datamodule.test_dataloader()\n",
    "preds = task_from_ckpt.predict(test_dataloader)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c789d71-2368-4add-8a7b-f51571aecfbd",
   "metadata": {},
   "source": [
    "Alternatively we can implicitly assign the `datamodule` to `Trainer` in `LightningTask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9836dc5d-8ee2-46fc-b7d8-94841cc13ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_from_ckpt.trainer.datamodule = pipeline.datamodule\n",
    "preds_with_names = task_from_ckpt.predict(test_dataloader, return_names=True)\n",
    "preds_with_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c321e2-9ecc-4b65-936b-c8e7cca1155a",
   "metadata": {},
   "source": [
    "We can also use previosly loaded lightning model (`LightningModule`) outside of the task and get the predictions. To do this we also need to intitialize a `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3afa250-2937-4aad-bb3c-172a68639892",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(default_root_dir=str(output_path))\n",
    "preds_from_model = trainer.predict(model_from_ckpt, dataloaders=test_dataloader)\n",
    "preds_from_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "86b40992624b6ecf125385760a49d2b554d653d5c84d942a6f4a5512888cc722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
