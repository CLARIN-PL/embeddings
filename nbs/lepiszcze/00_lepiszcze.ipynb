{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEPISZCZE\n",
    "\n",
    "> The use cases and examples how to train and submit models to the [LEPISZCZE](https://lepiszcze.ml/). \n",
    "\n",
    "- bibliography: ../references.bib\n",
    "- title-block-banner: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp lepiszcze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20528/865539875.py:3: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import pandas as pd\n",
    "from IPython.core.display import HTML, display\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { max-width:1800px !important;width:auto; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "display(HTML(\"<style>.container { max-width:1800px !important;width:auto; }</style>\"))\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We recommend to read our NeurIPS paper [@augustyniak2022this] where you can find our lessons learned from the process of designing and compiling LEPISZCZE benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from embeddings.config.lightning_config import LightningBasicConfig, LightningAdvancedConfig\n",
    "from embeddings.pipeline.lightning_classification import LightningClassificationPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with training a text classifier using `embeddings.pipeline.lightning_classification.LightningClassificationPipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr/>\n",
       "<h3>LightningClassificationPipeline</h3>\n",
       "<blockquote><pre><code>LightningClassificationPipeline(embedding_name_or_path:Union[str,pathlib.Path], dataset_name_or_path:Union[str,pathlib.Path], input_column_name:Union[str,Sequence[str]], target_column_name:str, output_path:Union[str,pathlib.Path], evaluation_filename:str='evaluation.json', config:Union[embeddings.config.lightning_config.LightningBasicConfig,embeddings.config.lightning_config.LightningAdvancedConfig]=LightningBasicConfig(use_scheduler=True, optimizer='Adam', warmup_steps=100, learning_rate=0.0001, adam_epsilon=1e-08, weight_decay=0.0, finetune_last_n_layers=-1, classifier_dropout=None, max_seq_length=None, batch_size=32, max_epochs=None, early_stopping_monitor='val/Loss', early_stopping_mode='min', early_stopping_patience=3, tokenizer_kwargs={}, batch_encoding_kwargs={}, dataloader_kwargs={}), devices:Union[List[int],str,int,NoneType]='auto', accelerator:Union[str,pytorch_lightning.accelerators.accelerator.Accelerator,NoneType]='auto', logging_config:embeddings.utils.loggers.LightningLoggingConfig=LightningLoggingConfig(loggers_names=[], tracking_project_name=None, wandb_entity=None, wandb_logger_kwargs={}), tokenizer_name_or_path:Union[str,pathlib.Path,NoneType]=None, predict_subset:embeddings.data.dataset.LightingDataModuleSubset=<LightingDataModuleSubset.TEST: 'test'>, load_dataset_kwargs:Optional[Dict[str,Any]]=None, model_checkpoint_kwargs:Optional[Dict[str,Any]]=None)</code></pre></blockquote><p>Helper class that provides a standard way to create an ABC using\n",
       "inheritance.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc(LightningClassificationPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEPISZCZE_SUBMISSIONS = Path(\"../lepiszcze-submissions\")\n",
    "LEPISZCZE_SUBMISSIONS.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LightningBasicConfig(\n",
    "    learning_rate=0.01, max_epochs=1, max_seq_length=128, finetune_last_n_layers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: polemo2-official/all_text\n",
      "Found cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n",
      "100%|██████████| 3/3 [00:00<00:00, 76.32it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.29s/ba]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.10ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.65ba/s]\n",
      "Casting the dataset: 100%|██████████| 7/7 [00:00<00:00,  9.94ba/s]\n",
      "Casting the dataset: 100%|██████████| 1/1 [00:00<00:00, 10.82ba/s]\n",
      "Casting the dataset: 100%|██████████| 1/1 [00:00<00:00, 11.31ba/s]\n"
     ]
    }
   ],
   "source": [
    "pipeline = LightningClassificationPipeline(\n",
    "    dataset_name_or_path=\"clarin-pl/polemo2-official\",\n",
    "    embedding_name_or_path=\"distilbert-base-uncased\",\n",
    "    input_column_name=\"text\",\n",
    "    target_column_name=\"target\",\n",
    "    output_path=\".\",\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took a couple of seconds but finally we have a pipeline objects ready and we need only run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.config.lightning_config import LightningAdvancedConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No config specified, defaulting to: polemo2-official/all_text\n",
      "Found cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n",
      "100%|██████████| 3/3 [00:00<00:00, 82.76it/s]\n",
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:849: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(strategy=\"dp\"|\"ddp\"|\"ddp2\")`. Setting `strategy=\"ddp_spawn\"` for you.\n",
      "  rank_zero_warn(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__new__() missing 1 required positional argument: 'task'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(results)\n",
      "File \u001b[0;32m/app/embeddings/pipeline/lightning_pipeline.py:49\u001b[0m, in \u001b[0;36mLightningPipeline.run\u001b[0;34m(self, run_name)\u001b[0m\n\u001b[1;32m     47\u001b[0m     run_name \u001b[39m=\u001b[39m standardize_name(run_name)\n\u001b[1;32m     48\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_artifacts()\n\u001b[0;32m---> 49\u001b[0m model_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mexecute(data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatamodule, run_name\u001b[39m=\u001b[39;49mrun_name)\n\u001b[1;32m     50\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluator\u001b[39m.\u001b[39mevaluate(model_result)\n\u001b[1;32m     51\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finish_logging()\n",
      "File \u001b[0;32m/app/embeddings/model/lightning_model.py:24\u001b[0m, in \u001b[0;36mLightningModel.execute\u001b[0;34m(self, data, run_name, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexecute\u001b[39m(\n\u001b[1;32m     21\u001b[0m     \u001b[39mself\u001b[39m, data: HuggingFaceDataModule, run_name: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[1;32m     22\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Predictions:\n\u001b[1;32m     23\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask\u001b[39m.\u001b[39mbuild_task_model()\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtask\u001b[39m.\u001b[39;49mfit_predict(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_subset, run_name\u001b[39m=\u001b[39;49mrun_name)\n",
      "File \u001b[0;32m/app/embeddings/task/lightning_task/lightning_task.py:111\u001b[0m, in \u001b[0;36mLightningTask.fit_predict\u001b[0;34m(self, data, predict_subset, run_name)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel:\n\u001b[1;32m    110\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMODEL_UNDEFINED_EXCEPTION\n\u001b[0;32m--> 111\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(data, run_name\u001b[39m=\u001b[39;49mrun_name)\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mdataset:\n\u001b[1;32m    113\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer, pl\u001b[39m.\u001b[39mTrainer)\n",
      "File \u001b[0;32m/app/embeddings/task/lightning_task/lightning_task.py:97\u001b[0m, in \u001b[0;36mLightningTask.fit\u001b[0;34m(self, data, run_name)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\n\u001b[1;32m     96\u001b[0m cleanup_torch_model_artifacts()\n\u001b[0;32m---> 97\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/app/embeddings/task/lightning_task/lightning_task.py:93\u001b[0m, in \u001b[0;36mLightningTask.fit\u001b[0;34m(self, data, run_name)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m     87\u001b[0m     default_root_dir\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_path),\n\u001b[1;32m     88\u001b[0m     callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[1;32m     89\u001b[0m     logger\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogging_config\u001b[39m.\u001b[39mget_lightning_loggers(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_path, run_name),\n\u001b[1;32m     90\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask_train_kwargs\n\u001b[1;32m     91\u001b[0m )\n\u001b[1;32m     92\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, data)\n\u001b[1;32m     94\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     95\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\n",
      "File \u001b[0;32m/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:737\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    732\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    733\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    734\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    735\u001b[0m     )\n\u001b[1;32m    736\u001b[0m     train_dataloaders \u001b[39m=\u001b[39m train_dataloader\n\u001b[0;32m--> 737\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    738\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    739\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:682\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[39mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[39mas all errors should funnel through them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[39m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    683\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:772\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[39m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[1;32m    771\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m--> 772\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    774\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    775\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1134\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_hook(\u001b[39m\"\u001b[39m\u001b[39mon_before_accelerator_backend_setup\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39msetup_environment()\n\u001b[0;32m-> 1134\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_setup_hook()  \u001b[39m# allow user to setup lightning_module in accelerator environment\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[39m# check if we should delay restoring checkpoint till later\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mrestore_checkpoint_after_pre_dispatch:\n",
      "File \u001b[0;32m/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1434\u001b[0m, in \u001b[0;36mTrainer._call_setup_hook\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1432\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatamodule \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1433\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatamodule\u001b[39m.\u001b[39msetup(stage\u001b[39m=\u001b[39mfn)\n\u001b[0;32m-> 1434\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_hook(\u001b[39m\"\u001b[39;49m\u001b[39msetup\u001b[39;49m\u001b[39m\"\u001b[39;49m, stage\u001b[39m=\u001b[39;49mfn)\n\u001b[1;32m   1436\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mbarrier(\u001b[39m\"\u001b[39m\u001b[39mpost_setup\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1496\u001b[0m, in \u001b[0;36mTrainer.call_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1494\u001b[0m model_fx \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(pl_module, hook_name, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1495\u001b[0m \u001b[39mif\u001b[39;00m callable(model_fx):\n\u001b[0;32m-> 1496\u001b[0m     output \u001b[39m=\u001b[39m model_fx(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1498\u001b[0m \u001b[39m# *Bad code alert*\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m \u001b[39m# The `Accelerator` mostly calls the `TrainingTypePlugin` but some of those calls are deprecated.\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[39m# The following logic selectively chooses which hooks are called on each object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1504\u001b[0m \n\u001b[1;32m   1505\u001b[0m \u001b[39m# call the accelerator hook\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m \u001b[39mif\u001b[39;00m hook_name \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mon_train_start\u001b[39m\u001b[39m\"\u001b[39m,) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator, hook_name):\n",
      "File \u001b[0;32m/app/embeddings/model/lightning_module/huggingface_module.py:45\u001b[0m, in \u001b[0;36mHuggingFaceLightningModule.setup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m     41\u001b[0m     ab_size \u001b[39m=\u001b[39m tb_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39maccumulate_grad_batches\n\u001b[1;32m     42\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_steps: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\n\u001b[1;32m     43\u001b[0m         (\u001b[39mlen\u001b[39m(train_loader\u001b[39m.\u001b[39mdataset) \u001b[39m/\u001b[39m ab_size) \u001b[39m*\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mmax_epochs)\n\u001b[1;32m     44\u001b[0m     )\n\u001b[0;32m---> 45\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_metrics()\n",
      "File \u001b[0;32m/app/embeddings/model/lightning_module/lightning_module.py:120\u001b[0m, in \u001b[0;36mLightningModule._init_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_init_metrics\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_metrics()\n\u001b[1;32m    121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mclone(prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain/\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mclone(prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval/\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/app/embeddings/model/lightning_module/lightning_module.py:129\u001b[0m, in \u001b[0;36mLightningModule.get_default_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_default_metrics\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m MetricCollection:\n\u001b[1;32m    126\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams, \u001b[39mdict\u001b[39m)\n\u001b[1;32m    127\u001b[0m     \u001b[39mreturn\u001b[39;00m MetricCollection(\n\u001b[1;32m    128\u001b[0m         [\n\u001b[0;32m--> 129\u001b[0m             Accuracy(num_classes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhparams[\u001b[39m\"\u001b[39;49m\u001b[39mnum_classes\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\n\u001b[1;32m    130\u001b[0m             Precision(num_classes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams[\u001b[39m\"\u001b[39m\u001b[39mnum_classes\u001b[39m\u001b[39m\"\u001b[39m], average\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    131\u001b[0m             Recall(num_classes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams[\u001b[39m\"\u001b[39m\u001b[39mnum_classes\u001b[39m\u001b[39m\"\u001b[39m], average\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    132\u001b[0m             F1Score(num_classes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams[\u001b[39m\"\u001b[39m\u001b[39mnum_classes\u001b[39m\u001b[39m\"\u001b[39m], average\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    133\u001b[0m         ]\n\u001b[1;32m    134\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: __new__() missing 1 required positional argument: 'task'"
     ]
    }
   ],
   "source": [
    "results = pipeline.run()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.pipeline.flair_classification import FlairClassificationPipeline\n",
    "from embeddings.utils.utils import build_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \".\"\n",
    "embedding_name_or_path = \"clarin-pl/word2vec-kgr10\"\n",
    "dataset_name = \"clarin-pl/polemo2-official\"\n",
    "input_column_name = \"text\"\n",
    "target_column_name = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 00:16:43,449 - embeddings.utils.utils - WARNING - String 'clarin-pl/word2vec-kgr10' contains '/'. Replacing it with '__'. Cleaned_text: clarin-pl__word2vec-kgr10.\n",
      "2022-12-16 00:16:43,450 - embeddings.utils.utils - WARNING - String 'clarin-pl/polemo2-official' contains '/'. Replacing it with '__'. Cleaned_text: clarin-pl__polemo2-official.\n",
      "2022-12-16 00:16:44,569 - embeddings.embedding.auto_flair - INFO - clarin-pl/word2vec-kgr10 not compatible with Transformers, trying to initialise as static embedding.\n",
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/huggingface_hub/file_download.py:594: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_path = build_output_path(root, embedding_name_or_path, dataset_name)\n",
    "pipeline = FlairClassificationPipeline(\n",
    "    embedding_name=embedding_name_or_path,\n",
    "    dataset_name=dataset_name,\n",
    "    input_column_name=input_column_name,\n",
    "    target_column_name=target_column_name,\n",
    "    output_path=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: polemo2-official/all_text\n",
      "Found cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n",
      "100%|██████████| 3/3 [00:00<00:00, 86.55it/s]\n",
      "2022-12-16 00:18:30,059 - embeddings.transformation.flair_transformation.corpus_transformation - INFO - Info of ['train', 'validation', 'test']:\n",
      "{'builder_name': 'polemo2-official',\n",
      " 'citation': '\\n'\n",
      "             '@inproceedings{kocon-etal-2019-multi,\\n'\n",
      "             '    title = \"Multi-Level Sentiment Analysis of {P}ol{E}mo 2.0: '\n",
      "             'Extended Corpus of Multi-Domain Consumer Reviews\",\\n'\n",
      "             '    author = \"Koco{\\'n}, Jan  and\\n'\n",
      "             '      Mi{\\\\l}kowski, Piotr  and\\n'\n",
      "             '      Za{\\'s}ko-Zieli{\\'n}ska, Monika\",\\n'\n",
      "             '    booktitle = \"Proceedings of the 23rd Conference on '\n",
      "             'Computational Natural Language Learning (CoNLL)\",\\n'\n",
      "             '    month = nov,\\n'\n",
      "             '    year = \"2019\",\\n'\n",
      "             '    address = \"Hong Kong, China\",\\n'\n",
      "             '    publisher = \"Association for Computational Linguistics\",\\n'\n",
      "             '    url = \"https://www.aclweb.org/anthology/K19-1092\",\\n'\n",
      "             '    doi = \"10.18653/v1/K19-1092\",\\n'\n",
      "             '    pages = \"980--991\",}\\n',\n",
      " 'config_name': 'all_text',\n",
      " 'dataset_size': 6606544,\n",
      " 'description': 'PolEmo 2.0:  Corpus of Multi-Domain Consumer Reviews, '\n",
      "                'evaluation data for article presented at CoNLL.',\n",
      " 'download_checksums': {'https://huggingface.co/datasets/clarin-pl/polemo2-official/resolve/main/data/all.text.dev.txt': {'checksum': '1d92db1b0b76f7c4787cbc1492f68be55c762deae0d6c6a0e5359748de7f4c47',\n",
      "                                                                                                                          'num_bytes': 663454},\n",
      "                        'https://huggingface.co/datasets/clarin-pl/polemo2-official/resolve/main/data/all.text.test.txt': {'checksum': '153ca33789f9b5bc5c8a7661bb13861a5c714b915b4f9969ae9901bee116d4c7',\n",
      "                                                                                                                           'num_bytes': 648758},\n",
      "                        'https://huggingface.co/datasets/clarin-pl/polemo2-official/resolve/main/data/all.text.train.txt': {'checksum': '03b9908ba4662b81879c719ae7d19e72ff8c705eb7dc127450bc232810068bae',\n",
      "                                                                                                                            'num_bytes': 5372370}},\n",
      " 'download_size': 6684582,\n",
      " 'features': {'target': ClassLabel(names=['zero', 'minus', 'plus', 'amb'], id=None),\n",
      "              'text': Value(dtype='string', id=None)},\n",
      " 'homepage': 'https://clarin-pl.eu/dspace/handle/11321/710',\n",
      " 'license': 'CC-BY-4.0',\n",
      " 'post_processed': None,\n",
      " 'post_processing_size': None,\n",
      " 'size_in_bytes': 13291126,\n",
      " 'splits': {'test': SplitInfo(name='test', num_bytes=640863, num_examples=820, shard_lengths=None, dataset_name='polemo2-official'),\n",
      "            'train': SplitInfo(name='train', num_bytes=5310040, num_examples=6573, shard_lengths=None, dataset_name='polemo2-official'),\n",
      "            'validation': SplitInfo(name='validation', num_bytes=655641, num_examples=823, shard_lengths=None, dataset_name='polemo2-official')},\n",
      " 'supervised_keys': None,\n",
      " 'task_templates': None,\n",
      " 'version': 0.0.0}\n",
      "2022-12-16 00:18:30,059 - embeddings.transformation.flair_transformation.corpus_transformation - INFO - Schemas:\tDatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'target'],\n",
      "        num_rows: 6573\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'target'],\n",
      "        num_rows: 823\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'target'],\n",
      "        num_rows: 820\n",
      "    })\n",
      "})\n",
      "100%|██████████| 6573/6573 [00:00<00:00, 15093.71ex/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 25.72ba/s]\n",
      "100%|██████████| 823/823 [00:00<00:00, 17241.72ex/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 30.31ba/s]\n",
      "100%|██████████| 820/820 [00:00<00:00, 18486.84ex/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 32.88ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-16 00:18:42,849 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6573it [00:00, 56695.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-16 00:18:42,971 Dictionary created for label 'None' with 5 values: 1 (seen 2469 times), 2 (seen 1824 times), 3 (seen 1309 times), 0 (seen 971 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-16 00:18:44,592 ----------------------------------------------------------------------------------------------------\n",
      "2022-12-16 00:18:44,594 Model: \"TextClassifier(\n",
      "  (decoder): Linear(in_features=300, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (document_embeddings): DocumentPoolEmbeddings(\n",
      "    fine_tune_mode=none, pooling=mean\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings(\n",
      "        '/root/.cache/huggingface/hub/894a9a0a7a7c9e5defa71b9ed26e5699b9394e25d3ebce51d39188935f15ac57.3b0d8f1d834bcf9f436953bd7051d5d9761aae9f482a26ed62e8c21283da012b'\n",
      "        (embedding): Embedding(2283378, 300)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2022-12-16 00:18:44,596 ----------------------------------------------------------------------------------------------------\n",
      "2022-12-16 00:18:44,597 Corpus: \"Corpus: 6573 train + 823 dev + 820 test sentences\"\n",
      "2022-12-16 00:18:44,598 ----------------------------------------------------------------------------------------------------\n",
      "2022-12-16 00:18:44,599 Parameters:\n",
      "2022-12-16 00:18:44,600  - learning_rate: \"0.001000\"\n",
      "2022-12-16 00:18:44,601  - mini_batch_size: \"32\"\n",
      "2022-12-16 00:18:44,602  - patience: \"3\"\n",
      "2022-12-16 00:18:44,603  - anneal_factor: \"0.5\"\n",
      "2022-12-16 00:18:44,604  - max_epochs: \"20\"\n",
      "2022-12-16 00:18:44,605  - shuffle: \"True\"\n",
      "2022-12-16 00:18:44,606  - train_with_dev: \"False\"\n",
      "2022-12-16 00:18:44,607  - batch_growth_annealing: \"False\"\n",
      "2022-12-16 00:18:44,608 ----------------------------------------------------------------------------------------------------\n",
      "2022-12-16 00:18:44,609 Model training base path: \"clarin-pl__word2vec-kgr10/clarin-pl__polemo2-official/20221216_001643\"\n",
      "2022-12-16 00:18:44,610 ----------------------------------------------------------------------------------------------------\n",
      "2022-12-16 00:18:44,611 Device: cuda:0\n",
      "2022-12-16 00:18:44,612 ----------------------------------------------------------------------------------------------------\n",
      "2022-12-16 00:18:44,613 Embeddings storage mode: cpu\n",
      "2022-12-16 00:18:44,614 ----------------------------------------------------------------------------------------------------\n",
      "2022-12-16 00:18:52,165 epoch 1 - iter 20/206 - loss 0.05042096 - samples/sec: 84.78 - lr: 0.001000\n",
      "2022-12-16 00:18:58,129 epoch 1 - iter 40/206 - loss 0.05039598 - samples/sec: 107.37 - lr: 0.001000\n",
      "2022-12-16 00:19:05,184 epoch 1 - iter 60/206 - loss 0.05036845 - samples/sec: 90.75 - lr: 0.001000\n",
      "2022-12-16 00:19:11,868 epoch 1 - iter 80/206 - loss 0.05034159 - samples/sec: 95.80 - lr: 0.001000\n",
      "2022-12-16 00:19:17,588 epoch 1 - iter 100/206 - loss 0.05031501 - samples/sec: 111.94 - lr: 0.001000\n",
      "2022-12-16 00:19:23,393 epoch 1 - iter 120/206 - loss 0.05028955 - samples/sec: 110.32 - lr: 0.001000\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "result = pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "86b40992624b6ecf125385760a49d2b554d653d5c84d942a6f4a5512888cc722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
