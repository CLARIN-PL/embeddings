{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEPISZCZE\n",
    "\n",
    "> The use cases and examples how to train and submit models to the [LEPISZCZE](https://lepiszcze.ml/). \n",
    "\n",
    "- bibliography: ../references.bib\n",
    "- title-block-banner: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We recommend to read our NeurIPS paper [@augustyniak2022this] where you can find our lessons learned from the process of designing and compiling LEPISZCZE benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from embeddings.config.lightning_config import LightningBasicConfig\n",
    "from embeddings.pipeline.lightning_classification import LightningClassificationPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with training a text classifier using `embeddings.pipeline.lightning_classification.LightningClassificationPipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### LightningClassificationPipeline\n",
       "\n",
       ">      LightningClassificationPipeline\n",
       ">                                       (embedding_name_or_path:Union[str,pathli\n",
       ">                                       b.Path], dataset_name_or_path:Union[str,\n",
       ">                                       pathlib.Path], input_column_name:Union[s\n",
       ">                                       tr,Sequence[str]],\n",
       ">                                       target_column_name:str,\n",
       ">                                       output_path:Union[str,pathlib.Path], eva\n",
       ">                                       luation_filename:str='evaluation.json', \n",
       ">                                       config:Union[embeddings.config.lightning\n",
       ">                                       _config.LightningBasicConfig,embeddings.\n",
       ">                                       config.lightning_config.LightningAdvance\n",
       ">                                       dConfig]=LightningBasicConfig(use_schedu\n",
       ">                                       ler=True, optimizer='Adam',\n",
       ">                                       warmup_steps=100, learning_rate=0.0001,\n",
       ">                                       adam_epsilon=1e-08, weight_decay=0.0,\n",
       ">                                       finetune_last_n_layers=-1,\n",
       ">                                       classifier_dropout=None,\n",
       ">                                       max_seq_length=None, batch_size=32,\n",
       ">                                       max_epochs=None,\n",
       ">                                       early_stopping_monitor='val/Loss',\n",
       ">                                       early_stopping_mode='min',\n",
       ">                                       early_stopping_patience=3,\n",
       ">                                       tokenizer_kwargs={},\n",
       ">                                       batch_encoding_kwargs={},\n",
       ">                                       dataloader_kwargs={}), devices:Union[int\n",
       ">                                       ,List[int],str,NoneType]='auto', acceler\n",
       ">                                       ator:Union[str,pytorch_lightning.acceler\n",
       ">                                       ators.accelerator.Accelerator,NoneType]=\n",
       ">                                       'auto', logging_config:embeddings.utils.\n",
       ">                                       loggers.LightningLoggingConfig=Lightning\n",
       ">                                       LoggingConfig(loggers_names=[],\n",
       ">                                       tracking_project_name=None,\n",
       ">                                       wandb_entity=None,\n",
       ">                                       wandb_logger_kwargs={}), tokenizer_name_\n",
       ">                                       or_path:Union[pathlib.Path,str,NoneType]\n",
       ">                                       =None, predict_subset:embeddings.data.da\n",
       ">                                       taset.LightingDataModuleSubset=<Lighting\n",
       ">                                       DataModuleSubset.TEST: 'test'>, load_dat\n",
       ">                                       aset_kwargs:Optional[Dict[str,Any]]=None\n",
       ">                                       , model_checkpoint_kwargs:Optional[Dict[\n",
       ">                                       str,Any]]=None)\n",
       "\n",
       "Helper class that provides a standard way to create an ABC using\n",
       "inheritance."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### LightningClassificationPipeline\n",
       "\n",
       ">      LightningClassificationPipeline\n",
       ">                                       (embedding_name_or_path:Union[str,pathli\n",
       ">                                       b.Path], dataset_name_or_path:Union[str,\n",
       ">                                       pathlib.Path], input_column_name:Union[s\n",
       ">                                       tr,Sequence[str]],\n",
       ">                                       target_column_name:str,\n",
       ">                                       output_path:Union[str,pathlib.Path], eva\n",
       ">                                       luation_filename:str='evaluation.json', \n",
       ">                                       config:Union[embeddings.config.lightning\n",
       ">                                       _config.LightningBasicConfig,embeddings.\n",
       ">                                       config.lightning_config.LightningAdvance\n",
       ">                                       dConfig]=LightningBasicConfig(use_schedu\n",
       ">                                       ler=True, optimizer='Adam',\n",
       ">                                       warmup_steps=100, learning_rate=0.0001,\n",
       ">                                       adam_epsilon=1e-08, weight_decay=0.0,\n",
       ">                                       finetune_last_n_layers=-1,\n",
       ">                                       classifier_dropout=None,\n",
       ">                                       max_seq_length=None, batch_size=32,\n",
       ">                                       max_epochs=None,\n",
       ">                                       early_stopping_monitor='val/Loss',\n",
       ">                                       early_stopping_mode='min',\n",
       ">                                       early_stopping_patience=3,\n",
       ">                                       tokenizer_kwargs={},\n",
       ">                                       batch_encoding_kwargs={},\n",
       ">                                       dataloader_kwargs={}), devices:Union[int\n",
       ">                                       ,List[int],str,NoneType]='auto', acceler\n",
       ">                                       ator:Union[str,pytorch_lightning.acceler\n",
       ">                                       ators.accelerator.Accelerator,NoneType]=\n",
       ">                                       'auto', logging_config:embeddings.utils.\n",
       ">                                       loggers.LightningLoggingConfig=Lightning\n",
       ">                                       LoggingConfig(loggers_names=[],\n",
       ">                                       tracking_project_name=None,\n",
       ">                                       wandb_entity=None,\n",
       ">                                       wandb_logger_kwargs={}), tokenizer_name_\n",
       ">                                       or_path:Union[pathlib.Path,str,NoneType]\n",
       ">                                       =None, predict_subset:embeddings.data.da\n",
       ">                                       taset.LightingDataModuleSubset=<Lighting\n",
       ">                                       DataModuleSubset.TEST: 'test'>, load_dat\n",
       ">                                       aset_kwargs:Optional[Dict[str,Any]]=None\n",
       ">                                       , model_checkpoint_kwargs:Optional[Dict[\n",
       ">                                       str,Any]]=None)\n",
       "\n",
       "Helper class that provides a standard way to create an ABC using\n",
       "inheritance."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LightningClassificationPipeline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to store submission data in a specific directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEPISZCZE_SUBMISSIONS = Path(\"../lepiszcze-submissions\")\n",
    "LEPISZCZE_SUBMISSIONS.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a pipeline object. We will use `LightningClassificationPipeline` with dataset related to sentiment analysis and a very small transfomer model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want only run training for testing purposes, hence it would be good no to generate to much greenhouse gases, hence we narrow max epochs to only 1. In the real traning code it would be good to customize traning procedure with more configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LightningBasicConfig(max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: polemo2-official/all_text\n",
      "Found cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n",
      "100%|██████████| 3/3 [00:00<00:00, 625.58it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-2e61085076a665b0.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-ac057aeafd577fd0.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-502164b331496757.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-13cbbe9129f685fa.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-b1c5d1c8fe129da7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-1f1e81ef3032c906.arrow\n"
     ]
    }
   ],
   "source": [
    "pipeline = LightningClassificationPipeline(\n",
    "    dataset_name_or_path=\"clarin-pl/polemo2-official\",\n",
    "    embedding_name_or_path=\"hf-internal-testing/tiny-albert\",\n",
    "    input_column_name=\"text\",\n",
    "    target_column_name=\"target\",\n",
    "    output_path=\".\",\n",
    "    devices=\"auto\",\n",
    "    accelerator=\"cpu\",\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took a couple of seconds but finally we have a pipeline objects ready and we need only run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hf-internal-testing/tiny-albert were not used when initializing AlbertForSequenceClassification: ['predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at hf-internal-testing/tiny-albert and are newly initialized: ['classifier.weight', 'albert.pooler.bias', 'classifier.bias', 'albert.pooler.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No config specified, defaulting to: polemo2-official/all_text\n",
      "Found cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n",
      "100%|██████████| 3/3 [00:00<00:00, 663.31it/s]\n",
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1579: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name          | Type                            | Params\n",
      "------------------------------------------------------------------\n",
      "0 | model         | AlbertForSequenceClassification | 352 K \n",
      "1 | metrics       | MetricCollection                | 0     \n",
      "2 | train_metrics | MetricCollection                | 0     \n",
      "3 | val_metrics   | MetricCollection                | 0     \n",
      "4 | test_metrics  | MetricCollection                | 0     \n",
      "------------------------------------------------------------------\n",
      "352 K     Trainable params\n",
      "0         Non-trainable params\n",
      "352 K     Total params\n",
      "1.410     Total estimated model params size (MB)\n",
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:623: UserWarning: Checkpoint directory /app/nbs/lepiszcze/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 232/232 [00:37<00:00,  6.26it/s, loss=1.35, v_num=, train/BaseLR=0.000, train/LambdaLR=0.000, val/MulticlassAccuracy=0.369, val/MulticlassPrecision=0.0923, val/MulticlassRecall=0.250, val/MulticlassF1Score=0.135]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  92%|█████████▏| 24/26 [00:00<00:00, 34.68it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test/Loss': 1.341328501701355,\n",
      " 'test/MulticlassAccuracy': 0.4134146273136139,\n",
      " 'test/MulticlassF1Score': 0.1462467610836029,\n",
      " 'test/MulticlassPrecision': 0.10335365682840347,\n",
      " 'test/MulticlassRecall': 0.25}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 26/26 [00:00<00:00, 34.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /app/nbs/lepiszcze/checkpoints/epoch=0-step=205.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded model weights from checkpoint at /app/nbs/lepiszcze/checkpoints/epoch=0-step=205.ckpt\n",
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 206it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/embeddings/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "results = pipeline.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we trained the model only for 1 epoch, the metrics are not too high and they are rather presented to show that the pipeline works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.41341463414634144,\n",
       " 'f1_macro': 0.1462467644521139,\n",
       " 'f1_micro': 0.41341463414634144,\n",
       " 'f1_weighted': 0.2418422104842274,\n",
       " 'recall_macro': 0.25,\n",
       " 'recall_micro': 0.41341463414634144,\n",
       " 'recall_weighted': 0.41341463414634144,\n",
       " 'precision_macro': 0.10335365853658536,\n",
       " 'precision_micro': 0.41341463414634144,\n",
       " 'precision_weighted': 0.17091165972635333,\n",
       " 'classes': {0: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 118},\n",
       "  1: {'precision': 0.41341463414634144,\n",
       "   'recall': 1.0,\n",
       "   'f1': 0.5849870578084556,\n",
       "   'support': 339},\n",
       "  2: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 227},\n",
       "  3: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 136}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embeddings",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
