{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ed8e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")\n",
    "\n",
    "from embeddings.config.lightning_config import (\n",
    "    LightningAdvancedConfig,\n",
    "    LightningBasicConfig,\n",
    ")\n",
    "from embeddings.defaults import DATASET_PATH, RESULTS_PATH\n",
    "from embeddings.pipeline.hf_preprocessing_pipeline import HuggingFacePreprocessingPipeline\n",
    "from embeddings.pipeline.lightning_classification import LightningClassificationPipeline\n",
    "from embeddings.utils.utils import build_output_path, format_eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d5b6e",
   "metadata": {},
   "source": [
    "# How to use our configs? \n",
    "\n",
    "Two types of config are defined in our library: `BasicConfig` and `AdvancedConfig`.\n",
    "`BasicConfig` allows for easy use of the most common parameters in the pipeline. However, the objects defined in our pipelines are constructed in a way that they can be further paramatrized with keyword arguments. These arguments can be utilized by constructing the `AdvancedConfig`.   \n",
    "In summary, the `BasicConfig` takes arguments and automatically assign them into proper keyword group, while the `AdvancedConfig` takes as the input keyword groups that should be already correctly mapped.  \n",
    "\n",
    "The keywords arguments will depend on the type of the pipelines for the Flair pipeline (that are used for static embeddings), and thus there are config defined for type of the task.\n",
    "\n",
    "The list of available config can be found below.\n",
    "\n",
    "\n",
    "### **Flair**:  \n",
    "   - FlairBasicConfig\n",
    "   - FlairSequenceLabelingBasicConfig\n",
    "   - FlairTextClassificationBasicConfig\n",
    "   - FlairSequenceLabelingAdvancedConfig\n",
    "   - FlairTextClassificationAdvancedConfig\n",
    "   \n",
    "### **Lightning**:\n",
    "   - LightningBasicConfig\n",
    "   - LightningAdvancedConfig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## What are the available advanced config keyword arguments and where to find them?\n",
    "\n",
    "In general, the keywords are passed to the object when constructing specific pipelines. Take for example the fragment of `LightningClassificationPipeline`:\n",
    "\n",
    "```\n",
    "datamodule = TextClassificationDataModule(\n",
    "    tokenizer_name_or_path=tokenizer_name_or_path,\n",
    "    dataset_name_or_path=dataset_name_or_path,\n",
    "    text_fields=input_column_name,\n",
    "    target_field=target_column_name,\n",
    "    train_batch_size=config_space.train_batch_size,\n",
    "    eval_batch_size=config_space.eval_batch_size,\n",
    "    tokenizer_kwargs=config_space.tokenizer_kwargs,\n",
    "    batch_encoding_kwargs=config_space.batch_encoding_kwargs,\n",
    "    load_dataset_kwargs=load_dataset_kwargs,\n",
    "    **config_space.datamodule_kwargs\n",
    ")\n",
    "task = TextClassificationTask(\n",
    "    model_name_or_path=embedding_name_or_path,\n",
    "    output_path=output_path,\n",
    "    finetune_last_n_layers=config_space.finetune_last_n_layers,\n",
    "    model_config_kwargs=config_space.model_config_kwargs,\n",
    "    task_model_kwargs=config_space.task_model_kwargs,\n",
    "    task_train_kwargs=config_space.task_train_kwargs,\n",
    "    early_stopping_kwargs=config_space.early_stopping_kwargs,\n",
    ")\n",
    "```\n",
    "\n",
    "We can identify and trace the keyword arguments to find the possible arguments that can be set in the config kwargs.\n",
    "\n",
    "Let's see an example of the process of defininf the parameters in our `LightningAdvancedConfig`. \n",
    "Tracing back different kwargs we can find: \n",
    "\n",
    "1. `task_train_kwargs`\n",
    "https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-flags - parameters that are passed to the `Lightning Trainer` object.\n",
    "\n",
    "2. `task_model_kwargs`\n",
    "https://github.com/CLARIN-PL/embeddings/blob/4292d110691c6c67695fefab74c927dbae9acff7/embeddings/model/lightning_module/lightning_module.py#L19 - parameters that are passed to the `Lightning module` object (we use `TextClassificationModule` which inherits from `HuggingFaceLightningModule` and `HuggingFaceLightningModule`).\n",
    "\n",
    "3. `datamodule_kwargs` - https://github.com/CLARIN-PL/embeddings/blob/main/embeddings/data/datamodule.py#L35 - parameters passed to the datamodule classes, currently `HuggingFaceDataModule` takes several arguments (such as max_seq_length, processing_batch_size or downsamples args) as an input\n",
    "\n",
    "4. `batch_encoding_kwargs` https://github.com/huggingface/transformers/blob/db7d6a80e82d66127b2a44b6e3382969fdc8b207/src/transformers/tokenization_utils_base.py#L2359 - parameters that are defined in `__call__` method of the tokenizer which allow for manipulation of the tokenized text by setting parameters such as truncation, padding, stride etc. and specifying the return format of the tokenized text\n",
    "\n",
    "5. `tokenizer_kwargs` https://github.com/huggingface/transformers/blob/074645e32acda6498f16203a8459bb597610f623/src/transformers/models/auto/tokenization_auto.py#L351\n",
    "This is a generic configuration class of the hugginface model's tokenizer, possible parameters depends on the tokenizer that is used. For example for bert uncased tokenizer these parameters are present here: https://huggingface.co/bert-base-uncased/blob/main/tokenizer_config.json\n",
    "\n",
    "6. `load_dataset_kwargs`\n",
    "https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/loading_methods#datasets.load_dataset - keyword arguments from the `datasets.load_dataset method` which loads a dataset from the Hugging Face Hub, or a local dataset; mostly metadata for downloading, loading, caching the dataset\n",
    "\n",
    "7. `model_config_kwargs`\n",
    "https://github.com/huggingface/transformers/blob/074645e32acda6498f16203a8459bb597610f623/src/transformers/models/auto/configuration_auto.py#L515\n",
    "This is a generic configuration class of the hugginface model, possible parameters depends on the model that is used. For example for bert uncased these parameters are present here: https://huggingface.co/bert-base-uncased/blob/main/config.json\n",
    "\n",
    "8. `early_stopping_kwargs`  \n",
    "https://github.com/PyTorchLightning/pytorch-lightning/blob/5d2d9b09df5359226fea6ad2722592839ac0ebc4/pytorch_lightning/callbacks/early_stopping.py#L35 - params defined in `__init__` of the `EarlyStopping` lightning callback; you can specify a metric to monitor and conditions to stop training when it stops improving \n",
    "9. `dataloader_kwargs`\n",
    "https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html#DataLoader - defined in `__init__` of the torch `DataLoader` object which wraps an iterable around the Dataset to enable easy access to the sample; specify params such as num of workers, sampling or shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986cddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_name_or_path = \"allegro/herbert-base-cased\"\n",
    "dataset_name = \"clarin-pl/polemo2-official\"\n",
    "input_columns_name = \"text\"\n",
    "target_column_name = \"target\"\n",
    "\n",
    "dataset_path = build_output_path(DATASET_PATH, embedding_name_or_path, dataset_name)\n",
    "dataset_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_path = build_output_path(RESULTS_PATH, embedding_name_or_path, dataset_name)\n",
    "output_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f70e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_config = LightningBasicConfig(\n",
    "    learning_rate=0.01, max_epochs=1, max_seq_length=128, finetune_last_n_layers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556574a3-8aa5-4cbd-9a41-eae4e3376c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_config = LightningAdvancedConfig(\n",
    "    finetune_last_n_layers=0,\n",
    "    datamodule_kwargs={\n",
    "        \"max_seq_length\": 64,\n",
    "    },\n",
    "        task_train_kwargs={\n",
    "            \"max_epochs\": 1,\n",
    "            \"devices\": \"auto\",\n",
    "            \"accelerator\": \"cpu\",\n",
    "            \"deterministic\": True,\n",
    "        },\n",
    "        task_model_kwargs={\n",
    "            \"learning_rate\": 5e-4,\n",
    "            \"train_batch_size\": 32,\n",
    "            \"eval_batch_size\": 32,\n",
    "            \"use_scheduler\": False,\n",
    "            \"optimizer\": \"AdamW\",\n",
    "            \"adam_epsilon\": 1e-8,\n",
    "            \"warmup_steps\": 100,\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "        early_stopping_kwargs={\n",
    "            \"monitor\": \"val/Loss\",\n",
    "            \"mode\": \"min\",\n",
    "            \"patience\": 3,\n",
    "        },\n",
    "    model_config_kwargs={\"classifier_dropout\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84692f2-6e15-46a6-920a-7762df0e2cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = HuggingFacePreprocessingPipeline(\n",
    "    dataset_name=\"clarin-pl/polemo2-official\",\n",
    "    load_dataset_kwargs={\n",
    "        \"train_domains\": [\"hotels\", \"medicine\"],\n",
    "        \"dev_domains\": [\"hotels\", \"medicine\"],\n",
    "        \"test_domains\": [\"hotels\", \"medicine\"],\n",
    "        \"text_cfg\": \"text\",\n",
    "    },\n",
    "    persist_path=str(dataset_path),\n",
    "    sample_missing_splits=None,\n",
    "    ignore_test_subset=False,\n",
    "    downsample_splits=(0.01, 0.01, 0.05),\n",
    "    seed=441,\n",
    ")\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcecbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = LightningClassificationPipeline(\n",
    "    embedding_name_or_path=embedding_name_or_path,\n",
    "    dataset_name_or_path=str(dataset_path),\n",
    "    input_column_name=input_columns_name,\n",
    "    target_column_name=target_column_name,\n",
    "    output_path=output_path,\n",
    "    config=advanced_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e85282",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:clarinpl-embeddings]",
   "language": "python",
   "name": "conda-env-clarinpl-embeddings-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
