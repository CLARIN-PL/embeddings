{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "633a339d",
   "metadata": {},
   "source": [
    "# How to use our configs? \n",
    "> Detailed tutorial about how to pass arguments to embeddings pipelines.\n",
    "\n",
    "- title-block-banner: true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.qmd import *\n",
    "import warnings\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# disable HF thousand warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "# set os environ variable for multiprocesses\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4999e8c8",
   "metadata": {},
   "source": [
    "Two types of config are defined in our library: `BasicConfig` and `AdvancedConfig`.\n",
    "`BasicConfig` allows for easy use of the most common parameters in the pipeline. However, the objects defined in our pipelines are constructed in a way that they can be further paramatrized with keyword arguments. These arguments can be utilized by constructing the `AdvancedConfig`.   \n",
    "In summary, the `BasicConfig` takes arguments and automatically assign them into proper keyword group, while the `AdvancedConfig` takes as the input keyword groups that should be already correctly mapped.  \n",
    "\n",
    "\n",
    "The list of available config can be found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab00cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "from embeddings.config.lightning_config import (\n",
    "    LightningAdvancedConfig,\n",
    "    LightningBasicConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8de07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(LightningBasicConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f004815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(LightningAdvancedConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430f11f",
   "metadata": {},
   "source": [
    "## Running pipeline with BasicConfig\n",
    "\n",
    "Let's run example pipeline on `polemo2` dataset, for tutorial purposes we use `allegro/herbert-base-cased`.\n",
    "\n",
    "But first we downsample our dataset due to hardware limitations for that purpose we use HuggingFacePreprocessingPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286a7ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exec_doc\n",
    "\n",
    "from embeddings.pipeline.hf_preprocessing_pipeline import HuggingFacePreprocessingPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf41e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HuggingFacePreprocessingPipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775ced7d",
   "metadata": {},
   "source": [
    "Then we need to use `run` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c627fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HuggingFacePreprocessingPipeline.run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4719a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exec_doc\n",
    "\n",
    "prepocessing = HuggingFacePreprocessingPipeline(\n",
    "    dataset_name=\"clarin-pl/polemo2-official\",\n",
    "    persist_path=\"data/polemo2_downsampled\",\n",
    "    downsample_splits=(0.001, 0.005, 0.005)\n",
    ")\n",
    "prepocessing.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cca9ad9",
   "metadata": {},
   "source": [
    "We have now our data prepared locally, now we need to define our `pipeline`.\n",
    "\n",
    "Let's start from config. \n",
    " We will use parameters from [`clarin-pl/lepiszcze-allegro__herbert-base-cased-polemo2`](https://huggingface.co/clarin-pl/lepiszcze-allegro__herbert-base-cased-polemo2), which configuration was obtained from `extensive hyperparmeter search`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d208178",
   "metadata": {},
   "source": [
    "::: {.callout-warning}  \n",
    "Due to hardware limitation we limit parmeter `max_epochs` to 1 and we leave `early stopping` configuration parameters as defaults \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16164a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(LightningBasicConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a1686d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LightningBasicConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#|exec_doc\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m cfg \u001b[38;5;241m=\u001b[39m \u001b[43mLightningBasicConfig\u001b[49m(\n\u001b[1;32m      4\u001b[0m         use_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m         warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      7\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m      8\u001b[0m         adam_epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-06\u001b[39m,\n\u001b[1;32m      9\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m     10\u001b[0m         finetune_last_n_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     11\u001b[0m         classifier_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     12\u001b[0m         max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     14\u001b[0m         max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m cfg\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LightningBasicConfig' is not defined"
     ]
    }
   ],
   "source": [
    "#|exec_doc\n",
    "\n",
    "cfg = LightningBasicConfig(\n",
    "        use_scheduler=True,\n",
    "        optimizer=\"Adam\",\n",
    "        warmup_steps=100,\n",
    "        learning_rate=0.001,\n",
    "        adam_epsilon=1e-06,\n",
    "        weight_decay=0.001,\n",
    "        finetune_last_n_layers=3,\n",
    "        classifier_dropout=0.2,\n",
    "        max_seq_length=None,\n",
    "        batch_size=64,\n",
    "        max_epochs=1,\n",
    ")\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e53f5",
   "metadata": {},
   "source": [
    "Now we define pipeline dedicated for text classification `LightningClassificationPipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7297f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.pipeline.lightning_classification import LightningClassificationPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa29a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(LightningClassificationPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55200cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict # For metrics conversion\n",
    "import pandas as pd  # For metrics conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f1256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exec_doc\n",
    "pipeline = LightningClassificationPipeline(\n",
    "    embedding_name_or_path=\"allegro/herbert-base-cased\",\n",
    "    dataset_name_or_path=\"data/polemo2_downsampled/\",\n",
    "    input_column_name=\"text\",\n",
    "    target_column_name=\"target\",\n",
    "    output_path=\".\",\n",
    "    config=cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f285db21",
   "metadata": {},
   "source": [
    "Similarly as with HuggingFacePreprocessingPipeline we use `run` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0302480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(LightningClassificationPipeline.run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b0f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exec_doc\n",
    "metrics = pipeline.run()\n",
    "\n",
    "# Converting metrics to DataFrame for better nb display\n",
    "\n",
    "metrics = pd.DataFrame.from_dict(asdict(metrics), orient=\"index\", columns=[\"values\"])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a1541a",
   "metadata": {},
   "source": [
    "## Running pipeline with AdvancedConfig\n",
    "\n",
    "As mentioned in previous section `LightningBasicConfig` is only limited to most important parameters. \n",
    "\n",
    "Let's see an example of the process of defining the parameters in our `LightningAdvancedConfig`. \n",
    "Tracing back different kwargs we can find: \n",
    "\n",
    "\n",
    "1. [`task_train_kwargs`](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-flags)\n",
    "Parameters that are passed to the `Lightning Trainer` object.\n",
    "\n",
    "\n",
    "1. [`task_model_kwargs`](https://github.com/CLARIN-PL/embeddings/blob/main/embeddings/model/lightning_module/lightning_module.py#L19)\n",
    "Parameters that are passed to the `Lightning module` object (we use `TextClassificationModule` which inherits from `HuggingFaceLightningModule` and `HuggingFaceLightningModule`).\n",
    "\n",
    "1. [`datamodule_kwargs`](https://github.com/CLARIN-PL/embeddings/blob/main/embeddings/data/datamodule.py#L35)  \n",
    "Parameters passed to the datamodule classes, currently `HuggingFaceDataModule` takes several arguments (such as max_seq_length, processing_batch_size or downsamples args) as an input\n",
    "\n",
    "1. [`batch_encoding_kwargs`](https://github.com/huggingface/transformers/blob/main/src/transformers/tokenization_utils_base.py#L2456)\n",
    "Parameters that are defined in `__call__` method of the tokenizer which allow for manipulation of the tokenized text by setting parameters such as truncation, padding, stride etc. and specifying the return format of the tokenized text\n",
    "\n",
    "1. [`tokenizer_kwargs`](https://github.com/huggingface/transformers/blob/074645e32acda6498f16203a8459bb597610f623/src/transformers/models/auto/tokenization_auto.py#L351)\n",
    "This is a generic configuration class of the hugginface model's tokenizer, possible parameters depends on the tokenizer that is used. For example for bert uncased tokenizer these parameters are present here: https://huggingface.co/bert-base-uncased/blob/main/tokenizer_config.json\n",
    "\n",
    "1. [`load_dataset_kwargs`](https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/loading_methods#datasets.load_dataset)\n",
    "Keyword arguments from the `datasets.load_dataset method` which loads a dataset from the Hugging Face Hub, or a local dataset; mostly metadata for downloading, loading, caching the dataset\n",
    "\n",
    "1. [`model_config_kwargs`](https://github.com/huggingface/transformers/blob/074645e32acda6498f16203a8459bb597610f623/src/transformers/models/auto/configuration_auto.py#L515)\n",
    "This is a generic configuration class of the hugginface model, possible parameters depends on the model that is used. For example for bert uncased these parameters are present here: https://huggingface.co/bert-base-uncased/blob/main/config.json\n",
    "\n",
    "1. [`early_stopping_kwargs`](  \n",
    "https://github.com/PyTorchLightning/pytorch-lightning/blob/5d2d9b09df5359226fea6ad2722592839ac0ebc4/pytorch_lightning/callbacks/early_stopping.py#L35) \n",
    "Params defined in `__init__` of the `EarlyStopping` lightning callback; you can specify a metric to monitor and conditions to stop training when it stops improving \n",
    "1. [`dataloader_kwargs`](\n",
    "https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html#DataLoader) \n",
    "Defined in `__init__` of the torch `DataLoader` object which wraps an iterable around the Dataset to enable easy access to the sample; specify params such as num of workers, sampling or shuffling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18cd711",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exec_doc\n",
    "\n",
    "advanced_config = LightningAdvancedConfig(\n",
    "    finetune_last_n_layers=0,\n",
    "    datamodule_kwargs={\n",
    "        \"max_seq_length\": None,\n",
    "    },\n",
    "    task_train_kwargs={\n",
    "        \"max_epochs\": 1,\n",
    "        \"devices\": \"auto\",\n",
    "        \"accelerator\": \"cpu\",\n",
    "        \"deterministic\": True,\n",
    "    },\n",
    "    task_model_kwargs={\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"train_batch_size\": 64,\n",
    "        \"eval_batch_size\": 64,\n",
    "        \"use_scheduler\": True,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"adam_epsilon\": 1e-6,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"weight_decay\": 0.001,\n",
    "    },\n",
    "    early_stopping_kwargs=None,\n",
    "    model_config_kwargs={\"classifier_dropout\": 0.2},\n",
    "    tokenizer_kwargs={},\n",
    "    batch_encoding_kwargs={},\n",
    "    dataloader_kwargs={}\n",
    ")\n",
    "advanced_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0052806",
   "metadata": {},
   "source": [
    "Now we can run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e9ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exec_doc\n",
    "\n",
    "pipeline = LightningClassificationPipeline(\n",
    "    embedding_name_or_path=\"allegro/herbert-base-cased\",\n",
    "    dataset_name_or_path=\"data/polemo2_downsampled/\",\n",
    "    input_column_name=\"text\",\n",
    "    target_column_name=\"target\",\n",
    "    output_path=\".\",\n",
    "    config=advanced_config\n",
    ")\n",
    "\n",
    "metrics_adv_cfg = pipeline.run()\n",
    "\n",
    "# Converting metrics to DataFrame for better nb display\n",
    "\n",
    "metrics_adv_cfg = pd.DataFrame.from_dict(asdict(metrics_adv_cfg), orient=\"index\", columns=[\"values\"])\n",
    "metrics_adv_cfg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
