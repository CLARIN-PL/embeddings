[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CLARIN Embeddings",
    "section": "",
    "text": "pip install clarinpl-embeddings"
  },
  {
    "objectID": "index.html#example-with-polemo2-dataset",
    "href": "index.html#example-with-polemo2-dataset",
    "title": "CLARIN Embeddings",
    "section": "Example with polemo2 dataset",
    "text": "Example with polemo2 dataset\n\nLightning pipeline\nfrom embeddings.config.lightning_config import LightningBasicConfig\nfrom embeddings.pipeline.lightning_classification import LightningClassificationPipeline\n\nconfig = LightningBasicConfig(\n    learning_rate=0.01, max_epochs=1, max_seq_length=128, finetune_last_n_layers=0,\n    accelerator=\"cpu\"\n)\n\npipeline = LightningClassificationPipeline(\n    embedding_name_or_path=\"allegro/herbert-base-cased\",\n    dataset_name_or_path=\"clarin-pl/polemo2-official\",\n    input_column_name=[\"text\"],\n    target_column_name=\"target\",\n    load_dataset_kwargs={\n        \"train_domains\": [\"hotels\", \"medicine\"],\n        \"dev_domains\": [\"hotels\", \"medicine\"],\n        \"test_domains\": [\"hotels\", \"medicine\"],\n        \"text_cfg\": \"text\",\n    },\n    output_path=\".\",\n    config=config\n)\nYou can also define an Advanced config with populated keyword arguments. In general, the keywords are passed to the object when constructing specific pipelines. We can identify and trace the keyword arguments to find the possible arguments that can be set in the config kwargs.\nfrom embeddings.config.lightning_config import LightningAdvancedConfig\n\nconfig = LightningAdvancedConfig(\n    finetune_last_n_layers=0,\n    task_train_kwargs={\n        \"max_epochs\": 1,\n        \"devices\": \"auto\",\n        \"accelerator\": \"cpu\",\n        \"deterministic\": True,\n    },\n    task_model_kwargs={\n        \"learning_rate\": 5e-4,\n        \"use_scheduler\": False,\n        \"optimizer\": \"AdamW\",\n        \"adam_epsilon\": 1e-8,\n        \"warmup_steps\": 100,\n        \"weight_decay\": 0.0,\n    },\n    datamodule_kwargs={\n        \"downsample_train\": 0.01,\n        \"downsample_val\": 0.01,\n        \"downsample_test\": 0.05,\n    },\n    dataloader_kwargs={\"num_workers\": 0},\n)"
  },
  {
    "objectID": "index.html#transformers-embeddings",
    "href": "index.html#transformers-embeddings",
    "title": "CLARIN Embeddings",
    "section": "Transformers embeddings",
    "text": "Transformers embeddings\n\n\n\nTask\nOptimized Pipeline\n\n\n\n\nLightning Text Classification\nOptimizedLightingClassificationPipeline\n\n\nLightning Sequence Labeling\nOptimizedLightingSequenceLabelingPipeline"
  },
  {
    "objectID": "index.html#example-with-text-classification",
    "href": "index.html#example-with-text-classification",
    "title": "CLARIN Embeddings",
    "section": "Example with Text Classification",
    "text": "Example with Text Classification\nOptimized pipelines can be run via following snippet of code:\n\nfrom embeddings.config.lighting_config_space import LightingTextClassificationConfigSpace\nfrom embeddings.pipeline.lightning_hps_pipeline import OptimizedLightingClassificationPipeline\n\npipeline = OptimizedLightingClassificationPipeline(\n    config_space=LightingTextClassificationConfigSpace(\n        embedding_name_or_path=\"allegro/herbert-base-cased\"\n    ),\n    dataset_name_or_path=\"clarin-pl/polemo2-official\",\n    input_column_name=\"text\",\n    target_column_name=\"target\",\n).persisting(best_params_path=\"best_prams.yaml\", log_path=\"hps_log.pickle\")\ndf, metadata = pipeline.run()\n\nTraining model with obtained parameters\nAfter the parameters search process we can train model with best parameters found. But firstly we have to set output_path parameter, which is not automatically generated from OptimizedLightingClassificationPipeline.\nmetadata[\"output_path\"] = \".\"\nNow we are able to train the pipeline\nfrom embeddings.pipeline.lightning_classification import LightningClassificationPipeline\n\npipeline = LightningClassificationPipeline(**metadata)\nresults = pipeline.run()\n\n\nSelection of best embedding model\nInstead of performing search with single embedding model we can search with multiple embedding models via passing them as list to ConfigSpace.\npipeline = OptimizedLightingClassificationPipeline(\n    config_space=LightingTextClassificationConfigSpace(\n        embedding_name_or_path=[\"allegro/herbert-base-cased\", \"clarin-pl/roberta-polish-kgr10\"]\n    ),\n    dataset_name_or_path=\"clarin-pl/polemo2-official\",\n    input_column_name=\"text\",\n    target_column_name=\"target\",\n).persisting(best_params_path=\"best_prams.yaml\", log_path=\"hps_log.pickle\")"
  },
  {
    "objectID": "02_LEPISZCZE/lepiszcze.html",
    "href": "02_LEPISZCZE/lepiszcze.html",
    "title": "LEPISZCZE",
    "section": "",
    "text": "We recommend to read our NeurIPS paper (Augustyniak et al. 2022) where you can find our lessons learned from the process of designing and compiling LEPISZCZE benchmark.\n\n\nfrom pathlib import Path\n\nfrom embeddings.config.lightning_config import LightningBasicConfig\nfrom embeddings.pipeline.lightning_classification import LightningClassificationPipeline\n\nWe will start with training a text classifier using embeddings.pipeline.lightning_classification.LightningClassificationPipeline\n\n\nLightningClassificationPipeline\n\n LightningClassificationPipeline\n                                  (embedding_name_or_path:Union[str,pathli\n                                  b.Path], dataset_name_or_path:Union[str,\n                                  pathlib.Path], input_column_name:Union[s\n                                  tr,Sequence[str]],\n                                  target_column_name:str,\n                                  output_path:Union[str,pathlib.Path], eva\n                                  luation_filename:str='evaluation.json', \n                                  config:Union[embeddings.config.lightning\n                                  _config.LightningBasicConfig,embeddings.\n                                  config.lightning_config.LightningAdvance\n                                  dConfig]=LightningBasicConfig(use_schedu\n                                  ler=True, optimizer='Adam',\n                                  warmup_steps=100, learning_rate=0.0001,\n                                  adam_epsilon=1e-08, weight_decay=0.0,\n                                  finetune_last_n_layers=-1,\n                                  classifier_dropout=None,\n                                  max_seq_length=None, batch_size=32,\n                                  max_epochs=None,\n                                  early_stopping_monitor='val/Loss',\n                                  early_stopping_mode='min',\n                                  early_stopping_patience=3,\n                                  tokenizer_kwargs={},\n                                  batch_encoding_kwargs={},\n                                  dataloader_kwargs={}), devices:Union[Lis\n                                  t[int],str,int,NoneType]='auto', acceler\n                                  ator:Union[str,pytorch_lightning.acceler\n                                  ators.accelerator.Accelerator,NoneType]=\n                                  'auto', logging_config:embeddings.utils.\n                                  loggers.LightningLoggingConfig=Lightning\n                                  LoggingConfig(loggers_names=[],\n                                  tracking_project_name=None,\n                                  wandb_entity=None,\n                                  wandb_logger_kwargs={}), tokenizer_name_\n                                  or_path:Union[pathlib.Path,str,NoneType]\n                                  =None, predict_subset:embeddings.data.da\n                                  taset.LightingDataModuleSubset=<Lighting\n                                  DataModuleSubset.TEST: 'test'>, load_dat\n                                  aset_kwargs:Optional[Dict[str,Any]]=None\n                                  , model_checkpoint_kwargs:Optional[Dict[\n                                  str,Any]]=None)\n\nHelper class that provides a standard way to create an ABC using inheritance.\nWe want to store submission data in a specific directory.\n\nLEPISZCZE_SUBMISSIONS = Path(\"../lepiszcze-submissions\")\nLEPISZCZE_SUBMISSIONS.mkdir(exist_ok=True, parents=True)\n\nThen we create a pipeline object. We will use LightningClassificationPipeline with dataset related to sentiment analysis and a very small transfomer model.\nWe want only run training for testing purposes, hence it would be good no to generate to much greenhouse gases, hence we narrow max epochs to only 1. In the real traning code it would be good to customize traning procedure with more configuration.\n\nconfig = LightningBasicConfig(max_epochs=1)\n\n\npipeline = LightningClassificationPipeline(\n    dataset_name_or_path=\"clarin-pl/polemo2-official\",\n    embedding_name_or_path=\"hf-internal-testing/tiny-albert\",\n    input_column_name=\"text\",\n    target_column_name=\"target\",\n    output_path=\".\",\n    devices=\"auto\",\n    accelerator=\"cpu\",\n    config=config\n)\n\nNo config specified, defaulting to: polemo2-official/all_text\nFound cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n100%|██████████| 3/3 [00:00<00:00, 625.58it/s]\nLoading cached processed dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-2e61085076a665b0.arrow\nLoading cached processed dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-ac057aeafd577fd0.arrow\nLoading cached processed dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-502164b331496757.arrow\nLoading cached processed dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-13cbbe9129f685fa.arrow\nLoading cached processed dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-b1c5d1c8fe129da7.arrow\nLoading cached processed dataset at /root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70/cache-1f1e81ef3032c906.arrow\n\n\nIt took a couple of seconds but finally we have a pipeline objects ready and we need only run it.\n\nresults = pipeline.run()\n\nSome weights of the model checkpoint at hf-internal-testing/tiny-albert were not used when initializing AlbertForSequenceClassification: ['predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight']\n- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of AlbertForSequenceClassification were not initialized from the model checkpoint at hf-internal-testing/tiny-albert and are newly initialized: ['classifier.weight', 'albert.pooler.bias', 'classifier.bias', 'albert.pooler.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo config specified, defaulting to: polemo2-official/all_text\nFound cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n100%|██████████| 3/3 [00:00<00:00, 663.31it/s]\nGPU available: True, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\n/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1579: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n  rank_zero_warn(\n\n  | Name          | Type                            | Params\n------------------------------------------------------------------\n0 | model         | AlbertForSequenceClassification | 352 K \n1 | metrics       | MetricCollection                | 0     \n2 | train_metrics | MetricCollection                | 0     \n3 | val_metrics   | MetricCollection                | 0     \n4 | test_metrics  | MetricCollection                | 0     \n------------------------------------------------------------------\n352 K     Trainable params\n0         Non-trainable params\n352 K     Total params\n1.410     Total estimated model params size (MB)\n/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:623: UserWarning: Checkpoint directory /app/nbs/lepiszcze/checkpoints exists and is not empty.\n  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n\n\n                                                              \n\n\n/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n\n\nEpoch 0: 100%|██████████| 232/232 [00:37<00:00,  6.26it/s, loss=1.35, v_num=, train/BaseLR=0.000, train/LambdaLR=0.000, val/MulticlassAccuracy=0.369, val/MulticlassPrecision=0.0923, val/MulticlassRecall=0.250, val/MulticlassF1Score=0.135]\n\n\n/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n\n\nTesting:  92%|█████████▏| 24/26 [00:00<00:00, 34.68it/s]--------------------------------------------------------------------------------\nDATALOADER:0 TEST RESULTS\n{'test/Loss': 1.341328501701355,\n 'test/MulticlassAccuracy': 0.4134146273136139,\n 'test/MulticlassF1Score': 0.1462467610836029,\n 'test/MulticlassPrecision': 0.10335365682840347,\n 'test/MulticlassRecall': 0.25}\n--------------------------------------------------------------------------------\nTesting: 100%|██████████| 26/26 [00:00<00:00, 34.61it/s]\n\n\nRestoring states from the checkpoint path at /app/nbs/lepiszcze/checkpoints/epoch=0-step=205.ckpt\n\n\n\n\n\nLoaded model weights from checkpoint at /app/nbs/lepiszcze/checkpoints/epoch=0-step=205.ckpt\n/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n\n\nPredicting: 206it [00:00, ?it/s]\n\n\n/opt/conda/envs/embeddings/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/envs/embeddings/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/envs/embeddings/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\nAs we trained the model only for 1 epoch, the metrics are not too high and they are rather presented to show that the pipeline works.\n\nresults.metrics\n\n{'accuracy': 0.41341463414634144,\n 'f1_macro': 0.1462467644521139,\n 'f1_micro': 0.41341463414634144,\n 'f1_weighted': 0.2418422104842274,\n 'recall_macro': 0.25,\n 'recall_micro': 0.41341463414634144,\n 'recall_weighted': 0.41341463414634144,\n 'precision_macro': 0.10335365853658536,\n 'precision_micro': 0.41341463414634144,\n 'precision_weighted': 0.17091165972635333,\n 'classes': {0: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 118},\n  1: {'precision': 0.41341463414634144,\n   'recall': 1.0,\n   'f1': 0.5849870578084556,\n   'support': 339},\n  2: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 227},\n  3: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 136}}}\n\n\n\n\n\n\n\nReferences\n\nAugustyniak, Lukasz, Kamil Tagowski, Albert Sawczyn, Denis Janiak, Roman Bartusiak, Adrian Dominik Szymczak, Arkadiusz Janz, et al. 2022. “This Is the Way: Designing and Compiling LEPISZCZE, a Comprehensive NLP Benchmark for Polish.” In Thirty-Sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. https://openreview.net/forum?id=CZAd_6uiUx0."
  },
  {
    "objectID": "02_LEPISZCZE/submission.html",
    "href": "02_LEPISZCZE/submission.html",
    "title": "Submission",
    "section": "",
    "text": "import datasets\nimport numpy as np\n\nfrom embeddings.evaluator.evaluation_results import Predictions\nfrom embeddings.evaluator.leaderboard import get_dataset_task\nfrom embeddings.evaluator.submission import AveragedSubmission\nfrom embeddings.utils.utils import get_installed_packages\n\nIt is important to note that we not only enable to easily train models but we also prepare many helpers to create a submission to the leaderboard.\nWe start with a couple of names.\n\nDATASET_NAME = \"clarin-pl/polemo2-official\"\nTARGET_COLUMN_NAME = \"target\"\n\nWe want also gahter all hyper parameters for each submission. We collecct some of params for presentation purposes.\n\nhparams = {\"hparam_name_1\": 0.2, \"hparam_name_2\": 0.1}\n\nWe doing the same with python packages. We can use one of the helper methods.\n\npackages = get_installed_packages()\npackages[:10]\n\n['absl-py==1.4.0',\n 'aiofiles==22.1.0',\n 'aiohttp==3.8.4',\n 'aiosignal==1.3.1',\n 'aiosqlite==0.18.0',\n 'alembic==1.9.3',\n 'anyio==3.6.2',\n 'appdirs==1.4.4',\n 'argon2-cffi-bindings==21.2.0',\n 'argon2-cffi==21.3.0']\n\n\nThe next step is related to datasets and predictions.\n\ndataset = datasets.load_dataset(DATASET_NAME)\ndataset\n\nNo config specified, defaulting to: polemo2-official/all_text\nFound cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n100%|██████████| 3/3 [00:00<00:00, 828.48it/s]\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'target'],\n        num_rows: 6573\n    })\n    validation: Dataset({\n        features: ['text', 'target'],\n        num_rows: 823\n    })\n    test: Dataset({\n        features: ['text', 'target'],\n        num_rows: 820\n    })\n})\n\n\n\ny_true = np.array(dataset[\"test\"][TARGET_COLUMN_NAME])\ny_true[:10]\n\narray([1, 2, 2, 2, 2, 0, 0, 0, 1, 3])\n\n\nIt is important that we want to store not single prediction for each off object but we want to calcualted standard deviations for each object, hence we need more than one prediction.\n\npredictions = [\n    Predictions(\n        y_true=y_true, y_pred=np.random.randint(low=0, high=4, size=len(y_true))\n    )\n    for _ in range(5)\n]\n\nFinaly, we can create a submission, gathering all information.\n\nsubmission = AveragedSubmission.from_predictions(\n    submission_name=\"my-great-submission\",  # put your submission here!\n    dataset_name=DATASET_NAME,\n    dataset_version=dataset[\"train\"].info.version.version_str,\n    embedding_name=\"my-great-model\",  # put your embedding name here!\n    predictions=predictions,\n    hparams=hparams,\n    packages=packages,\n    task=get_dataset_task(DATASET_NAME),\n)\n\nWe can even save our submission.\n\nsubmission.save_json(\n    root=\"my-great-submission\",\n    filename=\"my-great-model.json\",\n    compress=False,\n)\n\n\n!ls my-great-submission\n\nmy-great-model.json  my-great-submission_predictions.json\n\n\n\n!cat my-great-submission/my-great-model.json\n\n{\n  \"submission_name\": \"my-great-submission\",\n  \"dataset_name\": \"clarin-pl/polemo2-official\",\n  \"dataset_version\": \"0.0.0\",\n  \"embedding_name\": \"my-great-model\",\n  \"hparams\": {\n    \"hparam_name_1\": 0.2,\n    \"hparam_name_2\": 0.1\n  },\n  \"packages\": [\n    \"absl-py==1.4.0\",\n    \"aiofiles==22.1.0\",\n    \"aiohttp==3.8.4\",\n    \"aiosignal==1.3.1\",\n    \"aiosqlite==0.18.0\",\n    \"alembic==1.9.3\",\n    \"anyio==3.6.2\",\n    \"appdirs==1.4.4\",\n    \"argon2-cffi-bindings==21.2.0\",\n    \"argon2-cffi==21.3.0\",\n    \"arrow==1.2.3\",\n    \"asttokens==2.2.1\",\n    \"astunparse==1.6.3\",\n    \"async-timeout==4.0.2\",\n    \"attrs==22.2.0\",\n    \"babel==2.11.0\",\n    \"backcall==0.2.0\",\n    \"beautifulsoup4==4.11.2\",\n    \"black==21.12b0\",\n    \"bleach==6.0.0\",\n    \"cachetools==5.3.0\",\n    \"catalogue==2.0.8\",\n    \"certifi==2022.12.7\",\n    \"cffi==1.15.1\",\n    \"charset-normalizer==3.0.1\",\n    \"click==8.0.4\",\n    \"cmaes==0.9.1\",\n    \"colorlog==6.7.0\",\n    \"comm==0.1.2\",\n    \"contourpy==1.0.7\",\n    \"coverage==6.2\",\n    \"cycler==0.11.0\",\n    \"datasets==2.9.0\",\n    \"debugpy==1.6.6\",\n    \"decorator==5.1.1\",\n    \"defusedxml==0.7.1\",\n    \"dill==0.3.6\",\n    \"docker-pycreds==0.4.0\",\n    \"execnb==0.1.5\",\n    \"executing==1.2.0\",\n    \"fastcore==1.5.28\",\n    \"fastjsonschema==2.16.2\",\n    \"filelock==3.9.0\",\n    \"fonttools==4.38.0\",\n    \"fqdn==1.5.1\",\n    \"frozenlist==1.3.3\",\n    \"fsspec==2023.1.0\",\n    \"future==0.18.3\",\n    \"ghapi==1.0.3\",\n    \"gitdb==4.0.10\",\n    \"gitpython==3.1.30\",\n    \"google-auth-oauthlib==0.4.6\",\n    \"google-auth==2.16.0\",\n    \"greenlet==2.0.2\",\n    \"grpcio==1.51.1\",\n    \"huggingface-hub==0.12.0\",\n    \"idna==3.4\",\n    \"importlib-metadata==6.0.0\",\n    \"iniconfig==2.0.0\",\n    \"ipykernel==6.21.2\",\n    \"ipython-genutils==0.2.0\",\n    \"ipython==8.10.0\",\n    \"isoduration==20.11.0\",\n    \"isort==5.10.1\",\n    \"jedi==0.18.2\",\n    \"jinja2==3.1.2\",\n    \"joblib==1.2.0\",\n    \"json5==0.9.11\",\n    \"jsonpointer==2.3\",\n    \"jsonschema==4.17.3\",\n    \"jupyter-client==8.0.2\",\n    \"jupyter-core==5.2.0\",\n    \"jupyter-events==0.5.0\",\n    \"jupyter-server-fileid==0.6.0\",\n    \"jupyter-server-terminals==0.4.4\",\n    \"jupyter-server-ydoc==0.6.1\",\n    \"jupyter-server==2.2.1\",\n    \"jupyter-ydoc==0.2.2\",\n    \"jupyterlab-pygments==0.2.2\",\n    \"jupyterlab-server==2.19.0\",\n    \"jupyterlab==3.6.1\",\n    \"kiwisolver==1.4.4\",\n    \"mako==1.2.4\",\n    \"markdown==3.4.1\",\n    \"markupsafe==2.1.2\",\n    \"matplotlib-inline==0.1.6\",\n    \"matplotlib==3.6.3\",\n    \"mistune==2.0.5\",\n    \"multidict==6.0.4\",\n    \"multiprocess==0.70.14\",\n    \"mypy-extensions==1.0.0\",\n    \"mypy==0.991\",\n    \"nbclassic==0.5.1\",\n    \"nbclient==0.7.2\",\n    \"nbconvert==7.2.9\",\n    \"nbdev==2.3.11\",\n    \"nbformat==5.7.3\",\n    \"nest-asyncio==1.5.6\",\n    \"notebook-shim==0.2.2\",\n    \"notebook==6.5.2\",\n    \"numpy==1.23.4\",\n    \"oauthlib==3.2.2\",\n    \"optuna==3.1.0\",\n    \"packaging==23.0\",\n    \"pandas==1.5.3\",\n    \"pandocfilters==1.5.0\",\n    \"parso==0.8.3\",\n    \"pastel==0.2.1\",\n    \"pathspec==0.11.0\",\n    \"pathtools==0.1.2\",\n    \"pexpect==4.8.0\",\n    \"pickleshare==0.7.5\",\n    \"pillow==9.4.0\",\n    \"pip==23.0\",\n    \"platformdirs==3.0.0\",\n    \"pluggy==1.0.0\",\n    \"poethepoet==0.11.0\",\n    \"prometheus-client==0.16.0\",\n    \"prompt-toolkit==3.0.36\",\n    \"protobuf==4.21.12\",\n    \"psutil==5.9.4\",\n    \"ptyprocess==0.7.0\",\n    \"pure-eval==0.2.2\",\n    \"py==1.11.0\",\n    \"pyarrow==11.0.0\",\n    \"pyasn1-modules==0.2.8\",\n    \"pyasn1==0.4.8\",\n    \"pycparser==2.21\",\n    \"pydantic==1.10.4\",\n    \"pydeprecate==0.3.1\",\n    \"pyflakes==2.4.0\",\n    \"pygments==2.14.0\",\n    \"pyparsing==3.0.9\",\n    \"pyrsistent==0.19.3\",\n    \"pytest-env==0.6.2\",\n    \"pytest==6.2.5\",\n    \"python-dateutil==2.8.2\",\n    \"python-json-logger==2.0.5\",\n    \"pytorch-lightning==1.5.4\",\n    \"pytz==2022.7.1\",\n    \"pyyaml==6.0\",\n    \"pyzmq==25.0.0\",\n    \"regex==2022.10.31\",\n    \"requests-oauthlib==1.3.1\",\n    \"requests==2.28.2\",\n    \"responses==0.18.0\",\n    \"rfc3339-validator==0.1.4\",\n    \"rfc3986-validator==0.1.1\",\n    \"rsa==4.9\",\n    \"sacremoses==0.0.53\",\n    \"scikit-learn==1.2.1\",\n    \"scipy==1.9.3\",\n    \"seaborn==0.12.2\",\n    \"send2trash==1.8.0\",\n    \"sentry-sdk==1.15.0\",\n    \"seqeval==1.2.2\",\n    \"setproctitle==1.3.2\",\n    \"setuptools==65.7.0\",\n    \"six==1.16.0\",\n    \"smmap==5.0.0\",\n    \"sniffio==1.3.0\",\n    \"soupsieve==2.3.2.post1\",\n    \"sqlalchemy==2.0.3\",\n    \"srsly==2.4.5\",\n    \"stack-data==0.6.2\",\n    \"tensorboard-data-server==0.7.0\",\n    \"tensorboard-plugin-wit==1.8.1\",\n    \"tensorboard==2.12.0\",\n    \"terminado==0.17.1\",\n    \"threadpoolctl==3.1.0\",\n    \"tinycss2==1.2.1\",\n    \"tokenizers==0.13.2\",\n    \"toml==0.10.2\",\n    \"tomli==1.2.3\",\n    \"torch==1.12.1+cu113\",\n    \"torchaudio==0.12.1+cu113\",\n    \"torchmetrics==0.11.1\",\n    \"torchvision==0.13.1+cu113\",\n    \"tornado==6.2\",\n    \"tqdm==4.64.1\",\n    \"traitlets==5.9.0\",\n    \"transformers==4.26.1\",\n    \"typer==0.7.0\",\n    \"types-docutils==0.19.1.3\",\n    \"types-pyyaml==6.0.12.6\",\n    \"types-requests==2.26.1\",\n    \"types-setuptools==67.2.0.1\",\n    \"typing-extensions==4.4.0\",\n    \"uri-template==1.2.0\",\n    \"urllib3==1.26.14\",\n    \"wandb==0.13.10\",\n    \"watchdog==2.2.1\",\n    \"wcwidth==0.2.6\",\n    \"webcolors==1.12\",\n    \"webencodings==0.5.1\",\n    \"websocket-client==1.5.1\",\n    \"werkzeug==2.2.2\",\n    \"wheel==0.38.4\",\n    \"xgboost==1.7.3\",\n    \"xxhash==3.2.0\",\n    \"y-py==0.5.5\",\n    \"yarl==1.8.2\",\n    \"ypy-websocket==0.8.2\",\n    \"zipp==3.13.0\"\n  ],\n  \"config\": null,\n  \"leaderboard_task_name\": \"Sentiment Analysis\",\n  \"metrics\": [\n    {\n      \"accuracy\": 0.2707317073170732,\n      \"f1_macro\": 0.26321469558380844,\n      \"f1_micro\": 0.2707317073170732,\n      \"f1_weighted\": 0.27628920268832863,\n      \"recall_macro\": 0.2803374383302084,\n      \"recall_micro\": 0.2707317073170732,\n      \"recall_weighted\": 0.2707317073170732,\n      \"precision_macro\": 0.26682952745742244,\n      \"precision_micro\": 0.2707317073170732,\n      \"precision_weighted\": 0.3012629928300943,\n      \"classes\": {\n        \"0\": {\n          \"precision\": 0.18226600985221675,\n          \"recall\": 0.3135593220338983,\n          \"f1\": 0.23052959501557632,\n          \"support\": 118\n        },\n        \"1\": {\n          \"precision\": 0.38961038961038963,\n          \"recall\": 0.26548672566371684,\n          \"f1\": 0.3157894736842105,\n          \"support\": 339\n        },\n        \"2\": {\n          \"precision\": 0.2864864864864865,\n          \"recall\": 0.23348017621145375,\n          \"f1\": 0.2572815533980583,\n          \"support\": 227\n        },\n        \"3\": {\n          \"precision\": 0.208955223880597,\n          \"recall\": 0.3088235294117647,\n          \"f1\": 0.24925816023738873,\n          \"support\": 136\n        }\n      }\n    },\n    {\n      \"accuracy\": 0.22439024390243903,\n      \"f1_macro\": 0.21294902138982494,\n      \"f1_micro\": 0.22439024390243903,\n      \"f1_weighted\": 0.24030286334056883,\n      \"recall_macro\": 0.2195084944832831,\n      \"recall_micro\": 0.22439024390243903,\n      \"recall_weighted\": 0.22439024390243903,\n      \"precision_macro\": 0.23332179622411883,\n      \"precision_micro\": 0.22439024390243903,\n      \"precision_weighted\": 0.2879731607716613,\n      \"classes\": {\n        \"0\": {\n          \"precision\": 0.1145374449339207,\n          \"recall\": 0.22033898305084745,\n          \"f1\": 0.15072463768115943,\n          \"support\": 118\n        },\n        \"1\": {\n          \"precision\": 0.42934782608695654,\n          \"recall\": 0.23303834808259588,\n          \"f1\": 0.30210325047801145,\n          \"support\": 339\n        },\n        \"2\": {\n          \"precision\": 0.265,\n          \"recall\": 0.23348017621145375,\n          \"f1\": 0.24824355971896955,\n          \"support\": 227\n        },\n        \"3\": {\n          \"precision\": 0.12440191387559808,\n          \"recall\": 0.19117647058823528,\n          \"f1\": 0.15072463768115943,\n          \"support\": 136\n        }\n      }\n    },\n    {\n      \"accuracy\": 0.25,\n      \"f1_macro\": 0.24275437640503172,\n      \"f1_micro\": 0.25,\n      \"f1_weighted\": 0.25883890927696696,\n      \"recall_macro\": 0.2591245000460524,\n      \"recall_micro\": 0.25,\n      \"recall_weighted\": 0.25,\n      \"precision_macro\": 0.25787615946976955,\n      \"precision_micro\": 0.25,\n      \"precision_weighted\": 0.3033807816571067,\n      \"classes\": {\n        \"0\": {\n          \"precision\": 0.15021459227467812,\n          \"recall\": 0.2966101694915254,\n          \"f1\": 0.1994301994301994,\n          \"support\": 118\n        },\n        \"1\": {\n          \"precision\": 0.40804597701149425,\n          \"recall\": 0.20943952802359883,\n          \"f1\": 0.27680311890838205,\n          \"support\": 339\n        },\n        \"2\": {\n          \"precision\": 0.3116279069767442,\n          \"recall\": 0.29515418502202645,\n          \"f1\": 0.30316742081447967,\n          \"support\": 227\n        },\n        \"3\": {\n          \"precision\": 0.16161616161616163,\n          \"recall\": 0.23529411764705882,\n          \"f1\": 0.19161676646706588,\n          \"support\": 136\n        }\n      }\n    },\n    {\n      \"accuracy\": 0.22560975609756098,\n      \"f1_macro\": 0.2120784669389474,\n      \"f1_micro\": 0.22560975609756098,\n      \"f1_weighted\": 0.23774855341302614,\n      \"recall_macro\": 0.21755784862350558,\n      \"recall_micro\": 0.22560975609756098,\n      \"recall_weighted\": 0.22560975609756098,\n      \"precision_macro\": 0.22335565084380252,\n      \"precision_micro\": 0.22560975609756098,\n      \"precision_weighted\": 0.26888250066206376,\n      \"classes\": {\n        \"0\": {\n          \"precision\": 0.10952380952380952,\n          \"recall\": 0.19491525423728814,\n          \"f1\": 0.1402439024390244,\n          \"support\": 118\n        },\n        \"1\": {\n          \"precision\": 0.3761904761904762,\n          \"recall\": 0.23303834808259588,\n          \"f1\": 0.2877959927140255,\n          \"support\": 339\n        },\n        \"2\": {\n          \"precision\": 0.27014218009478674,\n          \"recall\": 0.2511013215859031,\n          \"f1\": 0.26027397260273977,\n          \"support\": 227\n        },\n        \"3\": {\n          \"precision\": 0.13756613756613756,\n          \"recall\": 0.19117647058823528,\n          \"f1\": 0.16,\n          \"support\": 136\n        }\n      }\n    },\n    {\n      \"accuracy\": 0.23780487804878048,\n      \"f1_macro\": 0.22578338654073793,\n      \"f1_micro\": 0.23780487804878048,\n      \"f1_weighted\": 0.24833186785701405,\n      \"recall_macro\": 0.23551622569493447,\n      \"recall_micro\": 0.23780487804878048,\n      \"recall_weighted\": 0.23780487804878048,\n      \"precision_macro\": 0.2362003959319496,\n      \"precision_micro\": 0.23780487804878048,\n      \"precision_weighted\": 0.2804571653307677,\n      \"classes\": {\n        \"0\": {\n          \"precision\": 0.14883720930232558,\n          \"recall\": 0.2711864406779661,\n          \"f1\": 0.1921921921921922,\n          \"support\": 118\n        },\n        \"1\": {\n          \"precision\": 0.38164251207729466,\n          \"recall\": 0.23303834808259588,\n          \"f1\": 0.2893772893772894,\n          \"support\": 339\n        },\n        \"2\": {\n          \"precision\": 0.2932692307692308,\n          \"recall\": 0.2687224669603524,\n          \"f1\": 0.28045977011494255,\n          \"support\": 227\n        },\n        \"3\": {\n          \"precision\": 0.12105263157894737,\n          \"recall\": 0.16911764705882354,\n          \"f1\": 0.1411042944785276,\n          \"support\": 136\n        }\n      }\n    }\n  ],\n  \"metrics_avg\": {\n    \"accuracy\": 0.24170731707317072,\n    \"f1_macro\": 0.2313559893716701,\n    \"f1_micro\": 0.24170731707317072,\n    \"f1_weighted\": 0.2523022793151809,\n    \"recall_macro\": 0.2424089014355968,\n    \"recall_micro\": 0.24170731707317072,\n    \"recall_weighted\": 0.24170731707317072,\n    \"precision_macro\": 0.24351670598541258,\n    \"precision_micro\": 0.24170731707317072,\n    \"precision_weighted\": 0.28839132025033876,\n    \"classes\": {\n      \"0\": {\n        \"precision\": 0.14107581317739012,\n        \"recall\": 0.2593220338983051,\n        \"f1\": 0.18262410535163034,\n        \"support\": 118\n      },\n      \"1\": {\n        \"precision\": 0.39696743619532227,\n        \"recall\": 0.23480825958702067,\n        \"f1\": 0.2943738250323838,\n        \"support\": 339\n      },\n      \"2\": {\n        \"precision\": 0.28530516086544966,\n        \"recall\": 0.2563876651982379,\n        \"f1\": 0.269885255329838,\n        \"support\": 227\n      },\n      \"3\": {\n        \"precision\": 0.15071841370348832,\n        \"recall\": 0.21911764705882353,\n        \"f1\": 0.17854077177282832,\n        \"support\": 136\n      }\n    }\n  },\n  \"metrics_median\": {\n    \"accuracy\": 0.23780487804878048,\n    \"f1_macro\": 0.22578338654073793,\n    \"f1_micro\": 0.23780487804878048,\n    \"f1_weighted\": 0.24833186785701405,\n    \"recall_macro\": 0.23551622569493447,\n    \"recall_micro\": 0.23780487804878048,\n    \"recall_weighted\": 0.23780487804878048,\n    \"precision_macro\": 0.2362003959319496,\n    \"precision_micro\": 0.23780487804878048,\n    \"precision_weighted\": 0.2879731607716613,\n    \"classes\": {\n      \"0\": {\n        \"precision\": 0.14883720930232558,\n        \"recall\": 0.2711864406779661,\n        \"f1\": 0.1921921921921922\n      },\n      \"1\": {\n        \"precision\": 0.38961038961038963,\n        \"recall\": 0.23303834808259588,\n        \"f1\": 0.2893772893772894\n      },\n      \"2\": {\n        \"precision\": 0.2864864864864865,\n        \"recall\": 0.2511013215859031,\n        \"f1\": 0.26027397260273977\n      },\n      \"3\": {\n        \"precision\": 0.13756613756613756,\n        \"recall\": 0.19117647058823528,\n        \"f1\": 0.16\n      }\n    }\n  },\n  \"metrics_std\": {\n    \"accuracy\": 0.019270608073295843,\n    \"f1_macro\": 0.021716316631819502,\n    \"f1_micro\": 0.019270608073295843,\n    \"f1_weighted\": 0.015729439855574547,\n    \"recall_macro\": 0.026960608260889717,\n    \"recall_micro\": 0.019270608073295843,\n    \"recall_weighted\": 0.019270608073295843,\n    \"precision_macro\": 0.01812190850612583,\n    \"precision_micro\": 0.019270608073295843,\n    \"precision_weighted\": 0.014440252955113348,\n    \"classes\": {\n      \"0\": {\n        \"precision\": 0.02974980159131166,\n        \"recall\": 0.0503506806017388,\n        \"f1\": 0.037022245544002845\n      },\n      \"1\": {\n        \"precision\": 0.02174790344417702,\n        \"recall\": 0.019963332525886293,\n        \"f1\": 0.01496109107948936\n      },\n      \"2\": {\n        \"precision\": 0.018707782111451528,\n        \"recall\": 0.026136382333376784,\n        \"f1\": 0.022017697139155554\n      },\n      \"3\": {\n        \"precision\": 0.03624874525075996,\n        \"recall\": 0.05561079529761482,\n        \"f1\": 0.04384893965204796\n      }\n    }\n  },\n  \"averaged_over\": 5\n}"
  },
  {
    "objectID": "01_Tutorials/baseline_sklearn_models_tutorial.html",
    "href": "01_Tutorials/baseline_sklearn_models_tutorial.html",
    "title": "Baseline Sklearn-based models",
    "section": "",
    "text": "This notebook’s purpose is to show how to use the sklearn-like models pipeline for text classification.\nThe pipeline trains a selected classifier on a selected dataset, training a specified vectorizer previously. Then, it computes the text classification evaluation metrics and saves them in a JSON file in a specified path.\nApart from the “SklearnClassificationPipeline” class, all you need to import is a selected sklearn-like classifier and any sklearn vectorizer, like CountVectorizer or TfidfVectorizer."
  },
  {
    "objectID": "01_Tutorials/baseline_sklearn_models_tutorial.html#adaboost-model",
    "href": "01_Tutorials/baseline_sklearn_models_tutorial.html#adaboost-model",
    "title": "Baseline Sklearn-based models",
    "section": "AdaBoost model",
    "text": "AdaBoost model\n\nembeddings_kwargs = {\"max_features\": 10000, \"max_df\": 10}\n\nclassifier_kwargs = {\"n_estimators\": 100}\n\n\nevaluation_filename = \"adaboost_tfidf_evaluation.json\"  # default name: evaluation_filename.json\noutput_path = \".\"\n\nadaboost_tfidf_pipeline = SklearnClassificationPipeline(\n    dataset_name_or_path=DATASET_NAME,\n    input_column_name=\"text\",\n    target_column_name=\"target\",\n    output_path=output_path,\n    classifier=AdaBoostClassifier,\n    vectorizer=TfidfVectorizer,\n    evaluation_filename=evaluation_filename,\n    classifier_kwargs=classifier_kwargs,\n    embedding_kwargs=embeddings_kwargs,\n)\n\n\nadaboost_tfidf_result = adaboost_tfidf_pipeline.run()\n\nNo config specified, defaulting to: polemo2-official/all_text\nFound cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n100%|██████████| 3/3 [00:00<00:00, 746.58it/s]\n\n\n\nadaboost_tfidf_result\n\nTextClassificationEvaluationResults(accuracy=0.4695121951219512, f1_macro=0.2911453518233179, f1_micro=0.4695121951219512, f1_weighted=0.33223355506282337, recall_macro=0.3516011699415029, recall_micro=0.4695121951219512, recall_weighted=0.4695121951219512, precision_macro=0.3180081961736342, precision_micro=0.4695121951219512, precision_weighted=0.3020448062562097, classes={0: {'precision': 0.8305084745762712, 'recall': 0.4152542372881356, 'f1': 0.5536723163841808, 'support': 118}, 1: {'precision': 0.44152431011826543, 'recall': 0.9911504424778761, 'f1': 0.610909090909091, 'support': 339}, 2: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 227}, 3: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 136}}, data=Predictions(y_pred=array([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n       1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n       1, 1, 1, 1, 1, 1]), y_true=array([1, 2, 2, 2, 2, 0, 0, 0, 1, 3, 1, 0, 2, 2, 2, 1, 1, 1, 1, 3, 3, 2,\n       2, 3, 1, 2, 1, 1, 1, 1, 3, 2, 2, 1, 1, 3, 2, 1, 1, 2, 2, 1, 1, 2,\n       0, 1, 1, 0, 1, 1, 2, 0, 2, 2, 1, 2, 2, 1, 2, 1, 0, 3, 3, 1, 0, 3,\n       0, 1, 0, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 0, 2, 1, 1, 1, 1, 2, 3, 3,\n       2, 3, 1, 2, 2, 2, 1, 1, 2, 1, 3, 2, 1, 0, 1, 1, 2, 3, 3, 2, 2, 3,\n       1, 1, 1, 3, 1, 0, 2, 1, 0, 3, 0, 3, 3, 1, 2, 1, 1, 1, 2, 0, 2, 2,\n       1, 1, 0, 2, 1, 3, 3, 0, 2, 1, 1, 2, 1, 2, 1, 0, 1, 1, 1, 0, 1, 1,\n       1, 1, 2, 2, 2, 1, 0, 3, 1, 1, 1, 2, 3, 0, 1, 2, 1, 1, 1, 0, 1, 1,\n       3, 0, 1, 3, 1, 1, 0, 2, 2, 2, 1, 1, 2, 3, 1, 2, 2, 2, 1, 1, 2, 1,\n       0, 3, 2, 3, 1, 0, 2, 1, 2, 3, 0, 2, 0, 0, 1, 1, 0, 1, 1, 2, 1, 1,\n       1, 1, 3, 1, 1, 2, 2, 1, 1, 2, 2, 3, 2, 3, 1, 3, 1, 3, 1, 3, 2, 3,\n       2, 2, 2, 3, 1, 1, 1, 1, 2, 2, 2, 3, 1, 2, 1, 0, 2, 2, 2, 0, 1, 1,\n       1, 3, 0, 1, 3, 2, 3, 1, 0, 1, 3, 3, 2, 3, 1, 1, 1, 2, 1, 2, 0, 1,\n       0, 3, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 0, 2, 1, 2, 1, 1, 3,\n       3, 2, 3, 0, 3, 0, 1, 0, 3, 0, 3, 0, 1, 1, 1, 3, 1, 3, 1, 3, 0, 0,\n       1, 1, 3, 0, 2, 0, 1, 2, 2, 0, 1, 1, 3, 2, 1, 2, 3, 3, 2, 3, 2, 2,\n       1, 0, 2, 1, 1, 1, 1, 1, 1, 3, 3, 3, 2, 1, 1, 3, 2, 1, 3, 2, 0, 1,\n       2, 1, 1, 2, 2, 1, 2, 2, 0, 1, 1, 2, 2, 2, 0, 3, 1, 1, 0, 1, 1, 0,\n       2, 3, 0, 0, 0, 2, 3, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1,\n       2, 1, 2, 1, 3, 3, 2, 1, 1, 1, 0, 3, 3, 0, 1, 1, 3, 2, 1, 2, 3, 0,\n       2, 0, 1, 3, 1, 1, 2, 2, 2, 1, 1, 3, 1, 3, 1, 1, 0, 1, 1, 2, 1, 2,\n       1, 2, 1, 2, 1, 2, 2, 0, 0, 2, 2, 0, 1, 3, 1, 1, 2, 2, 2, 1, 3, 1,\n       1, 3, 0, 2, 0, 1, 1, 0, 2, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 3, 1,\n       0, 2, 2, 0, 1, 2, 0, 0, 1, 2, 1, 1, 1, 1, 2, 2, 1, 0, 3, 1, 2, 1,\n       3, 2, 1, 2, 1, 1, 0, 0, 1, 1, 1, 1, 3, 0, 3, 2, 2, 2, 3, 2, 0, 3,\n       1, 2, 1, 0, 2, 3, 1, 1, 0, 1, 3, 1, 0, 1, 2, 3, 1, 1, 3, 3, 1, 1,\n       0, 1, 0, 3, 1, 3, 1, 0, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1,\n       2, 1, 0, 0, 0, 1, 1, 2, 1, 1, 1, 0, 2, 3, 2, 0, 3, 2, 2, 1, 3, 2,\n       2, 1, 1, 3, 2, 2, 3, 2, 1, 1, 3, 1, 1, 0, 1, 1, 2, 0, 1, 1, 1, 1,\n       1, 2, 1, 2, 3, 3, 3, 2, 2, 3, 2, 1, 1, 0, 1, 1, 1, 2, 2, 3, 2, 1,\n       1, 0, 2, 1, 2, 1, 1, 3, 2, 1, 1, 1, 1, 1, 2, 1, 0, 2, 1, 2, 3, 1,\n       2, 3, 1, 1, 1, 2, 1, 2, 0, 1, 0, 3, 2, 3, 2, 1, 1, 3, 1, 1, 0, 1,\n       2, 3, 1, 3, 1, 1, 3, 1, 1, 3, 0, 0, 3, 2, 3, 1, 2, 1, 0, 2, 0, 2,\n       1, 1, 3, 1, 2, 3, 0, 1, 1, 3, 1, 0, 3, 2, 1, 1, 3, 2, 2, 1, 1, 1,\n       1, 3, 1, 0, 2, 0, 0, 1, 3, 3, 1, 3, 0, 1, 1, 0, 2, 0, 3, 1, 1, 2,\n       1, 2, 1, 1, 3, 1, 3, 3, 1, 1, 1, 1, 0, 1, 2, 2, 2, 1, 2, 0, 2, 2,\n       2, 1, 1, 2, 1, 2, 2, 1, 3, 0, 0, 3, 1, 1, 2, 1, 3, 0, 0, 2, 3, 2,\n       2, 0, 1, 3, 2, 2]), y_probabilities=None, names=None))"
  },
  {
    "objectID": "01_Tutorials/baseline_sklearn_models_tutorial.html#old-good-svm",
    "href": "01_Tutorials/baseline_sklearn_models_tutorial.html#old-good-svm",
    "title": "Baseline Sklearn-based models",
    "section": "Old good SVM",
    "text": "Old good SVM\n\nsvm_kwargs = {\"kernel\": \"linear\", \"C\": 0.6}\n\nevaluation_filename_svm_tdidf = \"svm_tfidf_evaluation.json\"\n\nsvm_tfidf_pipeline = SklearnClassificationPipeline(\n    dataset_name_or_path=DATASET_NAME,\n    input_column_name=\"text\",\n    target_column_name=\"target\",\n    output_path=output_path,\n    classifier=SVC,\n    vectorizer=TfidfVectorizer,\n    evaluation_filename=evaluation_filename_svm_tdidf,\n    classifier_kwargs=svm_kwargs,\n    embedding_kwargs=embeddings_kwargs,\n)\n\n\nsvm_tfidf_result = svm_tfidf_pipeline.run()\n\nNo config specified, defaulting to: polemo2-official/all_text\nFound cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n100%|██████████| 3/3 [00:00<00:00, 904.07it/s]"
  },
  {
    "objectID": "01_Tutorials/baseline_sklearn_models_tutorial.html#kaggle-winner-xgboost",
    "href": "01_Tutorials/baseline_sklearn_models_tutorial.html#kaggle-winner-xgboost",
    "title": "Baseline Sklearn-based models",
    "section": "Kaggle-winner XGBoost",
    "text": "Kaggle-winner XGBoost\n\nembeddings_kwargs = {\"max_features\": 10000}\n\nxgb_kwargs = {\"n_estimators\": 200, \"max_depth\": 7}\n\nevaluation_filename_xgb_tdidf = \"xgb_tfidf_evaluation.json\"\n\nxgb_tfidf_pipeline = SklearnClassificationPipeline(\n    dataset_name_or_path=DATASET_NAME,\n    input_column_name=\"text\",\n    target_column_name=\"target\",\n    output_path=output_path,\n    classifier=XGBClassifier,\n    vectorizer=TfidfVectorizer,\n    evaluation_filename=evaluation_filename_xgb_tdidf,\n    classifier_kwargs=xgb_kwargs,\n    embedding_kwargs=embeddings_kwargs,\n)\n\n\nxgb_tfidf_result = xgb_tfidf_pipeline.run()\n\nNo config specified, defaulting to: polemo2-official/all_text\nFound cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n100%|██████████| 3/3 [00:00<00:00, 674.94it/s]"
  },
  {
    "objectID": "01_Tutorials/validate_lightning_models_inference.html",
    "href": "01_Tutorials/validate_lightning_models_inference.html",
    "title": "LM-based models inference",
    "section": "",
    "text": "Warning\n\n\n\nWIP - This tutorial is a work in progress. We will update and validate the content in the coming weeks.\n\n\n\nfrom typing import Any, Dict\n\nimport pytorch_lightning as pl\nfrom embeddings.config.lightning_config import LightningAdvancedConfig\nfrom embeddings.defaults import DATASET_PATH, RESULTS_PATH\nfrom embeddings.model.lightning_module.text_classification import (\n    TextClassificationModule,\n)\nfrom embeddings.pipeline.hf_preprocessing_pipeline import (\n    HuggingFacePreprocessingPipeline,\n)\nfrom embeddings.pipeline.lightning_classification import LightningClassificationPipeline\nfrom embeddings.task.lightning_task.text_classification import TextClassificationTask\nfrom embeddings.utils.utils import build_output_path\n\n\nembedding_name_or_path = \"hf-internal-testing/tiny-albert\"\ndataset_name = \"clarin-pl/polemo2-official\"\n\ndataset_path = build_output_path(DATASET_PATH, embedding_name_or_path, dataset_name)\noutput_path = build_output_path(\".\", embedding_name_or_path, dataset_name, mkdirs=True)\n\n\nPreprocess and downsample data\n\ndef preprocess_data(path: str) -> Dict[str, Any]:\n    pipeline = HuggingFacePreprocessingPipeline(\n        dataset_name=dataset_name,\n        load_dataset_kwargs={\n            \"train_domains\": [\"hotels\", \"medicine\"],\n            \"dev_domains\": [\"hotels\", \"medicine\"],\n            \"test_domains\": [\"hotels\", \"medicine\"],\n            \"text_cfg\": \"text\",\n        },\n        persist_path=path,\n        sample_missing_splits=None,\n        ignore_test_subset=False,\n        downsample_splits=(0.01, 0.01, 0.01),\n        seed=441,\n    )\n    pipeline.run()\n\n    return {\n        \"dataset_name_or_path\": path,\n        \"input_column_name\": [\"text\"],\n        \"target_column_name\": \"target\",\n    }\n\n\ndataset_kwargs = preprocess_data(dataset_path)\n\nDownloading and preparing dataset polemo2-official/default to /home/runner/.cache/huggingface/datasets/clarin-pl___polemo2-official/default-e0c1ce6ddfd81769/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70...\nDataset polemo2-official downloaded and prepared to /home/runner/.cache/huggingface/datasets/clarin-pl___polemo2-official/default-e0c1ce6ddfd81769/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70. Subsequent calls will reuse this data.\n\n\nDownloading readme:   0%|          | 0.00/5.35k [00:00<?, ?B/s]Downloading readme: 100%|##########| 5.35k/5.35k [00:00<00:00, 6.12MB/s]\nUsing custom data configuration default-e0c1ce6ddfd81769\nDownloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\nDownloading data:   0%|          | 0.00/2.66M [00:00<?, ?B/s]\nDownloading data:  82%|########2 | 2.19M/2.66M [00:00<00:00, 20.8MB/s]Downloading data: 100%|##########| 2.66M/2.66M [00:00<00:00, 23.6MB/s]\nDownloading data files:  50%|#####     | 1/2 [00:00<00:00,  2.60it/s]\nDownloading data:   0%|          | 0.00/2.20M [00:00<?, ?B/s]Downloading data: 100%|##########| 2.20M/2.20M [00:00<00:00, 66.1MB/s]\nDownloading data files: 100%|##########| 2/2 [00:00<00:00,  3.05it/s]Downloading data files: 100%|##########| 2/2 [00:00<00:00,  2.97it/s]\nExtracting data files:   0%|          | 0/2 [00:00<?, ?it/s]Extracting data files: 100%|##########| 2/2 [00:00<00:00, 1954.93it/s]\nDownloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\nDownloading data:   0%|          | 0.00/320k [00:00<?, ?B/s]Downloading data: 100%|##########| 320k/320k [00:00<00:00, 49.4MB/s]\nDownloading data files:  50%|#####     | 1/2 [00:00<00:00,  4.31it/s]\nDownloading data:   0%|          | 0.00/281k [00:00<?, ?B/s]Downloading data: 100%|##########| 281k/281k [00:00<00:00, 32.7MB/s]\nDownloading data files: 100%|##########| 2/2 [00:00<00:00,  4.40it/s]Downloading data files: 100%|##########| 2/2 [00:00<00:00,  4.38it/s]\nExtracting data files:   0%|          | 0/2 [00:00<?, ?it/s]Extracting data files: 100%|##########| 2/2 [00:00<00:00, 2031.14it/s]\nDownloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\nDownloading data:   0%|          | 0.00/309k [00:00<?, ?B/s]Downloading data: 100%|##########| 309k/309k [00:00<00:00, 46.1MB/s]\nDownloading data files:  50%|#####     | 1/2 [00:00<00:00,  3.62it/s]\nDownloading data:   0%|          | 0.00/280k [00:00<?, ?B/s]Downloading data: 100%|##########| 280k/280k [00:00<00:00, 21.2MB/s]\nDownloading data files: 100%|##########| 2/2 [00:00<00:00,  3.50it/s]Downloading data files: 100%|##########| 2/2 [00:00<00:00,  3.51it/s]\nExtracting data files:   0%|          | 0/2 [00:00<?, ?it/s]Extracting data files: 100%|##########| 2/2 [00:00<00:00, 2177.16it/s]\nGenerating train split: 0 examples [00:00, ? examples/s]Generating train split: 2000 examples [00:00, 18297.12 examples/s]Generating train split: 4000 examples [00:00, 18985.77 examples/s]                                                                  Generating validation split: 0 examples [00:00, ? examples/s]                                                             Generating test split: 0 examples [00:00, ? examples/s]                                                         0%|          | 0/3 [00:00<?, ?it/s]100%|##########| 3/3 [00:00<00:00, 913.53it/s]\nFlattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]Flattening the indices: 100%|##########| 1/1 [00:00<00:00, 409.48ba/s]\nSaving the dataset (0/1 shards):   0%|          | 0/58 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|##########| 58/58 [00:00<00:00, 17984.00 examples/s]                                                                                          Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]Flattening the indices: 100%|##########| 1/1 [00:00<00:00, 889.57ba/s]\nSaving the dataset (0/1 shards):   0%|          | 0/8 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|##########| 8/8 [00:00<00:00, 4058.84 examples/s]                                                                                       Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]Flattening the indices: 100%|##########| 1/1 [00:00<00:00, 902.19ba/s]\nSaving the dataset (0/1 shards):   0%|          | 0/8 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|##########| 8/8 [00:00<00:00, 4047.58 examples/s]                                                                                       \n\n\n\n\n\npreprocess_data\n\n preprocess_data (path:str)\n\n\n\nTrain simple downsampled pipeline\n\nconfig = LightningAdvancedConfig(\n    finetune_last_n_layers=0,\n    task_train_kwargs={\"max_epochs\": 1, \"deterministic\": True,},\n    task_model_kwargs={\n        \"learning_rate\": 5e-4,\n        \"train_batch_size\": 32,\n        \"eval_batch_size\": 32,\n        \"use_scheduler\": True,\n        \"optimizer\": \"AdamW\",\n        \"adam_epsilon\": 1e-8,\n        \"warmup_steps\": 100,\n        \"weight_decay\": 0.0,\n    },\n    datamodule_kwargs={\"max_seq_length\": 64,},\n    early_stopping_kwargs={\"monitor\": \"val/Loss\", \"mode\": \"min\", \"patience\": 3,},\n    tokenizer_kwargs={},\n    batch_encoding_kwargs={},\n    dataloader_kwargs={},\n    model_config_kwargs={},\n)\n\n\npipeline = LightningClassificationPipeline(\n    embedding_name_or_path=embedding_name_or_path,\n    output_path=output_path,\n    config=config,\n    devices=\"auto\",\n    accelerator=\"cpu\",\n    **dataset_kwargs\n)\nresult = pipeline.run()\n\nValidation sanity check: 0it [00:00, ?it/s]Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]Validation sanity check: 100%|##########| 1/1 [00:00<00:00,  3.78it/s]                                                                      Training: 0it [00:00, ?it/s]Training:   0%|          | 0/3 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/3 [00:00<?, ?it/s] Epoch 0:  33%|###3      | 1/3 [00:00<00:00,  3.34it/s]Epoch 0:  33%|###3      | 1/3 [00:00<00:00,  3.34it/s, loss=1.39, v_num=, train/BaseLR=0.000, train/LambdaLR=0.000]Epoch 0:  67%|######6   | 2/3 [00:00<00:00,  3.51it/s, loss=1.39, v_num=, train/BaseLR=0.000, train/LambdaLR=0.000]Epoch 0:  67%|######6   | 2/3 [00:00<00:00,  3.50it/s, loss=1.39, v_num=, train/BaseLR=5e-6, train/LambdaLR=5e-6]  \nValidating: 0it [00:00, ?it/s]\nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\nValidating: 100%|##########| 1/1 [00:00<00:00,  8.86it/s]Epoch 0: 100%|##########| 3/3 [00:00<00:00,  4.35it/s, loss=1.39, v_num=, train/BaseLR=5e-6, train/LambdaLR=5e-6, val/MulticlassAccuracy=0.250, val/MulticlassPrecision=0.0625, val/MulticlassRecall=0.250, val/MulticlassF1Score=0.100]\n                                                         Epoch 0: 100%|##########| 3/3 [00:00<00:00,  4.26it/s, loss=1.39, v_num=, train/BaseLR=5e-6, train/LambdaLR=5e-6, val/MulticlassAccuracy=0.250, val/MulticlassPrecision=0.0625, val/MulticlassRecall=0.250, val/MulticlassF1Score=0.100]\nTesting: 0it [00:00, ?it/s]Testing: 100%|##########| 1/1 [00:00<00:00,  8.32it/s]--------------------------------------------------------------------------------\nDATALOADER:0 TEST RESULTS\n{'test/Loss': 1.387669324874878,\n 'test/MulticlassAccuracy': 0.125,\n 'test/MulticlassF1Score': 0.0555555559694767,\n 'test/MulticlassPrecision': 0.03125,\n 'test/MulticlassRecall': 0.25}\n--------------------------------------------------------------------------------\nTesting: 100%|##########| 1/1 [00:00<00:00,  7.95it/s]\nPredicting: 2it [00:00, ?it/s]Predicting: 2it [00:00, ?it/s]\n\n\n  0%|          | 0/1 [00:00<?, ?ba/s]100%|##########| 1/1 [00:00<00:00, 30.19ba/s]\n  0%|          | 0/1 [00:00<?, ?ba/s]100%|##########| 1/1 [00:00<00:00, 156.22ba/s]\n  0%|          | 0/1 [00:00<?, ?ba/s]100%|##########| 1/1 [00:00<00:00, 132.78ba/s]\nCasting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]Casting the dataset: 100%|##########| 1/1 [00:00<00:00, 914.19ba/s]\nCasting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]Casting the dataset: 100%|##########| 1/1 [00:00<00:00, 1231.08ba/s]\nCasting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]Casting the dataset: 100%|##########| 1/1 [00:00<00:00, 1206.30ba/s]\nDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]Downloading builder script: 100%|##########| 4.20k/4.20k [00:00<00:00, 5.47MB/s]\nDownloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]Downloading builder script: 100%|##########| 6.77k/6.77k [00:00<00:00, 8.66MB/s]\nDownloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]Downloading builder script: 100%|##########| 7.36k/7.36k [00:00<00:00, 8.22MB/s]\nDownloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]Downloading builder script: 100%|##########| 7.55k/7.55k [00:00<00:00, 8.93MB/s]\n\n\n\n\nLoad model from chechpoint automatically generated with Trainer\n\nckpt_path = output_path / \"checkpoints\" / \"epoch=0-step=1.ckpt\"\n\ntask_from_ckpt = TextClassificationTask.from_checkpoint(\n    checkpoint_path=ckpt_path, output_path=output_path,\n)\n\n\nAlternatively we can load the model\n\nmodel_from_ckpt = TextClassificationModule.load_from_checkpoint(str(ckpt_path))\n\nThe warning appears when loading the model, however, it was validated that the loaded weights are the same as the weights that are being saved. The reason for this is that when the model_state_dict keys are loaded from the cached huggingface model some of them (cls.(…)) do not match the keys from the state_dict of the model weights that are saved.\nhttps://github.com/CLARIN-PL/embeddings/issues/225\n\n\n\nUse task from checkpoint for predictions\nreturn_names needs to be set to False since it uses the datamodule to retrieves the names while the datamodule is not loaded to Trainer in the LightningTask since we have not fitted it yet.\n\ntest_dataloader = pipeline.datamodule.test_dataloader()\npreds = task_from_ckpt.predict(test_dataloader)\npreds\n\nPredicting: 0it [00:00, ?it/s]Predicting: 100%|##########| 1/1 [00:00<00:00, 20.62it/s]\n\n\nPredictions(y_pred=array([3, 3, 3, 3, 3, 3, 3, 3]), y_true=array([1, 0, 1, 1, 3, 2, 1, 0]), y_probabilities=array([[0.25111997, 0.24871589, 0.2487349 , 0.25142926],\n       [0.25111797, 0.24872528, 0.24873577, 0.251421  ],\n       [0.2511297 , 0.24871656, 0.24872771, 0.25142604],\n       [0.25112182, 0.24871814, 0.24873568, 0.25142437],\n       [0.25112838, 0.24871287, 0.24872996, 0.25142872],\n       [0.25112715, 0.24872063, 0.24873313, 0.25141904],\n       [0.25112528, 0.24871972, 0.24873224, 0.2514228 ],\n       [0.25112545, 0.24872461, 0.24872872, 0.25142124]], dtype=float32), names=array(['zero', 'minus', 'plus', 'amb'], dtype='<U5'))\n\n\nAlternatively we can implicitly assign the datamodule to Trainer in LightningTask\n\ntask_from_ckpt.trainer.datamodule = pipeline.datamodule\npreds_with_names = task_from_ckpt.predict(test_dataloader, return_names=True)\npreds_with_names\n\nPredicting: 0it [00:00, ?it/s]Predicting: 100%|##########| 1/1 [00:00<00:00, 116.47it/s]\n\n\n/home/runner/work/embeddings/embeddings/.venv/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n  rank_zero_deprecation(\n/home/runner/work/embeddings/embeddings/.venv/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n  rank_zero_deprecation(\n\n\nPredictions(y_pred=array([3, 3, 3, 3, 3, 3, 3, 3]), y_true=array([1, 0, 1, 1, 3, 2, 1, 0]), y_probabilities=array([[0.25111997, 0.24871589, 0.2487349 , 0.25142926],\n       [0.25111797, 0.24872528, 0.24873577, 0.251421  ],\n       [0.2511297 , 0.24871656, 0.24872771, 0.25142604],\n       [0.25112182, 0.24871814, 0.24873568, 0.25142437],\n       [0.25112838, 0.24871287, 0.24872996, 0.25142872],\n       [0.25112715, 0.24872063, 0.24873313, 0.25141904],\n       [0.25112528, 0.24871972, 0.24873224, 0.2514228 ],\n       [0.25112545, 0.24872461, 0.24872872, 0.25142124]], dtype=float32), names=array(['zero', 'minus', 'plus', 'amb'], dtype='<U5'))\n\n\nWe can also use previosly loaded lightning model (LightningModule) outside of the task and get the predictions. To do this we also need to intitialize a Trainer.\n\ntrainer = pl.Trainer(default_root_dir=str(output_path))\npreds_from_model = trainer.predict(model_from_ckpt, dataloaders=test_dataloader)\npreds_from_model\n\nPredicting: 0it [00:00, ?it/s]Predicting: 100%|##########| 1/1 [00:00<00:00, 60.64it/s]\n\n\n[(tensor([[0.0135, 0.0039, 0.0040, 0.0148],\n          [0.0135, 0.0039, 0.0040, 0.0147],\n          [0.0136, 0.0039, 0.0040, 0.0147],\n          [0.0135, 0.0039, 0.0040, 0.0147],\n          [0.0136, 0.0039, 0.0040, 0.0148],\n          [0.0136, 0.0039, 0.0040, 0.0147],\n          [0.0136, 0.0039, 0.0040, 0.0147],\n          [0.0135, 0.0039, 0.0040, 0.0147]]),\n  tensor([3, 3, 3, 3, 3, 3, 3, 3]))]"
  },
  {
    "objectID": "01_Tutorials/tutorial_how_to_use_config_space.html",
    "href": "01_Tutorials/tutorial_how_to_use_config_space.html",
    "title": "How to use our configs?",
    "section": "",
    "text": "Two types of config are defined in our library: BasicConfig and AdvancedConfig."
  },
  {
    "objectID": "01_Tutorials/tutorial_how_to_use_config_space.html#basicconfig",
    "href": "01_Tutorials/tutorial_how_to_use_config_space.html#basicconfig",
    "title": "How to use our configs?",
    "section": "BasicConfig",
    "text": "BasicConfig\n\nallows for easy use of the most common parameters in the pipeline.\n\n\n\nLightningBasicConfig\n\n LightningBasicConfig (use_scheduler:bool=True, optimizer:str='Adam',\n                       warmup_steps:int=100, learning_rate:float=0.0001,\n                       adam_epsilon:float=1e-08, weight_decay:float=0.0,\n                       finetune_last_n_layers:int=-1,\n                       classifier_dropout:Optional[float]=None,\n                       max_seq_length:Optional[int]=None,\n                       batch_size:int=32, max_epochs:Optional[int]=None,\n                       early_stopping_monitor:str='val/Loss',\n                       early_stopping_mode:str='min',\n                       early_stopping_patience:int=3)"
  },
  {
    "objectID": "01_Tutorials/tutorial_how_to_use_config_space.html#advancedconfig",
    "href": "01_Tutorials/tutorial_how_to_use_config_space.html#advancedconfig",
    "title": "How to use our configs?",
    "section": "AdvancedConfig",
    "text": "AdvancedConfig\n\nthe objects defined in our pipelines are constructed in a way that they can be further paramatrized with keyword arguments. These arguments can be utilized by constructing the AdvancedConfig.\n\n\n\nLightningAdvancedConfig\n\n LightningAdvancedConfig (finetune_last_n_layers:int,\n                          task_model_kwargs:Dict[str,Any],\n                          datamodule_kwargs:Dict[str,Any],\n                          task_train_kwargs:Dict[str,Any],\n                          model_config_kwargs:Dict[str,Any],\n                          early_stopping_kwargs:Dict[str,Any],\n                          tokenizer_kwargs:Dict[str,Any],\n                          batch_encoding_kwargs:Dict[str,Any],\n                          dataloader_kwargs:Dict[str,Any])\n\nIn summary, the BasicConfig takes arguments and automatically assign them into proper keyword group, while the AdvancedConfig takes as the input keyword groups that should be already correctly mapped.\nThe list of available config can be found below."
  },
  {
    "objectID": "01_Tutorials/tutorial_how_to_use_config_space.html#running-pipeline-with-basicconfig",
    "href": "01_Tutorials/tutorial_how_to_use_config_space.html#running-pipeline-with-basicconfig",
    "title": "How to use our configs?",
    "section": "Running pipeline with BasicConfig",
    "text": "Running pipeline with BasicConfig\nLet’s run example pipeline on polemo2 dataset\nBut first we downsample our dataset due to hardware limitations for that purpose we use HuggingFacePreprocessingPipeline\n\nfrom embeddings.pipeline.hf_preprocessing_pipeline import HuggingFacePreprocessingPipeline\n\n\n\nHuggingFacePreprocessingPipeline\n\n HuggingFacePreprocessingPipeline (dataset_name:str, persist_path:str, sam\n                                   ple_missing_splits:Optional[Tuple[Optio\n                                   nal[float],Optional[float]]]=None, down\n                                   sample_splits:Optional[Tuple[Optional[f\n                                   loat],Optional[float],Optional[float]]]\n                                   =None, ignore_test_subset:bool=False,\n                                   seed:int=441, load_dataset_kwargs:Optio\n                                   nal[Dict[str,Any]]=None)\n\nPreprocessing pipeline dedicated to work with HuggingFace datasets.\nThen we need to use run method\n\n\n\nPreprocessingPipeline.run\n\n PreprocessingPipeline.run ()\n\n\nprepocessing = HuggingFacePreprocessingPipeline(\n    dataset_name=\"clarin-pl/polemo2-official\",\n    persist_path=\"data/polemo2_downsampled\",\n    downsample_splits=(0.001, 0.005, 0.005)\n)\nprepocessing.run()\n\nDownloading and preparing dataset polemo2-official/all_text (download: 6.37 MiB, generated: 6.30 MiB, post-processed: Unknown size, total: 12.68 MiB) to /home/runner/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70...\nDataset polemo2-official downloaded and prepared to /home/runner/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70. Subsequent calls will reuse this data.\n\n\nDownloading builder script:   0%|          | 0.00/5.90k [00:00<?, ?B/s]Downloading builder script: 100%|##########| 5.90k/5.90k [00:00<00:00, 5.38MB/s]\nDownloading metadata:   0%|          | 0.00/23.4k [00:00<?, ?B/s]Downloading metadata: 100%|##########| 23.4k/23.4k [00:00<00:00, 11.9MB/s]\nDownloading readme:   0%|          | 0.00/5.35k [00:00<?, ?B/s]Downloading readme: 100%|##########| 5.35k/5.35k [00:00<00:00, 6.41MB/s]\nNo config specified, defaulting to: polemo2-official/all_text\nDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\nDownloading data:   0%|          | 0.00/5.37M [00:00<?, ?B/s]Downloading data: 100%|##########| 5.37M/5.37M [00:00<00:00, 59.1MB/s]\nDownloading data files: 100%|##########| 1/1 [00:00<00:00,  2.30it/s]Downloading data files: 100%|##########| 1/1 [00:00<00:00,  2.30it/s]\nExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|##########| 1/1 [00:00<00:00, 1623.18it/s]\nDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\nDownloading data:   0%|          | 0.00/663k [00:00<?, ?B/s]Downloading data: 100%|##########| 663k/663k [00:00<00:00, 58.9MB/s]\nDownloading data files: 100%|##########| 1/1 [00:00<00:00,  3.90it/s]Downloading data files: 100%|##########| 1/1 [00:00<00:00,  3.90it/s]\nExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|##########| 1/1 [00:00<00:00, 1684.46it/s]\nDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\nDownloading data:   0%|          | 0.00/649k [00:00<?, ?B/s]Downloading data: 100%|##########| 649k/649k [00:00<00:00, 48.2MB/s]\nDownloading data files: 100%|##########| 1/1 [00:00<00:00,  3.86it/s]Downloading data files: 100%|##########| 1/1 [00:00<00:00,  3.86it/s]\nExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|##########| 1/1 [00:00<00:00, 1737.49it/s]\nGenerating train split:   0%|          | 0/6573 [00:00<?, ? examples/s]Generating train split:  30%|###       | 2000/6573 [00:00<00:00, 18783.77 examples/s]Generating train split:  61%|######1   | 4011/6573 [00:00<00:00, 19517.69 examples/s]Generating train split:  92%|#########1| 6033/6573 [00:00<00:00, 19828.30 examples/s]                                                                                     Generating validation split:   0%|          | 0/823 [00:00<?, ? examples/s]                                                                           Generating test split:   0%|          | 0/820 [00:00<?, ? examples/s]                                                                       0%|          | 0/3 [00:00<?, ?it/s]100%|##########| 3/3 [00:00<00:00, 941.98it/s]\nFlattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]Flattening the indices: 100%|##########| 1/1 [00:00<00:00, 823.38ba/s]\nSaving the dataset (0/1 shards):   0%|          | 0/7 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|##########| 7/7 [00:00<00:00, 3423.12 examples/s]                                                                                       Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]Flattening the indices: 100%|##########| 1/1 [00:00<00:00, 992.73ba/s]\nSaving the dataset (0/1 shards):   0%|          | 0/5 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|##########| 5/5 [00:00<00:00, 2650.26 examples/s]                                                                                       Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]Flattening the indices: 100%|##########| 1/1 [00:00<00:00, 1017.54ba/s]\nSaving the dataset (0/1 shards):   0%|          | 0/5 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|##########| 5/5 [00:00<00:00, 2642.25 examples/s]                                                                                       \n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'target'],\n        num_rows: 7\n    })\n    validation: Dataset({\n        features: ['text', 'target'],\n        num_rows: 5\n    })\n    test: Dataset({\n        features: ['text', 'target'],\n        num_rows: 5\n    })\n})\n\n\nWe have now our data prepared locally, now we need to define our pipeline.\nLet’s start from config. We will use parameters from clarin-pl/lepiszcze-allegro__herbert-base-cased-polemo2, which configuration was obtained from extensive hyperparmeter search.\n\n\n\n\n\n\nWarning\n\n\n\nDue to hardware limitation we limit parmeter max_epochs to 1 and we leave early stopping configuration parameters as defaults\n\n\n\n\n\nLightningBasicConfig\n\n LightningBasicConfig (use_scheduler:bool=True, optimizer:str='Adam',\n                       warmup_steps:int=100, learning_rate:float=0.0001,\n                       adam_epsilon:float=1e-08, weight_decay:float=0.0,\n                       finetune_last_n_layers:int=-1,\n                       classifier_dropout:Optional[float]=None,\n                       max_seq_length:Optional[int]=None,\n                       batch_size:int=32, max_epochs:Optional[int]=None,\n                       early_stopping_monitor:str='val/Loss',\n                       early_stopping_mode:str='min',\n                       early_stopping_patience:int=3)\n\n\nconfig = LightningBasicConfig(\n        use_scheduler=True,\n        optimizer=\"Adam\",\n        warmup_steps=100,\n        learning_rate=0.001,\n        adam_epsilon=1e-06,\n        weight_decay=0.001,\n        finetune_last_n_layers=3,\n        classifier_dropout=0.2,\n        max_seq_length=None,\n        batch_size=64,\n        max_epochs=1,\n)\nconfig\n\nLightningBasicConfig(use_scheduler=True, optimizer='Adam', warmup_steps=100, learning_rate=0.001, adam_epsilon=1e-06, weight_decay=0.001, finetune_last_n_layers=3, classifier_dropout=0.2, max_seq_length=None, batch_size=64, max_epochs=1, early_stopping_monitor='val/Loss', early_stopping_mode='min', early_stopping_patience=3, tokenizer_kwargs={}, batch_encoding_kwargs={}, dataloader_kwargs={})\n\n\nNow we define pipeline dedicated for text classification LightningClassificationPipeline\n\nfrom embeddings.pipeline.lightning_classification import LightningClassificationPipeline\n\n\n\n\nLightningClassificationPipeline\n\n LightningClassificationPipeline\n                                  (embedding_name_or_path:Union[str,pathli\n                                  b.Path], dataset_name_or_path:Union[str,\n                                  pathlib.Path], input_column_name:Union[s\n                                  tr,Sequence[str]],\n                                  target_column_name:str,\n                                  output_path:Union[str,pathlib.Path], eva\n                                  luation_filename:str='evaluation.json', \n                                  config:Union[embeddings.config.lightning\n                                  _config.LightningBasicConfig,embeddings.\n                                  config.lightning_config.LightningAdvance\n                                  dConfig]=LightningBasicConfig(use_schedu\n                                  ler=True, optimizer='Adam',\n                                  warmup_steps=100, learning_rate=0.0001,\n                                  adam_epsilon=1e-08, weight_decay=0.0,\n                                  finetune_last_n_layers=-1,\n                                  classifier_dropout=None,\n                                  max_seq_length=None, batch_size=32,\n                                  max_epochs=None,\n                                  early_stopping_monitor='val/Loss',\n                                  early_stopping_mode='min',\n                                  early_stopping_patience=3,\n                                  tokenizer_kwargs={},\n                                  batch_encoding_kwargs={},\n                                  dataloader_kwargs={}), devices:Union[Lis\n                                  t[int],str,int,NoneType]='auto', acceler\n                                  ator:Union[str,pytorch_lightning.acceler\n                                  ators.accelerator.Accelerator,NoneType]=\n                                  'auto', logging_config:embeddings.utils.\n                                  loggers.LightningLoggingConfig=Lightning\n                                  LoggingConfig(loggers_names=[],\n                                  tracking_project_name=None,\n                                  wandb_entity=None,\n                                  wandb_logger_kwargs={}), tokenizer_name_\n                                  or_path:Union[pathlib.Path,str,NoneType]\n                                  =None, predict_subset:embeddings.data.da\n                                  taset.LightingDataModuleSubset=<Lighting\n                                  DataModuleSubset.TEST: 'test'>, load_dat\n                                  aset_kwargs:Optional[Dict[str,Any]]=None\n                                  , model_checkpoint_kwargs:Optional[Dict[\n                                  str,Any]]=None)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nfrom dataclasses import asdict # For metrics conversion\nimport pandas as pd  # For metrics conversion\n\n\npipeline = LightningClassificationPipeline(\n    embedding_name_or_path=\"hf-internal-testing/tiny-albert\",\n    dataset_name_or_path=\"data/polemo2_downsampled/\",\n    input_column_name=\"text\",\n    target_column_name=\"target\",\n    output_path=\".\",\n    devices=\"auto\",\n    accelerator=\"cpu\",\n    config=config\n)\n\nDownloading (…)okenizer_config.json:   0%|          | 0.00/422 [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|##########| 422/422 [00:00<00:00, 92.0kB/s]\nDownloading (…)\"spiece.model\";:   0%|          | 0.00/321k [00:00<?, ?B/s]Downloading (…)\"spiece.model\";: 100%|##########| 321k/321k [00:00<00:00, 27.7MB/s]\nDownloading (…)/main/tokenizer.json:   0%|          | 0.00/478k [00:00<?, ?B/s]Downloading (…)/main/tokenizer.json: 100%|##########| 478k/478k [00:00<00:00, 33.1MB/s]\nDownloading (…)cial_tokens_map.json:   0%|          | 0.00/244 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|##########| 244/244 [00:00<00:00, 96.4kB/s]\n  0%|          | 0/1 [00:00<?, ?ba/s]100%|##########| 1/1 [00:00<00:00, 80.08ba/s]\n  0%|          | 0/1 [00:00<?, ?ba/s]100%|##########| 1/1 [00:00<00:00, 127.37ba/s]\n  0%|          | 0/1 [00:00<?, ?ba/s]100%|##########| 1/1 [00:00<00:00, 173.12ba/s]\nCasting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]Casting the dataset: 100%|##########| 1/1 [00:00<00:00, 1023.25ba/s]\nCasting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]Casting the dataset: 100%|##########| 1/1 [00:00<00:00, 1266.78ba/s]\nCasting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]Casting the dataset: 100%|##########| 1/1 [00:00<00:00, 1289.76ba/s]\n\n\nSimilarly as with HuggingFacePreprocessingPipeline we use run method\n\n\n\nLightningPipeline.run\n\n LightningPipeline.run (run_name:Optional[str]=None)\n\n\nmetrics = pipeline.run()\n\nValidation sanity check: 0it [00:00, ?it/s]Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]                                                              Training: 0it [00:00, ?it/s]Training:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] Epoch 0:  50%|#####     | 1/2 [00:00<00:00,  1.51it/s]Epoch 0:  50%|#####     | 1/2 [00:00<00:00,  1.51it/s, loss=1.39, v_num=, train/BaseLR=0.000, train/LambdaLR=0.000]\nValidating: 0it [00:00, ?it/s]\nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\nValidating: 100%|##########| 1/1 [00:00<00:00,  4.76it/s]Epoch 0: 100%|##########| 2/2 [00:00<00:00,  2.26it/s, loss=1.39, v_num=, train/BaseLR=0.000, train/LambdaLR=0.000, val/MulticlassAccuracy=0.200, val/MulticlassPrecision=0.050, val/MulticlassRecall=0.250, val/MulticlassF1Score=0.0833]\n                                                         Epoch 0: 100%|##########| 2/2 [00:00<00:00,  2.21it/s, loss=1.39, v_num=, train/BaseLR=0.000, train/LambdaLR=0.000, val/MulticlassAccuracy=0.200, val/MulticlassPrecision=0.050, val/MulticlassRecall=0.250, val/MulticlassF1Score=0.0833]\nTesting: 0it [00:00, ?it/s]Testing: 100%|##########| 1/1 [00:00<00:00,  4.68it/s]--------------------------------------------------------------------------------\nDATALOADER:0 TEST RESULTS\n{'test/Loss': 1.386914610862732,\n 'test/MulticlassAccuracy': 0.0,\n 'test/MulticlassF1Score': 0.0,\n 'test/MulticlassPrecision': 0.0,\n 'test/MulticlassRecall': 0.0}\n--------------------------------------------------------------------------------\nTesting: 100%|##########| 1/1 [00:00<00:00,  4.57it/s]\nPredicting: 1it [00:00, ?it/s]Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]\n\n\nDownloading (…)lve/main/config.json:   0%|          | 0.00/787 [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|##########| 787/787 [00:00<00:00, 227kB/s]\nDownloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/730k [00:00<?, ?B/s]Downloading (…)\"pytorch_model.bin\";: 100%|##########| 730k/730k [00:00<00:00, 62.6MB/s]\n/home/runner/work/embeddings/embeddings/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:407: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n  rank_zero_warn(\nDownloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]Downloading builder script: 100%|##########| 6.77k/6.77k [00:00<00:00, 7.37MB/s]\nDownloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]Downloading builder script: 100%|##########| 7.55k/7.55k [00:00<00:00, 8.78MB/s]\n/home/runner/work/embeddings/embeddings/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/runner/work/embeddings/embeddings/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/runner/work/embeddings/embeddings/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/runner/work/embeddings/embeddings/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/runner/work/embeddings/embeddings/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/runner/work/embeddings/embeddings/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\nmetrics = pd.DataFrame.from_dict(asdict(metrics), orient=\"index\", columns=[\"values\"])\nmetrics\n\n\n\n\n\n  \n    \n      \n      values\n    \n  \n  \n    \n      accuracy\n      0.0\n    \n    \n      f1_macro\n      0.0\n    \n    \n      f1_micro\n      0.0\n    \n    \n      f1_weighted\n      0.0\n    \n    \n      recall_macro\n      0.0\n    \n    \n      recall_micro\n      0.0\n    \n    \n      recall_weighted\n      0.0\n    \n    \n      precision_macro\n      0.0\n    \n    \n      precision_micro\n      0.0\n    \n    \n      precision_weighted\n      0.0\n    \n    \n      classes\n      {0: {'precision': 0.0, 'recall': 0.0, 'f1': 0....\n    \n    \n      data\n      {'y_pred': [0, 0, 0, 0, 0], 'y_true': [1, 1, 1..."
  },
  {
    "objectID": "01_Tutorials/tutorial_how_to_use_config_space.html#running-pipeline-with-advancedconfig",
    "href": "01_Tutorials/tutorial_how_to_use_config_space.html#running-pipeline-with-advancedconfig",
    "title": "How to use our configs?",
    "section": "Running pipeline with AdvancedConfig",
    "text": "Running pipeline with AdvancedConfig\nAs mentioned in previous section LightningBasicConfig is only limited to most important parameters.\nLet’s see an example of the process of defining the parameters in our LightningAdvancedConfig. Tracing back different kwargs we can find:\n\ntask_train_kwargs Parameters that are passed to the Lightning Trainer object.\ntask_model_kwargs Parameters that are passed to the Lightning module object (we use TextClassificationModule which inherits from HuggingFaceLightningModule and HuggingFaceLightningModule).\ndatamodule_kwargs\nParameters passed to the datamodule classes, currently HuggingFaceDataModule takes several arguments (such as max_seq_length, processing_batch_size or downsamples args) as an input\nbatch_encoding_kwargs Parameters that are defined in __call__ method of the tokenizer which allow for manipulation of the tokenized text by setting parameters such as truncation, padding, stride etc. and specifying the return format of the tokenized text\ntokenizer_kwargs This is a generic configuration class of the hugginface model’s tokenizer, possible parameters depends on the tokenizer that is used. For example for bert uncased tokenizer these parameters are present here: https://huggingface.co/bert-base-uncased/blob/main/tokenizer_config.json\nload_dataset_kwargs Keyword arguments from the datasets.load_dataset method which loads a dataset from the Hugging Face Hub, or a local dataset; mostly metadata for downloading, loading, caching the dataset\nmodel_config_kwargs This is a generic configuration class of the hugginface model, possible parameters depends on the model that is used. For example for bert uncased these parameters are present here: https://huggingface.co/bert-base-uncased/blob/main/config.json\nearly_stopping_kwargs Params defined in __init__ of the EarlyStopping lightning callback; you can specify a metric to monitor and conditions to stop training when it stops improving\ndataloader_kwargs Defined in __init__ of the torch DataLoader object which wraps an iterable around the Dataset to enable easy access to the sample; specify params such as num of workers, sampling or shuffling\n\nLets create an advanced config with all the parameters we want to use.\n\nadvanced_config = LightningAdvancedConfig(\n    finetune_last_n_layers=0,\n    datamodule_kwargs={\n        \"max_seq_length\": None,\n    },\n    task_train_kwargs={\n        \"max_epochs\": 1,\n        \"devices\": \"auto\",\n        \"accelerator\": \"cpu\",\n        \"deterministic\": True,\n    },\n    task_model_kwargs={\n        \"learning_rate\": 0.001,\n        \"train_batch_size\": 64,\n        \"eval_batch_size\": 64,\n        \"use_scheduler\": True,\n        \"optimizer\": \"Adam\",\n        \"adam_epsilon\": 1e-6,\n        \"warmup_steps\": 100,\n        \"weight_decay\": 0.001,\n    },\n    early_stopping_kwargs=None,\n    model_config_kwargs={\"classifier_dropout\": 0.2},\n    tokenizer_kwargs={},\n    batch_encoding_kwargs={},\n    dataloader_kwargs={}\n)\nadvanced_config\n\nLightningAdvancedConfig(finetune_last_n_layers=0, task_model_kwargs={'learning_rate': 0.001, 'train_batch_size': 64, 'eval_batch_size': 64, 'use_scheduler': True, 'optimizer': 'Adam', 'adam_epsilon': 1e-06, 'warmup_steps': 100, 'weight_decay': 0.001}, datamodule_kwargs={'max_seq_length': None}, task_train_kwargs={'max_epochs': 1, 'devices': 'auto', 'accelerator': 'cpu', 'deterministic': True}, model_config_kwargs={'classifier_dropout': 0.2}, early_stopping_kwargs=None, tokenizer_kwargs={}, batch_encoding_kwargs={}, dataloader_kwargs={})\n\n\nNow we can add config the pipeline and run it.\n\npipeline = LightningClassificationPipeline(\n    embedding_name_or_path=\"hf-internal-testing/tiny-albert\",\n    dataset_name_or_path=\"data/polemo2_downsampled/\",\n    input_column_name=\"text\",\n    target_column_name=\"target\",\n    output_path=\".\",\n    devices=\"auto\",\n    accelerator=\"cpu\",\n    config=advanced_config\n)\n\nmetrics_adv_cfg = pipeline.run()\n\nValidation sanity check: 0it [00:00, ?it/s]Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]                                                              Training: 0it [00:00, ?it/s]Training:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] Epoch 0:  50%|#####     | 1/2 [00:00<00:00, 50.25it/s, loss=1.39, v_num=, train/BaseLR=0.000, train/LambdaLR=0.000]\nValidating: 0it [00:00, ?it/s]\nValidating:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 0: 100%|##########| 2/2 [00:00<00:00, 58.78it/s, loss=1.39, v_num=, train/BaseLR=0.000, train/LambdaLR=0.000, val/MulticlassAccuracy=0.000, val/MulticlassPrecision=0.000, val/MulticlassRecall=0.000, val/MulticlassF1Score=0.000]\n                                                 Epoch 0: 100%|##########| 2/2 [00:00<00:00, 43.07it/s, loss=1.39, v_num=, train/BaseLR=0.000, train/LambdaLR=0.000, val/MulticlassAccuracy=0.000, val/MulticlassPrecision=0.000, val/MulticlassRecall=0.000, val/MulticlassF1Score=0.000]\nTesting: 0it [00:00, ?it/s]--------------------------------------------------------------------------------\nDATALOADER:0 TEST RESULTS\n{'test/Loss': 1.3876429796218872,\n 'test/MulticlassAccuracy': 0.20000000298023224,\n 'test/MulticlassF1Score': 0.0833333358168602,\n 'test/MulticlassPrecision': 0.05000000074505806,\n 'test/MulticlassRecall': 0.25}\n--------------------------------------------------------------------------------\nTesting: 100%|##########| 1/1 [00:00<00:00, 59.36it/s]\nPredicting: 1it [00:00, ?it/s]Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]\n\n\nLoading cached processed dataset at /home/runner/work/embeddings/embeddings/data/polemo2_downsampled/train/cache-b7d34a8b4462adee.arrow\n  0%|          | 0/1 [00:00<?, ?ba/s]100%|##########| 1/1 [00:00<00:00, 123.30ba/s]\nLoading cached processed dataset at /home/runner/work/embeddings/embeddings/data/polemo2_downsampled/test/cache-46a5927d86882d6c.arrow\nLoading cached processed dataset at /home/runner/work/embeddings/embeddings/data/polemo2_downsampled/train/cache-b9cb2d894e89a67c.arrow\nCasting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]Casting the dataset: 100%|##########| 1/1 [00:00<00:00, 1141.93ba/s]\nLoading cached processed dataset at /home/runner/work/embeddings/embeddings/data/polemo2_downsampled/test/cache-03a305f4ff8fab44.arrow\n/home/runner/work/embeddings/embeddings/.venv/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:623: UserWarning: Checkpoint directory /home/runner/work/embeddings/embeddings/checkpoints exists and is not empty.\n  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n/home/runner/work/embeddings/embeddings/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:407: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n  rank_zero_warn(\n/home/runner/work/embeddings/embeddings/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/runner/work/embeddings/embeddings/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/runner/work/embeddings/embeddings/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\nFinally, we can check out some of the metrics.\n\nmetrics_adv_cfg = pd.DataFrame.from_dict(asdict(metrics_adv_cfg), orient=\"index\", columns=[\"values\"])\nmetrics_adv_cfg\n\n\n\n\n\n  \n    \n      \n      values\n    \n  \n  \n    \n      accuracy\n      0.6\n    \n    \n      f1_macro\n      0.25\n    \n    \n      f1_micro\n      0.6\n    \n    \n      f1_weighted\n      0.45\n    \n    \n      recall_macro\n      0.333333\n    \n    \n      recall_micro\n      0.6\n    \n    \n      recall_weighted\n      0.6\n    \n    \n      precision_macro\n      0.2\n    \n    \n      precision_micro\n      0.6\n    \n    \n      precision_weighted\n      0.36\n    \n    \n      classes\n      {0: {'precision': 0.6, 'recall': 1.0, 'f1': 0....\n    \n    \n      data\n      {'y_pred': [1, 1, 1, 1, 1], 'y_true': [1, 1, 1...\n    \n  \n\n\n\n\nWe used a very small dataset and very small Language Model, so the results are not very good. However, in reality we surely will get better results with more sophisticated models and larger datasets.\nGood luck in your experiments!"
  }
]