[
  {
    "objectID": "tutorials/validate_lightning_models_inference.html",
    "href": "tutorials/validate_lightning_models_inference.html",
    "title": "LM-based models inference",
    "section": "",
    "text": "import os\n\nos.chdir(\"..\")\nfrom typing import Any, Dict\n\nimport pytorch_lightning as pl\nfrom embeddings.config.lightning_config import LightningAdvancedConfig\nfrom embeddings.defaults import DATASET_PATH, RESULTS_PATH\nfrom embeddings.model.lightning_module.text_classification import (\n    TextClassificationModule,\n)\nfrom embeddings.pipeline.hf_preprocessing_pipeline import (\n    HuggingFacePreprocessingPipeline,\n)\nfrom embeddings.pipeline.lightning_classification import LightningClassificationPipeline\nfrom embeddings.task.lightning_task.text_classification import TextClassificationTask\nfrom embeddings.utils.utils import build_output_path\n\n\nembedding_name_or_path = \"hf-internal-testing/tiny-albert\"\ndataset_name = \"clarin-pl/polemo2-official\"\n\ndataset_path = build_output_path(DATASET_PATH, embedding_name_or_path, dataset_name)\noutput_path = build_output_path(RESULTS_PATH, embedding_name_or_path, dataset_name)\n\n\nPreprocess and downsample data\n\ndef preprocess_data(path: str) -> Dict[str, Any]:\n    pipeline = HuggingFacePreprocessingPipeline(\n        dataset_name=dataset_name,\n        load_dataset_kwargs={\n            \"train_domains\": [\"hotels\", \"medicine\"],\n            \"dev_domains\": [\"hotels\", \"medicine\"],\n            \"test_domains\": [\"hotels\", \"medicine\"],\n            \"text_cfg\": \"text\",\n        },\n        persist_path=path,\n        sample_missing_splits=None,\n        ignore_test_subset=False,\n        downsample_splits=(0.01, 0.01, 0.05),\n        seed=441,\n    )\n    pipeline.run()\n\n    return {\n        \"dataset_name_or_path\": path,\n        \"input_column_name\": [\"text\"],\n        \"target_column_name\": \"target\",\n    }\n\n\ndataset_kwargs = preprocess_data(dataset_path)\n\n\n\nTrain simple downsampled pipeline\n\nconfig = LightningAdvancedConfig(\n    finetune_last_n_layers=0,\n    task_train_kwargs={\"max_epochs\": 1, \"deterministic\": True,},\n    task_model_kwargs={\n        \"learning_rate\": 5e-4,\n        \"train_batch_size\": 32,\n        \"eval_batch_size\": 32,\n        \"use_scheduler\": True,\n        \"optimizer\": \"AdamW\",\n        \"adam_epsilon\": 1e-8,\n        \"warmup_steps\": 100,\n        \"weight_decay\": 0.0,\n    },\n    datamodule_kwargs={\"max_seq_length\": 64,},\n    early_stopping_kwargs={\"monitor\": \"val/Loss\", \"mode\": \"min\", \"patience\": 3,},\n    tokenizer_kwargs={},\n    batch_encoding_kwargs={},\n    dataloader_kwargs={},\n    model_config_kwargs={},\n)\n\n\npipeline = LightningClassificationPipeline(\n    embedding_name_or_path=embedding_name_or_path,\n    output_path=output_path,\n    config=config,\n    devices=\"auto\",\n    accelerator=\"cpu\",\n    **dataset_kwargs\n)\nresult = pipeline.run()\n\n\n\nLoad model from chechpoint automatically generated with Trainer\n\nckpt_path = output_path / \"checkpoints\" / \"last.ckpt\"\nckpt_path\n\n\ntask_from_ckpt = TextClassificationTask.from_checkpoint(\n    checkpoint_path=ckpt_path, output_path=output_path,\n)\n\n\nAlternatively we can load the model\n\nmodel_from_ckpt = TextClassificationModule.load_from_checkpoint(str(ckpt_path))\n\nThe warning appears when loading the model, however, it was validated that the loaded weights are the same as the weights that are being saved. The reason for this is that when the model_state_dict keys are loaded from the cached huggingface model some of them (cls.(…)) do not match the keys from the state_dict of the model weights that are saved.\nhttps://github.com/CLARIN-PL/embeddings/issues/225\n\n\n\nUse task from checkpoint for predictions\nreturn_names needs to be set to False since it uses the datamodule to retrieves the names while the datamodule is not loaded to Trainer in the LightningTask since we have not fitted it yet.\n\ntest_dataloader = pipeline.datamodule.test_dataloader()\npreds = task_from_ckpt.predict(test_dataloader)\npreds\n\nAlternatively we can implicitly assign the datamodule to Trainer in LightningTask\n\ntask_from_ckpt.trainer.datamodule = pipeline.datamodule\npreds_with_names = task_from_ckpt.predict(test_dataloader, return_names=True)\npreds_with_names\n\nWe can also use previosly loaded lightning model (LightningModule) outside of the task and get the predictions. To do this we also need to intitialize a Trainer.\n\ntrainer = pl.Trainer(default_root_dir=str(output_path))\npreds_from_model = trainer.predict(model_from_ckpt, dataloaders=test_dataloader)\npreds_from_model"
  },
  {
    "objectID": "tutorials/baseline_sklearn_models_tutorial.html",
    "href": "tutorials/baseline_sklearn_models_tutorial.html",
    "title": "Baseline Sklearn-based models",
    "section": "",
    "text": "This notebook’s purpose is to show how to use the sklearn-like models pipeline for text classification.\nThe pipeline trains a selected classifier on a selected dataset, training a specified vectorizer previously. Then, it computes the text classification evaluation metrics and saves them in a JSON file in a specified path.\nApart from the “SklearnClassificationPipeline” class, all you need to import is a selected sklearn-like classifier and any sklearn vectorizer, like CountVectorizer or TfidfVectorizer."
  },
  {
    "objectID": "tutorials/baseline_sklearn_models_tutorial.html#adaboost-model",
    "href": "tutorials/baseline_sklearn_models_tutorial.html#adaboost-model",
    "title": "Baseline Sklearn-based models",
    "section": "AdaBoost model",
    "text": "AdaBoost model\n\nembeddings_kwargs = {\n    \"max_features\": 10000,\n    \"max_df\": 10\n}\n\nclassifier_kwargs = {\n    \"n_estimators\": 100\n}\n\n\nevaluation_filename = \"adaboost_tfidf_evaluation.json\"  #default name: evaluation_filename.json\noutput_path = \".\"\n\nadaboost_tfidf_pipeline = SklearnClassificationPipeline(\n    dataset_name_or_path=DATASET_NAME,\n    input_column_name=\"text\",\n    target_column_name=\"target\",\n    output_path=output_path,\n    classifier=AdaBoostClassifier,\n    vectorizer=TfidfVectorizer,\n    evaluation_filename=evaluation_filename,\n    classifier_kwargs=classifier_kwargs,\n    embedding_kwargs=embeddings_kwargs\n)\n\n\nadaboost_tfidf_result = adaboost_tfidf_pipeline.run()\n\nNo config specified, defaulting to: polemo2-official/all_text\nFound cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n100%|██████████| 3/3 [00:00<00:00, 276.14it/s]\n\n\n\nadaboost_tfidf_result\n\nTextClassificationEvaluationResults(accuracy=0.4658536585365854, f1_macro=0.2856571505243167, f1_micro=0.4658536585365854, f1_weighted=0.3284353721974178, recall_macro=0.346626418679066, recall_micro=0.4658536585365854, recall_weighted=0.4658536585365854, precision_macro=0.31249434337949133, precision_micro=0.4658536585365854, precision_weighted=0.2983611252756568, classes={0: {'precision': 0.8103448275862069, 'recall': 0.3983050847457627, 'f1': 0.5340909090909091, 'support': 118}, 1: {'precision': 0.43963254593175854, 'recall': 0.9882005899705014, 'f1': 0.6085376930063578, 'support': 339}, 2: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 227}, 3: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 136}}, data=Predictions(y_pred=array([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n       1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n       1, 1, 1, 1, 1, 1]), y_true=array([1, 2, 2, 2, 2, 0, 0, 0, 1, 3, 1, 0, 2, 2, 2, 1, 1, 1, 1, 3, 3, 2,\n       2, 3, 1, 2, 1, 1, 1, 1, 3, 2, 2, 1, 1, 3, 2, 1, 1, 2, 2, 1, 1, 2,\n       0, 1, 1, 0, 1, 1, 2, 0, 2, 2, 1, 2, 2, 1, 2, 1, 0, 3, 3, 1, 0, 3,\n       0, 1, 0, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 0, 2, 1, 1, 1, 1, 2, 3, 3,\n       2, 3, 1, 2, 2, 2, 1, 1, 2, 1, 3, 2, 1, 0, 1, 1, 2, 3, 3, 2, 2, 3,\n       1, 1, 1, 3, 1, 0, 2, 1, 0, 3, 0, 3, 3, 1, 2, 1, 1, 1, 2, 0, 2, 2,\n       1, 1, 0, 2, 1, 3, 3, 0, 2, 1, 1, 2, 1, 2, 1, 0, 1, 1, 1, 0, 1, 1,\n       1, 1, 2, 2, 2, 1, 0, 3, 1, 1, 1, 2, 3, 0, 1, 2, 1, 1, 1, 0, 1, 1,\n       3, 0, 1, 3, 1, 1, 0, 2, 2, 2, 1, 1, 2, 3, 1, 2, 2, 2, 1, 1, 2, 1,\n       0, 3, 2, 3, 1, 0, 2, 1, 2, 3, 0, 2, 0, 0, 1, 1, 0, 1, 1, 2, 1, 1,\n       1, 1, 3, 1, 1, 2, 2, 1, 1, 2, 2, 3, 2, 3, 1, 3, 1, 3, 1, 3, 2, 3,\n       2, 2, 2, 3, 1, 1, 1, 1, 2, 2, 2, 3, 1, 2, 1, 0, 2, 2, 2, 0, 1, 1,\n       1, 3, 0, 1, 3, 2, 3, 1, 0, 1, 3, 3, 2, 3, 1, 1, 1, 2, 1, 2, 0, 1,\n       0, 3, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 0, 2, 1, 2, 1, 1, 3,\n       3, 2, 3, 0, 3, 0, 1, 0, 3, 0, 3, 0, 1, 1, 1, 3, 1, 3, 1, 3, 0, 0,\n       1, 1, 3, 0, 2, 0, 1, 2, 2, 0, 1, 1, 3, 2, 1, 2, 3, 3, 2, 3, 2, 2,\n       1, 0, 2, 1, 1, 1, 1, 1, 1, 3, 3, 3, 2, 1, 1, 3, 2, 1, 3, 2, 0, 1,\n       2, 1, 1, 2, 2, 1, 2, 2, 0, 1, 1, 2, 2, 2, 0, 3, 1, 1, 0, 1, 1, 0,\n       2, 3, 0, 0, 0, 2, 3, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1,\n       2, 1, 2, 1, 3, 3, 2, 1, 1, 1, 0, 3, 3, 0, 1, 1, 3, 2, 1, 2, 3, 0,\n       2, 0, 1, 3, 1, 1, 2, 2, 2, 1, 1, 3, 1, 3, 1, 1, 0, 1, 1, 2, 1, 2,\n       1, 2, 1, 2, 1, 2, 2, 0, 0, 2, 2, 0, 1, 3, 1, 1, 2, 2, 2, 1, 3, 1,\n       1, 3, 0, 2, 0, 1, 1, 0, 2, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 3, 1,\n       0, 2, 2, 0, 1, 2, 0, 0, 1, 2, 1, 1, 1, 1, 2, 2, 1, 0, 3, 1, 2, 1,\n       3, 2, 1, 2, 1, 1, 0, 0, 1, 1, 1, 1, 3, 0, 3, 2, 2, 2, 3, 2, 0, 3,\n       1, 2, 1, 0, 2, 3, 1, 1, 0, 1, 3, 1, 0, 1, 2, 3, 1, 1, 3, 3, 1, 1,\n       0, 1, 0, 3, 1, 3, 1, 0, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1,\n       2, 1, 0, 0, 0, 1, 1, 2, 1, 1, 1, 0, 2, 3, 2, 0, 3, 2, 2, 1, 3, 2,\n       2, 1, 1, 3, 2, 2, 3, 2, 1, 1, 3, 1, 1, 0, 1, 1, 2, 0, 1, 1, 1, 1,\n       1, 2, 1, 2, 3, 3, 3, 2, 2, 3, 2, 1, 1, 0, 1, 1, 1, 2, 2, 3, 2, 1,\n       1, 0, 2, 1, 2, 1, 1, 3, 2, 1, 1, 1, 1, 1, 2, 1, 0, 2, 1, 2, 3, 1,\n       2, 3, 1, 1, 1, 2, 1, 2, 0, 1, 0, 3, 2, 3, 2, 1, 1, 3, 1, 1, 0, 1,\n       2, 3, 1, 3, 1, 1, 3, 1, 1, 3, 0, 0, 3, 2, 3, 1, 2, 1, 0, 2, 0, 2,\n       1, 1, 3, 1, 2, 3, 0, 1, 1, 3, 1, 0, 3, 2, 1, 1, 3, 2, 2, 1, 1, 1,\n       1, 3, 1, 0, 2, 0, 0, 1, 3, 3, 1, 3, 0, 1, 1, 0, 2, 0, 3, 1, 1, 2,\n       1, 2, 1, 1, 3, 1, 3, 3, 1, 1, 1, 1, 0, 1, 2, 2, 2, 1, 2, 0, 2, 2,\n       2, 1, 1, 2, 1, 2, 2, 1, 3, 0, 0, 3, 1, 1, 2, 1, 3, 0, 0, 2, 3, 2,\n       2, 0, 1, 3, 2, 2]), y_probabilities=None, names=None))"
  },
  {
    "objectID": "tutorials/baseline_sklearn_models_tutorial.html#old-good-svm",
    "href": "tutorials/baseline_sklearn_models_tutorial.html#old-good-svm",
    "title": "Baseline Sklearn-based models",
    "section": "Old good SVM",
    "text": "Old good SVM\n\nsvm_kwargs = {\n    \"kernel\": \"linear\",\n    \"C\": 0.6\n}\n\nevaluation_filename_svm_tdidf = \"svm_tfidf_evaluation.json\"\n\nsvm_tfidf_pipeline = SklearnClassificationPipeline(\n    dataset_name_or_path=DATASET_NAME,\n    input_column_name=\"text\",\n    target_column_name=\"target\",\n    output_path=output_path,\n    classifier=SVC,\n    vectorizer=TfidfVectorizer,\n    evaluation_filename=evaluation_filename_svm_tdidf,\n    classifier_kwargs=svm_kwargs,\n    embedding_kwargs=embeddings_kwargs\n)\n\n\nsvm_tfidf_result = svm_tfidf_pipeline.run()\n\nNo config specified, defaulting to: polemo2-official/all_text\nFound cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n100%|██████████| 3/3 [00:00<00:00, 49.09it/s]"
  },
  {
    "objectID": "tutorials/baseline_sklearn_models_tutorial.html#kaggle-winner-xgboost",
    "href": "tutorials/baseline_sklearn_models_tutorial.html#kaggle-winner-xgboost",
    "title": "Baseline Sklearn-based models",
    "section": "Kaggle-winner XGBoost",
    "text": "Kaggle-winner XGBoost\n\nembeddings_kwargs = {\n    \"max_features\": 10000\n}\n\nxgb_kwargs = {\n    \"n_estimators\": 200,\n    \"max_depth\": 7\n}\n\nevaluation_filename_xgb_tdidf = \"xgb_tfidf_evaluation.json\"\n\nxgb_tfidf_pipeline = SklearnClassificationPipeline(\n    dataset_name_or_path=DATASET_NAME,\n    input_column_name=\"text\",\n    target_column_name=\"target\",\n    output_path=output_path,\n    classifier=XGBClassifier,\n    vectorizer=TfidfVectorizer,\n    evaluation_filename=evaluation_filename_xgb_tdidf,\n    classifier_kwargs=xgb_kwargs,\n    embedding_kwargs=embeddings_kwargs\n)\n\n\nxgb_tfidf_result = xgb_tfidf_pipeline.run()\n\nNo config specified, defaulting to: polemo2-official/all_text\nFound cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n100%|██████████| 3/3 [00:00<00:00, 53.77it/s]"
  },
  {
    "objectID": "tutorials/how_to_use_polsih_static_embeddings.html",
    "href": "tutorials/how_to_use_polsih_static_embeddings.html",
    "title": "Static word embeddings",
    "section": "",
    "text": "This notebook presents how to load Gensim embeddings from file and use them to train Flair model for sequence labelling task.\nExamplary embeddings are located here http://dsmodels.nlp.ipipan.waw.pl/\nThey were trained in many different ways:\n\nUsing different corpus.\nBased lemmas or forms.\nWith all part of speach or some of them.\nWith different vector size.\nWith CBOW or Skip-Gram neural net architecture.\nWIth different nerual net training algorithms.\n\n\nimport os\nos.chdir(\"..\")\n\nimport pathlib\n\nimport torch\nfrom flair.data import Sentence\n\nfrom embeddings.defaults import RESULTS_PATH\nfrom embeddings.embedding.auto_flair import AutoFlairWordEmbedding\nfrom embeddings.evaluator.sequence_labeling_evaluator import SequenceLabelingEvaluator\nfrom embeddings.pipeline.flair_sequence_labeling import FlairSequenceLabelingPipeline\nfrom embeddings.utils.utils import build_output_path\n\ntorch.set_printoptions(threshold=8)\n\n\nGet embeddings for words\n\nembedding_path = pathlib.Path(\"../wiki-lemmas-restricted-100-skipg-ns.txt.gz\")\nmodel_type_reference=\"embeddings.embedding.static.word2vec.IPIPANWord2VecEmbedding\"\n\nembeding = AutoFlairWordEmbedding.from_file(embedding_path, model_type_reference)\n\nsentence = Sentence(\"Nas nie przekonają, że białe jest białe, a czarne jest czarne.\")\n\nembeding.embed([sentence])\n\n2022-02-08 13:10:48,921 - embeddings.embedding.auto_flair - INFO - wiki-lemmas-restricted-100-skipg-ns.txt.gz not compatible with Transformers, trying to initialise as static embedding.\nWARNING:root:Couldn't unpickle model file. Unpickle model with different method.\n\n\n[Sentence: \"Nas nie przekonają , że białe jest białe , a czarne jest czarne .\"   [− Tokens: 14]]\n\n\n\nfor token in sentence:\n    print(token)\n    print(token.embedding)\n\nToken: 1 Nas\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\nToken: 2 nie\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\nToken: 3 przekonają\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\nToken: 4 ,\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\nToken: 5 że\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\nToken: 6 białe\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\nToken: 7 jest\ntensor([-0.2300, -0.0609,  0.2554,  ...,  0.2459, -0.0658,  0.1421])\nToken: 8 białe\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\nToken: 9 ,\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\nToken: 10 a\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\nToken: 11 czarne\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\nToken: 12 jest\ntensor([-0.2300, -0.0609,  0.2554,  ...,  0.2459, -0.0658,  0.1421])\nToken: 13 czarne\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\nToken: 14 .\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\n\n\n\n\nTraining model for sequence labelling\nBefore we train a model we need to define its parameters:\n\nembedding_path - path to file that contains gensim embeddings\ndataset_name (that points to the dataset located on huggingface datasets)\ninput_column_name - specific for selected dataset\ntarget_column_name - specific for selected dataset\nroot - root path of output path\nhidden_size - hidden_size of model that will be trained in sequence labelling task, the model is defined here\n\nExtra important parameters:\n\ntask_model_kwargs - parameters of the model that will be trained (e.g. number of RNN layers, type of RNN layers, …)\ntask_train_kwargs - training parameters (all of them can be found here)\n\n\ndataset_name= \"clarin-pl/kpwr-ner\"\ninput_column_name = \"tokens\"\ntarget_column_name = \"ner\"\nroot = RESULTS_PATH.joinpath(\"pos_tagging\")\n\noutput_path = build_output_path(root, embedding_path.stem, dataset_name)\noutput_path.mkdir(parents=True, exist_ok=True)\n\nhidden_size = 64\ntask_train_kwargs = {\n    \"max_epochs\": 3 # for testing purpose only\n}\n\n\npipeline = FlairSequenceLabelingPipeline(\n    embedding_path,\n    dataset_name,\n    input_column_name,\n    target_column_name,\n    output_path,\n    hidden_size,\n    model_type_reference=model_type_reference,\n    task_train_kwargs=task_train_kwargs,\n)\nresult = pipeline.run()\n\n2022-02-08 13:11:37,840 - embeddings.embedding.auto_flair - INFO - wiki-lemmas-restricted-100-skipg-ns.txt.gz not compatible with Transformers, trying to initialise as static embedding.\nWARNING:root:Couldn't unpickle model file. Unpickle model with different method.\nWARNING:datasets.builder:Using custom data configuration default\nWARNING:datasets.builder:Reusing dataset kpwrner (/Users/lukaszkoziol/.cache/huggingface/datasets/clarin-pl___kpwrner/default/0.0.0/001e3d471298007e8412e3a6ccc06bec000dec1bce0cf8e0ba7e5b7e105b1342)\n\n\n\n\n\n2022-02-08 13:12:11,307 - embeddings.transformation.flair_transformation.corpus_transformation - INFO - Info of ['train', 'test']:\n{'builder_name': 'kpwrner',\n 'citation': '',\n 'config_name': 'default',\n 'dataset_size': 13212646,\n 'description': 'KPWR-NER tagging dataset.',\n 'download_checksums': {'https://huggingface.co/datasets/clarin-pl/kpwr-ner/resolve/main/data/kpwr-ner-n82-test.iob': {'checksum': '7b86fd227605b7e5f807eedbcd87573271d8adb86cfddf56c763b1751e71a924',\n                                                                                                                       'num_bytes': 2247780},\n                        'https://huggingface.co/datasets/clarin-pl/kpwr-ner/resolve/main/data/kpwr-ner-n82-train-tune.iob': {'checksum': '7ab673f299b3a9e875c2c46ef1051807d98f923f0356d0be78556c832481efea',\n                                                                                                                             'num_bytes': 6719818}},\n 'download_size': 8967598,\n 'features': {'lemmas': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n              'ner': Sequence(feature=ClassLabel(num_classes=161, names=['B-nam_adj', 'B-nam_adj_city', 'B-nam_adj_country', 'B-nam_adj_person', 'B-nam_eve', 'B-nam_eve_human', 'B-nam_eve_human_cultural', 'B-nam_eve_human_holiday', 'B-nam_eve_human_sport', 'B-nam_fac_bridge', 'B-nam_fac_goe', 'B-nam_fac_goe_stop', 'B-nam_fac_park', 'B-nam_fac_road', 'B-nam_fac_square', 'B-nam_fac_system', 'B-nam_liv_animal', 'B-nam_liv_character', 'B-nam_liv_god', 'B-nam_liv_habitant', 'B-nam_liv_person', 'B-nam_loc', 'B-nam_loc_astronomical', 'B-nam_loc_country_region', 'B-nam_loc_gpe_admin1', 'B-nam_loc_gpe_admin2', 'B-nam_loc_gpe_admin3', 'B-nam_loc_gpe_city', 'B-nam_loc_gpe_conurbation', 'B-nam_loc_gpe_country', 'B-nam_loc_gpe_district', 'B-nam_loc_gpe_subdivision', 'B-nam_loc_historical_region', 'B-nam_loc_hydronym', 'B-nam_loc_hydronym_lake', 'B-nam_loc_hydronym_ocean', 'B-nam_loc_hydronym_river', 'B-nam_loc_hydronym_sea', 'B-nam_loc_land', 'B-nam_loc_land_continent', 'B-nam_loc_land_island', 'B-nam_loc_land_mountain', 'B-nam_loc_land_peak', 'B-nam_loc_land_region', 'B-nam_num_house', 'B-nam_num_phone', 'B-nam_org_company', 'B-nam_org_group', 'B-nam_org_group_band', 'B-nam_org_group_team', 'B-nam_org_institution', 'B-nam_org_nation', 'B-nam_org_organization', 'B-nam_org_organization_sub', 'B-nam_org_political_party', 'B-nam_oth', 'B-nam_oth_currency', 'B-nam_oth_data_format', 'B-nam_oth_license', 'B-nam_oth_position', 'B-nam_oth_tech', 'B-nam_oth_www', 'B-nam_pro', 'B-nam_pro_award', 'B-nam_pro_brand', 'B-nam_pro_media', 'B-nam_pro_media_periodic', 'B-nam_pro_media_radio', 'B-nam_pro_media_tv', 'B-nam_pro_media_web', 'B-nam_pro_model_car', 'B-nam_pro_software', 'B-nam_pro_software_game', 'B-nam_pro_title', 'B-nam_pro_title_album', 'B-nam_pro_title_article', 'B-nam_pro_title_book', 'B-nam_pro_title_document', 'B-nam_pro_title_song', 'B-nam_pro_title_treaty', 'B-nam_pro_title_tv', 'B-nam_pro_vehicle', 'I-nam_adj_country', 'I-nam_eve', 'I-nam_eve_human', 'I-nam_eve_human_cultural', 'I-nam_eve_human_holiday', 'I-nam_eve_human_sport', 'I-nam_fac_bridge', 'I-nam_fac_goe', 'I-nam_fac_goe_stop', 'I-nam_fac_park', 'I-nam_fac_road', 'I-nam_fac_square', 'I-nam_fac_system', 'I-nam_liv_animal', 'I-nam_liv_character', 'I-nam_liv_god', 'I-nam_liv_person', 'I-nam_loc', 'I-nam_loc_astronomical', 'I-nam_loc_country_region', 'I-nam_loc_gpe_admin1', 'I-nam_loc_gpe_admin2', 'I-nam_loc_gpe_admin3', 'I-nam_loc_gpe_city', 'I-nam_loc_gpe_conurbation', 'I-nam_loc_gpe_country', 'I-nam_loc_gpe_district', 'I-nam_loc_gpe_subdivision', 'I-nam_loc_historical_region', 'I-nam_loc_hydronym', 'I-nam_loc_hydronym_lake', 'I-nam_loc_hydronym_ocean', 'I-nam_loc_hydronym_river', 'I-nam_loc_hydronym_sea', 'I-nam_loc_land', 'I-nam_loc_land_continent', 'I-nam_loc_land_island', 'I-nam_loc_land_mountain', 'I-nam_loc_land_peak', 'I-nam_loc_land_region', 'I-nam_num_house', 'I-nam_num_phone', 'I-nam_org_company', 'I-nam_org_group', 'I-nam_org_group_band', 'I-nam_org_group_team', 'I-nam_org_institution', 'I-nam_org_nation', 'I-nam_org_organization', 'I-nam_org_organization_sub', 'I-nam_org_political_party', 'I-nam_oth', 'I-nam_oth_currency', 'I-nam_oth_data_format', 'I-nam_oth_license', 'I-nam_oth_position', 'I-nam_oth_tech', 'I-nam_oth_www', 'I-nam_pro', 'I-nam_pro_award', 'I-nam_pro_brand', 'I-nam_pro_media', 'I-nam_pro_media_periodic', 'I-nam_pro_media_radio', 'I-nam_pro_media_tv', 'I-nam_pro_media_web', 'I-nam_pro_model_car', 'I-nam_pro_software', 'I-nam_pro_software_game', 'I-nam_pro_title', 'I-nam_pro_title_album', 'I-nam_pro_title_article', 'I-nam_pro_title_book', 'I-nam_pro_title_document', 'I-nam_pro_title_song', 'I-nam_pro_title_treaty', 'I-nam_pro_title_tv', 'I-nam_pro_vehicle', 'O'], names_file=None, id=None), length=-1, id=None),\n              'orth': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n              'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)},\n 'homepage': 'https://clarin-pl.eu/dspace/handle/11321/294',\n 'license': '',\n 'post_processed': None,\n 'post_processing_size': None,\n 'size_in_bytes': 22180244,\n 'splits': {'test': SplitInfo(name='test', num_bytes=3298573, num_examples=4323, dataset_name='kpwrner'),\n            'train': SplitInfo(name='train', num_bytes=9914073, num_examples=13959, dataset_name='kpwrner')},\n 'supervised_keys': None,\n 'task_templates': None,\n 'version': 0.0.0}\n2022-02-08 13:12:11,309 - embeddings.transformation.flair_transformation.corpus_transformation - INFO - Schemas:    DatasetDict({\n    train: Dataset({\n        features: ['tokens', 'lemmas', 'orth', 'ner'],\n        num_rows: 13959\n    })\n    test: Dataset({\n        features: ['tokens', 'lemmas', 'orth', 'ner'],\n        num_rows: 4323\n    })\n})\nWARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/lukaszkoziol/.cache/huggingface/datasets/clarin-pl___kpwrner/default/0.0.0/001e3d471298007e8412e3a6ccc06bec000dec1bce0cf8e0ba7e5b7e105b1342/cache-5e4722cf782173d2.arrow\nWARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/lukaszkoziol/.cache/huggingface/datasets/clarin-pl___kpwrner/default/0.0.0/001e3d471298007e8412e3a6ccc06bec000dec1bce0cf8e0ba7e5b7e105b1342/cache-665f7bfa0d3216de.arrow\n2022-02-08 13:12:17,138 - embeddings.task.flair_task.flair_task - WARNING - Dev subset is missing in the corpus - wrapping with an empty list\n\n\n2022-02-08 13:12:17,148 ----------------------------------------------------------------------------------------------------\n2022-02-08 13:12:17,149 Model: \"SequenceTagger(\n  (embeddings): WordEmbeddingsPL(\n    '../wiki-lemmas-restricted-100-skipg-ns.txt.gz'\n    (embedding): Embedding(446609, 100)\n  )\n  (word_dropout): WordDropout(p=0.05)\n  (locked_dropout): LockedDropout(p=0.5)\n  (embedding2nn): Linear(in_features=100, out_features=100, bias=True)\n  (rnn): LSTM(100, 64, batch_first=True, bidirectional=True)\n  (linear): Linear(in_features=128, out_features=163, bias=True)\n  (beta): 1.0\n  (weights): None\n  (weight_tensor) None\n)\"\n2022-02-08 13:12:17,150 ----------------------------------------------------------------------------------------------------\n2022-02-08 13:12:17,151 Corpus: \"Corpus: 13959 train + 0 dev + 4323 test sentences\"\n2022-02-08 13:12:17,152 ----------------------------------------------------------------------------------------------------\n2022-02-08 13:12:17,153 Parameters:\n2022-02-08 13:12:17,153  - learning_rate: \"0.1\"\n2022-02-08 13:12:17,154  - mini_batch_size: \"32\"\n2022-02-08 13:12:17,155  - patience: \"3\"\n2022-02-08 13:12:17,156  - anneal_factor: \"0.5\"\n2022-02-08 13:12:17,157  - max_epochs: \"3\"\n2022-02-08 13:12:17,157  - shuffle: \"True\"\n2022-02-08 13:12:17,158  - train_with_dev: \"True\"\n2022-02-08 13:12:17,159  - batch_growth_annealing: \"False\"\n2022-02-08 13:12:17,160 ----------------------------------------------------------------------------------------------------\n2022-02-08 13:12:17,160 Model training base path: \"/Users/lukaszkoziol/Projects/clarin/embeddings/resources/results/pos_tagging/wiki-lemmas-restricted-100-skipg-ns.txt/clarin-pl__kpwr-ner\"\n2022-02-08 13:12:17,161 ----------------------------------------------------------------------------------------------------\n2022-02-08 13:12:17,162 Device: cpu\n2022-02-08 13:12:17,162 ----------------------------------------------------------------------------------------------------\n2022-02-08 13:12:17,163 Embeddings storage mode: cpu\n2022-02-08 13:12:17,173 ----------------------------------------------------------------------------------------------------\n2022-02-08 13:12:41,817 epoch 1 - iter 43/437 - loss 0.95137164 - samples/sec: 55.85 - lr: 0.100000\n2022-02-08 13:13:09,632 epoch 1 - iter 86/437 - loss 0.79235960 - samples/sec: 49.48 - lr: 0.100000\n2022-02-08 13:13:27,743 epoch 1 - iter 129/437 - loss 0.79993568 - samples/sec: 75.99 - lr: 0.100000\n2022-02-08 13:13:50,296 epoch 1 - iter 172/437 - loss 0.70606005 - samples/sec: 61.02 - lr: 0.100000\n2022-02-08 13:14:07,252 epoch 1 - iter 215/437 - loss 0.61976304 - samples/sec: 81.17 - lr: 0.100000\n2022-02-08 13:14:32,659 epoch 1 - iter 258/437 - loss 0.58055640 - samples/sec: 54.17 - lr: 0.100000\n2022-02-08 13:15:16,856 epoch 1 - iter 301/437 - loss 0.59306165 - samples/sec: 31.14 - lr: 0.100000\n2022-02-08 13:15:35,274 epoch 1 - iter 344/437 - loss 0.63092813 - samples/sec: 74.73 - lr: 0.100000\n2022-02-08 13:15:56,447 epoch 1 - iter 387/437 - loss 0.66910945 - samples/sec: 65.00 - lr: 0.100000\n2022-02-08 13:16:26,575 epoch 1 - iter 430/437 - loss 0.68428783 - samples/sec: 45.68 - lr: 0.100000\n2022-02-08 13:16:30,780 ----------------------------------------------------------------------------------------------------\n2022-02-08 13:16:30,782 EPOCH 1 done: loss 0.6880 - lr 0.1000000\n2022-02-08 13:16:30,782 BAD EPOCHS (no improvement): 0\n2022-02-08 13:16:30,784 ----------------------------------------------------------------------------------------------------\n2022-02-08 13:17:07,313 epoch 2 - iter 43/437 - loss 0.56538654 - samples/sec: 37.68 - lr: 0.100000\n2022-02-08 13:17:42,529 epoch 2 - iter 86/437 - loss 0.54506007 - samples/sec: 39.08 - lr: 0.100000\n2022-02-08 13:18:15,719 epoch 2 - iter 129/437 - loss 0.54197868 - samples/sec: 41.47 - lr: 0.100000\n2022-02-08 13:18:54,629 epoch 2 - iter 172/437 - loss 0.55047567 - samples/sec: 35.37 - lr: 0.100000\n2022-02-08 13:19:27,185 epoch 2 - iter 215/437 - loss 0.55522700 - samples/sec: 42.27 - lr: 0.100000\n2022-02-08 13:20:09,280 epoch 2 - iter 258/437 - loss 0.55841158 - samples/sec: 32.69 - lr: 0.100000\n2022-02-08 13:20:46,569 epoch 2 - iter 301/437 - loss 0.55715816 - samples/sec: 36.91 - lr: 0.100000\n2022-02-08 13:21:23,416 epoch 2 - iter 344/437 - loss 0.54770699 - samples/sec: 37.35 - lr: 0.100000\n2022-02-08 13:21:54,530 epoch 2 - iter 387/437 - loss 0.53896365 - samples/sec: 44.23 - lr: 0.100000\n2022-02-08 13:22:26,383 epoch 2 - iter 430/437 - loss 0.53573987 - samples/sec: 43.21 - lr: 0.100000\n2022-02-08 13:22:33,840 ----------------------------------------------------------------------------------------------------\n2022-02-08 13:22:33,841 EPOCH 2 done: loss 0.5344 - lr 0.1000000\n2022-02-08 13:22:33,842 BAD EPOCHS (no improvement): 0\n2022-02-08 13:22:33,844 ----------------------------------------------------------------------------------------------------\n2022-02-08 13:23:20,529 epoch 3 - iter 43/437 - loss 0.48468883 - samples/sec: 29.48 - lr: 0.100000\n2022-02-08 13:23:56,509 epoch 3 - iter 86/437 - loss 0.49504739 - samples/sec: 38.25 - lr: 0.100000\n2022-02-08 13:24:36,010 epoch 3 - iter 129/437 - loss 0.49641896 - samples/sec: 34.84 - lr: 0.100000\n2022-02-08 13:25:21,810 epoch 3 - iter 172/437 - loss 0.49006870 - samples/sec: 30.05 - lr: 0.100000\n2022-02-08 13:25:58,303 epoch 3 - iter 215/437 - loss 0.48949477 - samples/sec: 37.71 - lr: 0.100000\n2022-02-08 13:26:37,964 epoch 3 - iter 258/437 - loss 0.48738675 - samples/sec: 34.70 - lr: 0.100000\n2022-02-08 13:27:14,635 epoch 3 - iter 301/437 - loss 0.48331637 - samples/sec: 37.53 - lr: 0.100000\n2022-02-08 13:27:51,305 epoch 3 - iter 344/437 - loss 0.47962623 - samples/sec: 37.53 - lr: 0.100000\n2022-02-08 13:28:28,819 epoch 3 - iter 387/437 - loss 0.47569281 - samples/sec: 36.69 - lr: 0.100000\n2022-02-08 13:29:04,500 epoch 3 - iter 430/437 - loss 0.47695729 - samples/sec: 38.57 - lr: 0.100000\n2022-02-08 13:29:08,761 ----------------------------------------------------------------------------------------------------\n2022-02-08 13:29:08,762 EPOCH 3 done: loss 0.4774 - lr 0.1000000\n2022-02-08 13:29:08,763 BAD EPOCHS (no improvement): 0\n2022-02-08 13:29:09,424 ----------------------------------------------------------------------------------------------------\n2022-02-08 13:29:09,426 Testing using last state of model ...\n2022-02-08 13:30:03,199 0.6285  0.1463  0.2373  0.1379\n2022-02-08 13:30:03,200 \nResults:\n- F-score (micro) 0.2373\n- F-score (macro) 0.023\n- Accuracy 0.1379\n\nBy class:\n                           precision    recall  f1-score   support\n\n           nam_liv_person     0.7263    0.4110    0.5249       949\n         nam_loc_gpe_city     0.3789    0.1968    0.2590       437\n      nam_loc_gpe_country     0.6770    0.3053    0.4208       357\n      nam_org_institution     0.0000    0.0000    0.0000       266\n     nam_org_organization     0.5000    0.0041    0.0081       246\n       nam_org_group_team     0.6413    0.3960    0.4896       149\n          nam_adj_country     0.0000    0.0000    0.0000       166\n         nam_pro_software     0.0000    0.0000    0.0000        97\n             nam_fac_road     0.0000    0.0000    0.0000        95\n   nam_pro_title_document     0.0000    0.0000    0.0000        88\n   nam_pro_media_periodic     0.0000    0.0000    0.0000        84\n          nam_org_company     0.4286    0.0395    0.0723        76\n           nam_org_nation     0.0000    0.0000    0.0000        81\n            nam_eve_human     0.0000    0.0000    0.0000        78\n             nam_oth_tech     0.0000    0.0000    0.0000        61\n              nam_fac_goe     0.0000    0.0000    0.0000        64\n       nam_loc_gpe_admin1     0.0000    0.0000    0.0000        64\n  nam_org_political_party     0.0000    0.0000    0.0000        58\n      nam_eve_human_sport     0.0000    0.0000    0.0000        55\n                  nam_adj     0.0000    0.0000    0.0000        52\n         nam_oth_currency     0.0000    0.0000    0.0000        51\n   nam_loc_hydronym_river     0.0000    0.0000    0.0000        51\n       nam_loc_gpe_admin3     0.0000    0.0000    0.0000        47\n            nam_pro_brand     0.0000    0.0000    0.0000        46\n             nam_adj_city     0.0000    0.0000    0.0000        42\n        nam_pro_media_web     0.0000    0.0000    0.0000        40\n       nam_loc_gpe_admin2     0.0000    0.0000    0.0000        36\n            nam_pro_title     0.0000    0.0000    0.0000        35\n              nam_liv_god     0.0000    0.0000    0.0000        35\n   nam_loc_land_continent     0.0000    0.0000    0.0000        32\n           nam_fac_system     0.0000    0.0000    0.0000        26\n        nam_pro_model_car     0.0000    0.0000    0.0000        26\n  nam_loc_gpe_subdivision     0.0000    0.0000    0.0000        26\n         nam_pro_title_tv     0.0000    0.0000    0.0000        24\n            nam_pro_award     0.0000    0.0000    0.0000        23\n                  nam_oth     0.0000    0.0000    0.0000        22\nnam_loc_historical_region     0.0000    0.0000    0.0000        22\n   nam_eve_human_cultural     0.0000    0.0000    0.0000        22\n              nam_oth_www     0.0000    0.0000    0.0000        20\n       nam_org_group_band     0.0000    0.0000    0.0000        19\n            nam_org_group     0.0000    0.0000    0.0000        18\n           nam_adj_person     0.0000    0.0000    0.0000        18\n     nam_loc_gpe_district     0.0000    0.0000    0.0000        18\n          nam_oth_license     0.0000    0.0000    0.0000        11\n           nam_liv_animal     0.0000    0.0000    0.0000        11\n      nam_loc_land_island     0.0000    0.0000    0.0000        11\n            nam_num_house     0.0000    0.0000    0.0000        11\n       nam_pro_title_book     0.0000    0.0000    0.0000        11\n      nam_loc_land_region     0.0000    0.0000    0.0000        11\n             nam_fac_park     0.0000    0.0000    0.0000        10\n         nam_oth_position     0.0000    0.0000    0.0000        10\n      nam_oth_data_format     0.0000    0.0000    0.0000        10\n    nam_loc_land_mountain     0.0000    0.0000    0.0000         9\n    nam_eve_human_holiday     0.0000    0.0000    0.0000         9\n                  nam_eve     0.0000    0.0000    0.0000         8\n            nam_pro_media     0.0000    0.0000    0.0000         8\n         nam_liv_habitant     0.0000    0.0000    0.0000         7\n       nam_pro_title_song     0.0000    0.0000    0.0000         7\n      nam_pro_title_album     0.0000    0.0000    0.0000         7\n         nam_pro_media_tv     0.0000    0.0000    0.0000         7\n           nam_fac_square     0.0000    0.0000    0.0000         6\n          nam_pro_vehicle     0.0000    0.0000    0.0000         4\n   nam_loc_country_region     0.0000    0.0000    0.0000         4\n                  nam_loc     0.0000    0.0000    0.0000         4\n           nam_fac_bridge     0.0000    0.0000    0.0000         4\n         nam_fac_goe_stop     0.0000    0.0000    0.0000         4\n     nam_loc_hydronym_sea     0.0000    0.0000    0.0000         3\n      nam_pro_media_radio     0.0000    0.0000    0.0000         3\n    nam_pro_software_game     0.0000    0.0000    0.0000         3\n nam_org_organization_sub     0.0000    0.0000    0.0000         3\n    nam_loc_hydronym_lake     0.0000    0.0000    0.0000         2\n                  nam_pro     0.0000    0.0000    0.0000         2\n            nam_num_phone     0.0000    0.0000    0.0000         2\n             nam_loc_land     0.0000    0.0000    0.0000         2\n     nam_pro_title_treaty     0.0000    0.0000    0.0000         2\n   nam_loc_hydronym_ocean     0.0000    0.0000    0.0000         1\n         nam_loc_hydronym     0.0000    0.0000    0.0000         1\n\n                micro avg     0.6285    0.1463    0.2373      4430\n                macro avg     0.0435    0.0176    0.0230      4430\n             weighted avg     0.3042    0.1463    0.1901      4430\n              samples avg     0.1379    0.1379    0.1379      4430\n\n\n\n2022-02-08 13:30:03,201 ----------------------------------------------------------------------------------------------------\n\n\n/Users/lukaszkoziol/Projects/clarin/embeddings/.venv/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\nresult\n\n{'seqeval__mode_None__scheme_None': {'nam_adj': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 52},\n  'nam_adj_city': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 42},\n  'nam_adj_country': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 166},\n  'nam_adj_person': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 18},\n  'nam_eve': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 8},\n  'nam_eve_human': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 78},\n  'nam_eve_human_cultural': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 22},\n  'nam_eve_human_holiday': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 9},\n  'nam_eve_human_sport': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 55},\n  'nam_fac_bridge': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 4},\n  'nam_fac_goe': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 64},\n  'nam_fac_goe_stop': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 4},\n  'nam_fac_park': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 10},\n  'nam_fac_road': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 95},\n  'nam_fac_square': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 6},\n  'nam_fac_system': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 26},\n  'nam_liv_animal': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 11},\n  'nam_liv_god': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 35},\n  'nam_liv_habitant': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 7},\n  'nam_liv_person': {'precision': 0.7262569832402235,\n   'recall': 0.410958904109589,\n   'f1': 0.5248990578734858,\n   'number': 949},\n  'nam_loc': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 4},\n  'nam_loc_country_region': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 4},\n  'nam_loc_gpe_admin1': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 64},\n  'nam_loc_gpe_admin2': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 36},\n  'nam_loc_gpe_admin3': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 47},\n  'nam_loc_gpe_city': {'precision': 0.3788546255506608,\n   'recall': 0.19679633867276888,\n   'f1': 0.2590361445783133,\n   'number': 437},\n  'nam_loc_gpe_country': {'precision': 0.6770186335403726,\n   'recall': 0.30532212885154064,\n   'f1': 0.42084942084942084,\n   'number': 357},\n  'nam_loc_gpe_district': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 18},\n  'nam_loc_gpe_subdivision': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 26},\n  'nam_loc_historical_region': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 22},\n  'nam_loc_hydronym': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 1},\n  'nam_loc_hydronym_lake': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 2},\n  'nam_loc_hydronym_ocean': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 1},\n  'nam_loc_hydronym_river': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 51},\n  'nam_loc_hydronym_sea': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 3},\n  'nam_loc_land': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2},\n  'nam_loc_land_continent': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 32},\n  'nam_loc_land_island': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 11},\n  'nam_loc_land_mountain': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 9},\n  'nam_loc_land_region': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 11},\n  'nam_num_house': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 11},\n  'nam_num_phone': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2},\n  'nam_org_company': {'precision': 0.42857142857142855,\n   'recall': 0.039473684210526314,\n   'f1': 0.07228915662650602,\n   'number': 76},\n  'nam_org_group': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 18},\n  'nam_org_group_band': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 19},\n  'nam_org_group_team': {'precision': 0.6413043478260869,\n   'recall': 0.3959731543624161,\n   'f1': 0.4896265560165975,\n   'number': 149},\n  'nam_org_institution': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 266},\n  'nam_org_nation': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 81},\n  'nam_org_organization': {'precision': 0.5,\n   'recall': 0.0040650406504065045,\n   'f1': 0.00806451612903226,\n   'number': 246},\n  'nam_org_organization_sub': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 3},\n  'nam_org_political_party': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 58},\n  'nam_oth': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 22},\n  'nam_oth_currency': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 51},\n  'nam_oth_data_format': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 10},\n  'nam_oth_license': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 11},\n  'nam_oth_position': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 10},\n  'nam_oth_tech': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 61},\n  'nam_oth_www': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 20},\n  'nam_pro': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2},\n  'nam_pro_award': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 23},\n  'nam_pro_brand': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 46},\n  'nam_pro_media': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 8},\n  'nam_pro_media_periodic': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 84},\n  'nam_pro_media_radio': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 3},\n  'nam_pro_media_tv': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 7},\n  'nam_pro_media_web': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 40},\n  'nam_pro_model_car': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 26},\n  'nam_pro_software': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 97},\n  'nam_pro_software_game': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 3},\n  'nam_pro_title': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 35},\n  'nam_pro_title_album': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 7},\n  'nam_pro_title_book': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 11},\n  'nam_pro_title_document': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 88},\n  'nam_pro_title_song': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 7},\n  'nam_pro_title_treaty': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 2},\n  'nam_pro_title_tv': {'precision': 0.0,\n   'recall': 0.0,\n   'f1': 0.0,\n   'number': 24},\n  'nam_pro_vehicle': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 4},\n  'overall_precision': 0.6285160038797284,\n  'overall_recall': 0.14627539503386006,\n  'overall_f1': 0.23731917231276325,\n  'overall_accuracy': 0.9100877192982456},\n 'data': {'y_true': array([list(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-nam_org_company', 'B-nam_pro_software', 'O']),\n         list(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-nam_org_company', 'B-nam_pro_software', 'O']),\n         list(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-nam_pro_software', 'O', 'O', 'B-nam_pro_software', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']),\n         ...,\n         list(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-nam_loc_gpe_admin1', 'O']),\n         list(['O', 'O', 'O', 'O', 'B-nam_loc_gpe_city', 'O', 'B-nam_loc_gpe_city', 'I-nam_loc_gpe_city', 'O', 'O', 'O', 'O', 'B-nam_loc_gpe_city', 'O']),\n         list(['O', 'B-nam_loc_gpe_city', 'I-nam_loc_gpe_city', 'O', 'O', 'O', 'O', 'O', 'O', 'B-nam_org_company', 'I-nam_org_company', 'O', 'O', 'O', 'O', 'O', 'O', 'B-nam_loc_hydronym_river', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])],\n        dtype=object),\n  'y_pred': array([list(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']),\n         list(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']),\n         list(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']),\n         ...,\n         list(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']),\n         list(['O', 'O', 'O', 'O', 'B-nam_loc_gpe_city', 'O', 'B-nam_org_group_team', 'I-nam_org_group_team', 'O', 'O', 'O', 'O', 'B-nam_loc_gpe_city', 'O']),\n         list(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-nam_loc_gpe_city', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])],\n        dtype=object)}}"
  },
  {
    "objectID": "tutorials/tutorial_how_to_use_config_space.html",
    "href": "tutorials/tutorial_how_to_use_config_space.html",
    "title": "How to use our configs?",
    "section": "",
    "text": "Two types of config are defined in our library: BasicConfig and AdvancedConfig. BasicConfig allows for easy use of the most common parameters in the pipeline. However, the objects defined in our pipelines are constructed in a way that they can be further paramatrized with keyword arguments. These arguments can be utilized by constructing the AdvancedConfig.\nIn summary, the BasicConfig takes arguments and automatically assign them into proper keyword group, while the AdvancedConfig takes as the input keyword groups that should be already correctly mapped.\nThe list of available config can be found below."
  },
  {
    "objectID": "tutorials/tutorial_how_to_use_config_space.html#running-pipeline-with-basicconfig",
    "href": "tutorials/tutorial_how_to_use_config_space.html#running-pipeline-with-basicconfig",
    "title": "How to use our configs?",
    "section": "Running pipeline with BasicConfig",
    "text": "Running pipeline with BasicConfig\nLet’s run example pipeline on polemo2 dataset, for tutorial purposes we use allegro/herbert-base-cased.\nBut first we downsample our dataset due to hardware limitations for that purpose we use HuggingFacePreprocessingPipeline\n\nfrom embeddings.pipeline.hf_preprocessing_pipeline import HuggingFacePreprocessingPipeline\n\n\n\nHuggingFacePreprocessingPipeline\n\n HuggingFacePreprocessingPipeline (dataset_name:str, persist_path:str, sam\n                                   ple_missing_splits:Optional[Tuple[Optio\n                                   nal[float],Optional[float]]]=None, down\n                                   sample_splits:Optional[Tuple[Optional[f\n                                   loat],Optional[float],Optional[float]]]\n                                   =None, ignore_test_subset:bool=False,\n                                   seed:int=441, load_dataset_kwargs:Optio\n                                   nal[Dict[str,Any]]=None)\n\nPreprocessing pipeline dedicated to work with HuggingFace datasets.\nThen we need to use run method\n\n\n\nPreprocessingPipeline.run\n\n PreprocessingPipeline.run ()\n\n\nprepocessing = HuggingFacePreprocessingPipeline(\n    dataset_name=\"clarin-pl/polemo2-official\",\n    persist_path=\"data/polemo2_downsampled\",\n    downsample_splits=(0.001, 0.005, 0.005)\n)\nprepocessing.run()\n\nDownloading and preparing dataset polemo2-official/all_text (download: 6.37 MiB, generated: 6.30 MiB, post-processed: Unknown size, total: 12.68 MiB) to /home/runner/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70...\nDataset polemo2-official downloaded and prepared to /home/runner/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70. Subsequent calls will reuse this data.\n\n\nDownloading builder script:   0%|          | 0.00/5.90k [00:00<?, ?B/s]Downloading builder script: 100%|##########| 5.90k/5.90k [00:00<00:00, 4.09MB/s]\nDownloading metadata:   0%|          | 0.00/23.4k [00:00<?, ?B/s]Downloading metadata: 100%|##########| 23.4k/23.4k [00:00<00:00, 11.7MB/s]\nDownloading readme:   0%|          | 0.00/5.35k [00:00<?, ?B/s]Downloading readme: 100%|##########| 5.35k/5.35k [00:00<00:00, 3.73MB/s]\nNo config specified, defaulting to: polemo2-official/all_text\nDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\nDownloading data:   0%|          | 0.00/5.37M [00:00<?, ?B/s]Downloading data: 100%|##########| 5.37M/5.37M [00:00<00:00, 59.3MB/s]\nDownloading data files: 100%|##########| 1/1 [00:00<00:00,  1.41it/s]Downloading data files: 100%|##########| 1/1 [00:00<00:00,  1.41it/s]\nExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|##########| 1/1 [00:00<00:00, 1547.71it/s]\nDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\nDownloading data:   0%|          | 0.00/663k [00:00<?, ?B/s]Downloading data: 100%|##########| 663k/663k [00:00<00:00, 49.5MB/s]\nDownloading data files: 100%|##########| 1/1 [00:00<00:00,  1.83it/s]Downloading data files: 100%|##########| 1/1 [00:00<00:00,  1.82it/s]\nExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|##########| 1/1 [00:00<00:00, 1563.29it/s]\nDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\nDownloading data:   0%|          | 0.00/649k [00:00<?, ?B/s]Downloading data: 100%|##########| 649k/649k [00:00<00:00, 41.7MB/s]\nDownloading data files: 100%|##########| 1/1 [00:00<00:00,  1.73it/s]Downloading data files: 100%|##########| 1/1 [00:00<00:00,  1.73it/s]\nExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|##########| 1/1 [00:00<00:00, 1529.09it/s]\nGenerating train split:   0%|          | 0/6573 [00:00<?, ? examples/s]Generating train split:  30%|###       | 2000/6573 [00:00<00:00, 17605.41 examples/s]Generating train split:  59%|#####8    | 3860/6573 [00:00<00:00, 18156.77 examples/s]Generating train split: 100%|##########| 6573/6573 [00:00<00:00, 18120.11 examples/s]                                                                                     Generating validation split:   0%|          | 0/823 [00:00<?, ? examples/s]                                                                           Generating test split:   0%|          | 0/820 [00:00<?, ? examples/s]                                                                       0%|          | 0/3 [00:00<?, ?it/s]100%|##########| 3/3 [00:00<00:00, 877.90it/s]\nFlattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]Flattening the indices: 100%|##########| 1/1 [00:00<00:00, 729.95ba/s]\nSaving the dataset (0/1 shards):   0%|          | 0/7 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|##########| 7/7 [00:00<00:00, 3144.83 examples/s]                                                                                       Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]Flattening the indices: 100%|##########| 1/1 [00:00<00:00, 852.85ba/s]\nSaving the dataset (0/1 shards):   0%|          | 0/5 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|##########| 5/5 [00:00<00:00, 2335.10 examples/s]                                                                                       Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]Flattening the indices: 100%|##########| 1/1 [00:00<00:00, 855.63ba/s]\nSaving the dataset (0/1 shards):   0%|          | 0/5 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|##########| 5/5 [00:00<00:00, 2343.97 examples/s]                                                                                       \n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'target'],\n        num_rows: 7\n    })\n    validation: Dataset({\n        features: ['text', 'target'],\n        num_rows: 5\n    })\n    test: Dataset({\n        features: ['text', 'target'],\n        num_rows: 5\n    })\n})\n\n\nWe have now our data prepared locally, now we need to define our pipeline.\nLet’s start from config. We will use parameters from clarin-pl/lepiszcze-allegro__herbert-base-cased-polemo2, which configuration was obtained from extensive hyperparmeter search.\n\n\n\n\n\n\nWarning\n\n\n\nDue to hardware limitation we limit parmeter max_epochs to 1 and we leave early stopping configuration parameters as defaults\n\n\n\n\n\nLightningBasicConfig\n\n LightningBasicConfig (use_scheduler:bool=True, optimizer:str='Adam',\n                       warmup_steps:int=100, learning_rate:float=0.0001,\n                       adam_epsilon:float=1e-08, weight_decay:float=0.0,\n                       finetune_last_n_layers:int=-1,\n                       classifier_dropout:Optional[float]=None,\n                       max_seq_length:Optional[int]=None,\n                       batch_size:int=32, max_epochs:Optional[int]=None,\n                       early_stopping_monitor:str='val/Loss',\n                       early_stopping_mode:str='min',\n                       early_stopping_patience:int=3)\n\n\ncfg = LightningBasicConfig(\n        use_scheduler=True,\n        optimizer=\"Adam\",\n        warmup_steps=100,\n        learning_rate=0.001,\n        adam_epsilon=1e-06,\n        weight_decay=0.001,\n        finetune_last_n_layers=3,\n        classifier_dropout=0.2,\n        max_seq_length=None,\n        batch_size=64,\n        max_epochs=1,\n)\ncfg\n\nLightningBasicConfig(use_scheduler=True, optimizer='Adam', warmup_steps=100, learning_rate=0.001, adam_epsilon=1e-06, weight_decay=0.001, finetune_last_n_layers=3, classifier_dropout=0.2, max_seq_length=None, batch_size=64, max_epochs=1, early_stopping_monitor='val/Loss', early_stopping_mode='min', early_stopping_patience=3, tokenizer_kwargs={}, batch_encoding_kwargs={}, dataloader_kwargs={})\n\n\nNow we define pipeline dedicated for text classification LightningClassificationPipeline\n\nfrom embeddings.pipeline.lightning_classification import LightningClassificationPipeline\n\n\n\n\nLightningClassificationPipeline\n\n LightningClassificationPipeline\n                                  (embedding_name_or_path:Union[str,pathli\n                                  b.Path], dataset_name_or_path:Union[str,\n                                  pathlib.Path], input_column_name:Union[s\n                                  tr,Sequence[str]],\n                                  target_column_name:str,\n                                  output_path:Union[str,pathlib.Path], eva\n                                  luation_filename:str='evaluation.json', \n                                  config:Union[embeddings.config.lightning\n                                  _config.LightningBasicConfig,embeddings.\n                                  config.lightning_config.LightningAdvance\n                                  dConfig]=LightningBasicConfig(use_schedu\n                                  ler=True, optimizer='Adam',\n                                  warmup_steps=100, learning_rate=0.0001,\n                                  adam_epsilon=1e-08, weight_decay=0.0,\n                                  finetune_last_n_layers=-1,\n                                  classifier_dropout=None,\n                                  max_seq_length=None, batch_size=32,\n                                  max_epochs=None,\n                                  early_stopping_monitor='val/Loss',\n                                  early_stopping_mode='min',\n                                  early_stopping_patience=3,\n                                  tokenizer_kwargs={},\n                                  batch_encoding_kwargs={},\n                                  dataloader_kwargs={}), devices:Union[int\n                                  ,List[int],str,NoneType]='auto', acceler\n                                  ator:Union[str,pytorch_lightning.acceler\n                                  ators.accelerator.Accelerator,NoneType]=\n                                  'auto', logging_config:embeddings.utils.\n                                  loggers.LightningLoggingConfig=Lightning\n                                  LoggingConfig(loggers_names=[],\n                                  tracking_project_name=None,\n                                  wandb_entity=None,\n                                  wandb_logger_kwargs={}), tokenizer_name_\n                                  or_path:Union[pathlib.Path,str,NoneType]\n                                  =None, predict_subset:embeddings.data.da\n                                  taset.LightingDataModuleSubset=<Lighting\n                                  DataModuleSubset.TEST: 'test'>, load_dat\n                                  aset_kwargs:Optional[Dict[str,Any]]=None\n                                  , model_checkpoint_kwargs:Optional[Dict[\n                                  str,Any]]=None)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nfrom dataclasses import asdict # For metrics conversion\nimport pandas as pd  # For metrics conversion\n\n\npipeline = LightningClassificationPipeline(\n    embedding_name_or_path=\"allegro/herbert-base-cased\",\n    dataset_name_or_path=\"data/polemo2_downsampled/\",\n    input_column_name=\"text\",\n    target_column_name=\"target\",\n    output_path=\".\",\n    config=cfg\n)\n\nDownloading (…)okenizer_config.json:   0%|          | 0.00/229 [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|##########| 229/229 [00:00<00:00, 40.0kB/s]\nDownloading (…)lve/main/config.json:   0%|          | 0.00/472 [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|##########| 472/472 [00:00<00:00, 85.9kB/s]\nDownloading (…)olve/main/vocab.json:   0%|          | 0.00/907k [00:00<?, ?B/s]Downloading (…)olve/main/vocab.json: 100%|##########| 907k/907k [00:00<00:00, 44.5MB/s]\nDownloading (…)olve/main/merges.txt:   0%|          | 0.00/556k [00:00<?, ?B/s]Downloading (…)olve/main/merges.txt: 100%|##########| 556k/556k [00:00<00:00, 32.3MB/s]\nDownloading (…)cial_tokens_map.json:   0%|          | 0.00/129 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|##########| 129/129 [00:00<00:00, 47.2kB/s]\n  0%|          | 0/1 [00:00<?, ?ba/s]100%|##########| 1/1 [00:00<00:00, 87.85ba/s]\n  0%|          | 0/1 [00:00<?, ?ba/s]100%|##########| 1/1 [00:00<00:00, 163.62ba/s]\n  0%|          | 0/1 [00:00<?, ?ba/s]100%|##########| 1/1 [00:00<00:00, 179.37ba/s]\nCasting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]Casting the dataset: 100%|##########| 1/1 [00:00<00:00, 803.20ba/s]\nCasting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]Casting the dataset: 100%|##########| 1/1 [00:00<00:00, 1120.87ba/s]\nCasting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]Casting the dataset: 100%|##########| 1/1 [00:00<00:00, 1105.22ba/s]\n\n\nSimilarly as with HuggingFacePreprocessingPipeline we use run method\n\n\n\nLightningPipeline.run\n\n LightningPipeline.run (run_name:Optional[str]=None)\n\n\nmetrics = pipeline.run()\n\n# Converting metrics to DataFrame for better nb display\n\nmetrics = pd.DataFrame.from_dict(asdict(metrics), orient=\"index\", columns=[\"values\"])\nmetrics\n\nValidation sanity check: 0it [00:00, ?it/s]Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]Validation sanity check: 100%|##########| 1/1 [00:00<00:00,  1.12it/s]                                                                      Training: 0it [00:00, ?it/s]Training:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] Epoch 0:  50%|#####     | 1/2 [00:03<00:03,  3.48s/it]Epoch 0:  50%|#####     | 1/2 [00:03<00:03,  3.48s/it, loss=1.6, v_num=, train/BaseLR=0.000, train/LambdaLR=0.000]\nValidating: 0it [00:00, ?it/s]\nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\nValidating: 100%|##########| 1/1 [00:00<00:00,  1.08it/s]Epoch 0: 100%|##########| 2/2 [00:04<00:00,  2.21s/it, loss=1.6, v_num=, train/BaseLR=0.000, train/LambdaLR=0.000, val/MulticlassAccuracy=0.200, val/MulticlassPrecision=0.050, val/MulticlassRecall=0.250, val/MulticlassF1Score=0.0833]\n                                                         Epoch 0: 100%|##########| 2/2 [00:05<00:00,  2.98s/it, loss=1.6, v_num=, train/BaseLR=0.000, train/LambdaLR=0.000, val/MulticlassAccuracy=0.200, val/MulticlassPrecision=0.050, val/MulticlassRecall=0.250, val/MulticlassF1Score=0.0833]\nTesting: 0it [00:00, ?it/s]Testing: 100%|##########| 1/1 [00:00<00:00,  1.20it/s]--------------------------------------------------------------------------------\nDATALOADER:0 TEST RESULTS\n{'test/Loss': 1.6176023483276367,\n 'test/MulticlassAccuracy': 0.0,\n 'test/MulticlassF1Score': 0.0,\n 'test/MulticlassPrecision': 0.0,\n 'test/MulticlassRecall': 0.0}\n--------------------------------------------------------------------------------\nTesting: 100%|##########| 1/1 [00:00<00:00,  1.19it/s]\nPredicting: 1it [00:00, ?it/s]Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]\n\n\nDownloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/654M [00:00<?, ?B/s]Downloading (…)\"pytorch_model.bin\";:   2%|1         | 10.5M/654M [00:00<00:14, 44.7MB/s]Downloading (…)\"pytorch_model.bin\";:   3%|3         | 21.0M/654M [00:00<00:13, 47.5MB/s]Downloading (…)\"pytorch_model.bin\";:   5%|4         | 31.5M/654M [00:00<00:11, 55.5MB/s]Downloading (…)\"pytorch_model.bin\";:   6%|6         | 41.9M/654M [00:01<00:17, 34.6MB/s]Downloading (…)\"pytorch_model.bin\";:   8%|8         | 52.4M/654M [00:01<00:15, 39.5MB/s]Downloading (…)\"pytorch_model.bin\";:  10%|9         | 62.9M/654M [00:01<00:14, 41.6MB/s]Downloading (…)\"pytorch_model.bin\";:  11%|#1        | 73.4M/654M [00:01<00:15, 36.7MB/s]Downloading (…)\"pytorch_model.bin\";:  13%|#2        | 83.9M/654M [00:02<00:15, 36.9MB/s]Downloading (…)\"pytorch_model.bin\";:  14%|#4        | 94.4M/654M [00:02<00:16, 34.9MB/s]Downloading (…)\"pytorch_model.bin\";:  16%|#6        | 105M/654M [00:02<00:14, 38.8MB/s] Downloading (…)\"pytorch_model.bin\";:  18%|#7        | 115M/654M [00:02<00:11, 44.9MB/s]Downloading (…)\"pytorch_model.bin\";:  19%|#9        | 126M/654M [00:03<00:13, 39.1MB/s]Downloading (…)\"pytorch_model.bin\";:  21%|##        | 136M/654M [00:03<00:12, 40.6MB/s]Downloading (…)\"pytorch_model.bin\";:  22%|##2       | 147M/654M [00:03<00:12, 41.3MB/s]Downloading (…)\"pytorch_model.bin\";:  24%|##4       | 157M/654M [00:03<00:10, 45.4MB/s]Downloading (…)\"pytorch_model.bin\";:  26%|##5       | 168M/654M [00:04<00:12, 37.5MB/s]Downloading (…)\"pytorch_model.bin\";:  27%|##7       | 178M/654M [00:04<00:13, 35.3MB/s]Downloading (…)\"pytorch_model.bin\";:  29%|##8       | 189M/654M [00:04<00:11, 40.3MB/s]Downloading (…)\"pytorch_model.bin\";:  30%|###       | 199M/654M [00:05<00:12, 36.9MB/s]Downloading (…)\"pytorch_model.bin\";:  32%|###2      | 210M/654M [00:05<00:14, 29.8MB/s]Downloading (…)\"pytorch_model.bin\";:  34%|###3      | 220M/654M [00:05<00:12, 35.4MB/s]Downloading (…)\"pytorch_model.bin\";:  35%|###5      | 231M/654M [00:05<00:10, 41.0MB/s]Downloading (…)\"pytorch_model.bin\";:  37%|###6      | 241M/654M [00:06<00:09, 44.7MB/s]Downloading (…)\"pytorch_model.bin\";:  38%|###8      | 252M/654M [00:06<00:10, 39.0MB/s]Downloading (…)\"pytorch_model.bin\";:  40%|####      | 262M/654M [00:06<00:09, 42.2MB/s]Downloading (…)\"pytorch_model.bin\";:  42%|####1     | 273M/654M [00:06<00:09, 42.1MB/s]Downloading (…)\"pytorch_model.bin\";:  43%|####3     | 283M/654M [00:07<00:08, 43.7MB/s]Downloading (…)\"pytorch_model.bin\";:  45%|####4     | 294M/654M [00:07<00:09, 36.3MB/s]Downloading (…)\"pytorch_model.bin\";:  46%|####6     | 304M/654M [00:07<00:09, 38.6MB/s]Downloading (…)\"pytorch_model.bin\";:  48%|####8     | 315M/654M [00:07<00:08, 42.3MB/s]Downloading (…)\"pytorch_model.bin\";:  50%|####9     | 325M/654M [00:08<00:07, 43.5MB/s]Downloading (…)\"pytorch_model.bin\";:  51%|#####1    | 336M/654M [00:08<00:11, 28.7MB/s]Downloading (…)\"pytorch_model.bin\";:  53%|#####2    | 346M/654M [00:09<00:09, 31.2MB/s]Downloading (…)\"pytorch_model.bin\";:  54%|#####4    | 357M/654M [00:09<00:09, 30.9MB/s]Downloading (…)\"pytorch_model.bin\";:  56%|#####6    | 367M/654M [00:09<00:09, 31.1MB/s]Downloading (…)\"pytorch_model.bin\";:  58%|#####7    | 377M/654M [00:10<00:10, 26.6MB/s]Downloading (…)\"pytorch_model.bin\";:  59%|#####9    | 388M/654M [00:10<00:09, 29.1MB/s]Downloading (…)\"pytorch_model.bin\";:  61%|######    | 398M/654M [00:10<00:07, 32.8MB/s]Downloading (…)\"pytorch_model.bin\";:  63%|######2   | 409M/654M [00:11<00:06, 35.7MB/s]Downloading (…)\"pytorch_model.bin\";:  64%|######4   | 419M/654M [00:11<00:06, 35.9MB/s]Downloading (…)\"pytorch_model.bin\";:  66%|######5   | 430M/654M [00:11<00:05, 37.5MB/s]Downloading (…)\"pytorch_model.bin\";:  67%|######7   | 440M/654M [00:11<00:05, 41.4MB/s]Downloading (…)\"pytorch_model.bin\";:  69%|######8   | 451M/654M [00:11<00:04, 45.3MB/s]Downloading (…)\"pytorch_model.bin\";:  71%|#######   | 461M/654M [00:12<00:05, 34.2MB/s]Downloading (…)\"pytorch_model.bin\";:  72%|#######2  | 472M/654M [00:12<00:04, 36.5MB/s]Downloading (…)\"pytorch_model.bin\";:  74%|#######3  | 482M/654M [00:12<00:04, 41.3MB/s]Downloading (…)\"pytorch_model.bin\";:  75%|#######5  | 493M/654M [00:13<00:03, 44.5MB/s]Downloading (…)\"pytorch_model.bin\";:  77%|#######6  | 503M/654M [00:13<00:04, 33.0MB/s]Downloading (…)\"pytorch_model.bin\";:  79%|#######8  | 514M/654M [00:13<00:03, 35.8MB/s]Downloading (…)\"pytorch_model.bin\";:  80%|########  | 524M/654M [00:13<00:03, 41.8MB/s]Downloading (…)\"pytorch_model.bin\";:  82%|########1 | 535M/654M [00:14<00:02, 45.6MB/s]Downloading (…)\"pytorch_model.bin\";:  83%|########3 | 545M/654M [00:14<00:02, 40.2MB/s]Downloading (…)\"pytorch_model.bin\";:  85%|########4 | 556M/654M [00:14<00:02, 44.9MB/s]Downloading (…)\"pytorch_model.bin\";:  87%|########6 | 566M/654M [00:14<00:01, 46.3MB/s]Downloading (…)\"pytorch_model.bin\";:  88%|########8 | 577M/654M [00:15<00:01, 44.7MB/s]Downloading (…)\"pytorch_model.bin\";:  90%|########9 | 587M/654M [00:15<00:01, 44.3MB/s]Downloading (…)\"pytorch_model.bin\";:  91%|#########1| 598M/654M [00:15<00:01, 46.1MB/s]Downloading (…)\"pytorch_model.bin\";:  93%|#########2| 608M/654M [00:15<00:00, 49.0MB/s]Downloading (…)\"pytorch_model.bin\";:  95%|#########4| 619M/654M [00:15<00:00, 49.5MB/s]Downloading (…)\"pytorch_model.bin\";:  96%|#########6| 629M/654M [00:16<00:00, 39.1MB/s]Downloading (…)\"pytorch_model.bin\";:  98%|#########7| 640M/654M [00:16<00:00, 39.1MB/s]Downloading (…)\"pytorch_model.bin\";:  99%|#########9| 650M/654M [00:16<00:00, 43.3MB/s]Downloading (…)\"pytorch_model.bin\";: 100%|##########| 654M/654M [00:16<00:00, 38.9MB/s]\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:407: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n  rank_zero_warn(\n/home/runner/work/embeddings/embeddings/embeddings/metric/hugging_face_metric.py:27: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  datasets.load_metric(metric, **init_kwargs) if isinstance(metric, str) else metric\nDownloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]Downloading builder script: 4.21kB [00:00, 3.02MB/s]                   \nDownloading builder script:   0%|          | 0.00/2.32k [00:00<?, ?B/s]Downloading builder script: 6.50kB [00:00, 3.91MB/s]                   \nDownloading builder script:   0%|          | 0.00/2.52k [00:00<?, ?B/s]Downloading builder script: 7.38kB [00:00, 4.28MB/s]                   \nDownloading builder script:   0%|          | 0.00/2.58k [00:00<?, ?B/s]Downloading builder script: 7.55kB [00:00, 5.26MB/s]                   \n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n\n\n\n  \n    \n      \n      values\n    \n  \n  \n    \n      accuracy\n      0.0\n    \n    \n      f1_macro\n      0.0\n    \n    \n      f1_micro\n      0.0\n    \n    \n      f1_weighted\n      0.0\n    \n    \n      recall_macro\n      0.0\n    \n    \n      recall_micro\n      0.0\n    \n    \n      recall_weighted\n      0.0\n    \n    \n      precision_macro\n      0.0\n    \n    \n      precision_micro\n      0.0\n    \n    \n      precision_weighted\n      0.0\n    \n    \n      classes\n      {0: {'precision': 0.0, 'recall': 0.0, 'f1': 0....\n    \n    \n      data\n      {'y_pred': [0, 0, 0, 0, 0], 'y_true': [1, 1, 1..."
  },
  {
    "objectID": "tutorials/tutorial_how_to_use_config_space.html#running-pipeline-with-advancedconfig",
    "href": "tutorials/tutorial_how_to_use_config_space.html#running-pipeline-with-advancedconfig",
    "title": "How to use our configs?",
    "section": "Running pipeline with AdvancedConfig",
    "text": "Running pipeline with AdvancedConfig\nAs mentioned in previous section LightningBasicConfig is only limited to most important parameters.\nLet’s see an example of the process of defining the parameters in our LightningAdvancedConfig. Tracing back different kwargs we can find:\n\ntask_train_kwargs Parameters that are passed to the Lightning Trainer object.\ntask_model_kwargs Parameters that are passed to the Lightning module object (we use TextClassificationModule which inherits from HuggingFaceLightningModule and HuggingFaceLightningModule).\ndatamodule_kwargs\nParameters passed to the datamodule classes, currently HuggingFaceDataModule takes several arguments (such as max_seq_length, processing_batch_size or downsamples args) as an input\nbatch_encoding_kwargs Parameters that are defined in __call__ method of the tokenizer which allow for manipulation of the tokenized text by setting parameters such as truncation, padding, stride etc. and specifying the return format of the tokenized text\ntokenizer_kwargs This is a generic configuration class of the hugginface model’s tokenizer, possible parameters depends on the tokenizer that is used. For example for bert uncased tokenizer these parameters are present here: https://huggingface.co/bert-base-uncased/blob/main/tokenizer_config.json\nload_dataset_kwargs Keyword arguments from the datasets.load_dataset method which loads a dataset from the Hugging Face Hub, or a local dataset; mostly metadata for downloading, loading, caching the dataset\nmodel_config_kwargs This is a generic configuration class of the hugginface model, possible parameters depends on the model that is used. For example for bert uncased these parameters are present here: https://huggingface.co/bert-base-uncased/blob/main/config.json\nearly_stopping_kwargs Params defined in __init__ of the EarlyStopping lightning callback; you can specify a metric to monitor and conditions to stop training when it stops improving\ndataloader_kwargs Defined in __init__ of the torch DataLoader object which wraps an iterable around the Dataset to enable easy access to the sample; specify params such as num of workers, sampling or shuffling\n\n\nadvanced_config = LightningAdvancedConfig(\n    finetune_last_n_layers=0,\n    datamodule_kwargs={\n        \"max_seq_length\": None,\n    },\n    task_train_kwargs={\n        \"max_epochs\": 1,\n        \"devices\": \"auto\",\n        \"accelerator\": \"cpu\",\n        \"deterministic\": True,\n    },\n    task_model_kwargs={\n        \"learning_rate\": 0.001,\n        \"train_batch_size\": 64,\n        \"eval_batch_size\": 64,\n        \"use_scheduler\": True,\n        \"optimizer\": \"Adam\",\n        \"adam_epsilon\": 1e-6,\n        \"warmup_steps\": 100,\n        \"weight_decay\": 0.001,\n    },\n    early_stopping_kwargs=None,\n    model_config_kwargs={\"classifier_dropout\": 0.2},\n    tokenizer_kwargs={},\n    batch_encoding_kwargs={},\n    dataloader_kwargs={}\n)\nadvanced_config\n\nLightningAdvancedConfig(finetune_last_n_layers=0, task_model_kwargs={'learning_rate': 0.001, 'train_batch_size': 64, 'eval_batch_size': 64, 'use_scheduler': True, 'optimizer': 'Adam', 'adam_epsilon': 1e-06, 'warmup_steps': 100, 'weight_decay': 0.001}, datamodule_kwargs={'max_seq_length': None}, task_train_kwargs={'max_epochs': 1, 'devices': 'auto', 'accelerator': 'cpu', 'deterministic': True}, model_config_kwargs={'classifier_dropout': 0.2}, early_stopping_kwargs=None, tokenizer_kwargs={}, batch_encoding_kwargs={}, dataloader_kwargs={})\n\n\nNow we can run pipeline\n\npipeline = LightningClassificationPipeline(\n    embedding_name_or_path=\"allegro/herbert-base-cased\",\n    dataset_name_or_path=\"data/polemo2_downsampled/\",\n    input_column_name=\"text\",\n    target_column_name=\"target\",\n    output_path=\".\",\n    config=advanced_config\n)\n\nmetrics_adv_cfg = pipeline.run()\n\n# Converting metrics to DataFrame for better nb display\n\nmetrics_adv_cfg = pd.DataFrame.from_dict(asdict(metrics_adv_cfg), orient=\"index\", columns=[\"values\"])\nmetrics_adv_cfg\n\nValidation sanity check: 0it [00:00, ?it/s]Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]Validation sanity check: 100%|##########| 1/1 [00:00<00:00,  1.12it/s]                                                                      Training: 0it [00:00, ?it/s]Training:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] Epoch 0:  50%|#####     | 1/2 [00:02<00:02,  2.12s/it]Epoch 0:  50%|#####     | 1/2 [00:02<00:02,  2.12s/it, loss=1.5, v_num=, train/BaseLR=0.000, train/LambdaLR=0.000]\nValidating: 0it [00:00, ?it/s]\nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\nValidating: 100%|##########| 1/1 [00:00<00:00,  1.16it/s]Epoch 0: 100%|##########| 2/2 [00:02<00:00,  1.50s/it, loss=1.5, v_num=, train/BaseLR=0.000, train/LambdaLR=0.000, val/MulticlassAccuracy=0.200, val/MulticlassPrecision=0.0625, val/MulticlassRecall=0.250, val/MulticlassF1Score=0.100]\n                                                         Epoch 0: 100%|##########| 2/2 [00:04<00:00,  2.03s/it, loss=1.5, v_num=, train/BaseLR=0.000, train/LambdaLR=0.000, val/MulticlassAccuracy=0.200, val/MulticlassPrecision=0.0625, val/MulticlassRecall=0.250, val/MulticlassF1Score=0.100]\nTesting: 0it [00:00, ?it/s]Testing: 100%|##########| 1/1 [00:00<00:00,  1.21it/s]--------------------------------------------------------------------------------\nDATALOADER:0 TEST RESULTS\n{'test/Loss': 1.3993116617202759,\n 'test/MulticlassAccuracy': 0.0,\n 'test/MulticlassF1Score': 0.0,\n 'test/MulticlassPrecision': 0.0,\n 'test/MulticlassRecall': 0.0}\n--------------------------------------------------------------------------------\nTesting: 100%|##########| 1/1 [00:00<00:00,  1.20it/s]\nPredicting: 1it [00:00, ?it/s]Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]\n\n\nLoading cached processed dataset at /home/runner/work/embeddings/embeddings/data/polemo2_downsampled/train/cache-798c0784c1839f82.arrow\n  0%|          | 0/1 [00:00<?, ?ba/s]100%|##########| 1/1 [00:00<00:00, 170.20ba/s]\nLoading cached processed dataset at /home/runner/work/embeddings/embeddings/data/polemo2_downsampled/test/cache-e6d987954fa38d1f.arrow\nLoading cached processed dataset at /home/runner/work/embeddings/embeddings/data/polemo2_downsampled/train/cache-c077b5bab67b5851.arrow\nCasting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]Casting the dataset: 100%|##########| 1/1 [00:00<00:00, 923.45ba/s]\nLoading cached processed dataset at /home/runner/work/embeddings/embeddings/data/polemo2_downsampled/test/cache-e5f15840a94d9906.arrow\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:623: UserWarning: Checkpoint directory /home/runner/work/embeddings/embeddings/checkpoints exists and is not empty.\n  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:407: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n  rank_zero_warn(\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n\n\n\n  \n    \n      \n      values\n    \n  \n  \n    \n      accuracy\n      0.0\n    \n    \n      f1_macro\n      0.0\n    \n    \n      f1_micro\n      0.0\n    \n    \n      f1_weighted\n      0.0\n    \n    \n      recall_macro\n      0.0\n    \n    \n      recall_micro\n      0.0\n    \n    \n      recall_weighted\n      0.0\n    \n    \n      precision_macro\n      0.0\n    \n    \n      precision_micro\n      0.0\n    \n    \n      precision_weighted\n      0.0\n    \n    \n      classes\n      {0: {'precision': 0.0, 'recall': 0.0, 'f1': 0....\n    \n    \n      data\n      {'y_pred': [0, 0, 0, 0, 0], 'y_true': [1, 1, 1..."
  },
  {
    "objectID": "tutorials/validate_flair_models_inference.html",
    "href": "tutorials/validate_flair_models_inference.html",
    "title": "Model inference",
    "section": "",
    "text": "import os\n\nos.chdir(\"..\")\n\nfrom embeddings.data.data_loader import HuggingFaceDataLoader\nfrom embeddings.defaults import DATASET_PATH, RESULTS_PATH\nfrom embeddings.embedding.auto_flair import AutoFlairWordEmbedding\nfrom embeddings.evaluator.sequence_labeling_evaluator import SequenceLabelingEvaluator\nfrom embeddings.model.flair_model import FlairModel\nfrom embeddings.pipeline.standard_pipeline import StandardPipeline\nfrom embeddings.task.flair_task.sequence_labeling import SequenceLabeling\nfrom embeddings.transformation.flair_transformation.column_corpus_transformation import (\n    ColumnCorpusTransformation,\n)\nfrom embeddings.data.dataset import Dataset\n\nfrom embeddings.transformation.flair_transformation.downsample_corpus_transformation import (\n    DownsampleFlairCorpusTransformation,\n)\nfrom embeddings.transformation.flair_transformation.split_sample_corpus_transformation import (\n    SampleSplitsFlairCorpusTransformation,\n)\nfrom embeddings.utils.utils import build_output_path\n\n\nRun downsampled flair pipeline\n\nembedding_name_or_path = \"clarin-pl/word2vec-kgr10\"\ndataset_name = \"clarin-pl/kpwr-ner\"\n\noutput_path = build_output_path(RESULTS_PATH, embedding_name_or_path, dataset_name)\n\ndataset = Dataset(dataset_name)\ndata_loader = HuggingFaceDataLoader()\ntransformation = (\n    ColumnCorpusTransformation(\"tokens\", \"ner\")\n    .then(SampleSplitsFlairCorpusTransformation(dev_fraction=0.1, seed=441))\n    .then(DownsampleFlairCorpusTransformation(downsample_train=0.005, downsample_dev=0.01, downsample_test=0.01))\n)\ntask = SequenceLabeling(\n    output_path,\n    hidden_size=256,\n    task_train_kwargs={\"max_epochs\": 1, \"mini_batch_size\": 64},\n)\nembedding = AutoFlairWordEmbedding.from_hub(embedding_name_or_path)\nmodel = FlairModel(embedding, task)\nevaluator = SequenceLabelingEvaluator()\n\npipeline = StandardPipeline(dataset, data_loader, transformation, model, evaluator)\n\n\n_ = pipeline.run()\n\n\n\nLoad model from checkpoint\n\n!ls $output_path\n\n\ntask_from_ckpt = SequenceLabeling.from_checkpoint(checkpoint_path=(output_path / \"final-model.pt\"), output_path=output_path)\n\n\n\nPredict for test data\n\nloaded_data = data_loader.load(dataset)\ntransformed_data = transformation.transform(loaded_data)\ntest_data = transformed_data.test\n\n\ny_pred, loss = task_from_ckpt.predict(test_data)\ny_true = task_from_ckpt.get_y(test_data, task_from_ckpt.y_type, task_from_ckpt.y_dictionary)\n\n\nevaluator.evaluate({\"y_pred\": y_pred, \"y_true\": y_true})"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CLARIN Embeddings",
    "section": "",
    "text": "Warning\n\n\n\nThe library is currently in an active development state. Some functionalities may be subject to change before the stable release. Users can track our milestones here."
  },
  {
    "objectID": "index.html#writing-custom-huggingface-based-pipeline",
    "href": "index.html#writing-custom-huggingface-based-pipeline",
    "title": "CLARIN Embeddings",
    "section": "Writing custom HuggingFace-based pipeline",
    "text": "Writing custom HuggingFace-based pipeline\nfrom pathlib import Path\n\nfrom embeddings.data.data_loader import HuggingFaceDataLoader\nfrom embeddings.data.dataset import Dataset\nfrom embeddings.embedding.auto_flair import AutoFlairDocumentEmbedding\nfrom embeddings.evaluator.text_classification_evaluator import TextClassificationEvaluator\nfrom embeddings.model.flair_model import FlairModel\nfrom embeddings.pipeline.standard_pipeline import StandardPipeline\nfrom embeddings.task.flair_task.text_classification import TextClassification\nfrom embeddings.transformation.flair_transformation.classification_corpus_transformation import (\n    ClassificationCorpusTransformation,\n)\n\ndataset = Dataset(\"clarin-pl/polemo2-official\")\ndata_loader = HuggingFaceDataLoader()\ntransformation = ClassificationCorpusTransformation(\"text\", \"target\")\nembedding = AutoFlairDocumentEmbedding.from_hub(\"clarin-pl/word2vec-kgr10\")\ntask = TextClassification(Path(\".\"))\nmodel = FlairModel(embedding, task)\nevaluator = TextClassificationEvaluator()\n\npipeline = StandardPipeline(dataset, data_loader, transformation, model, evaluator)\nresult = pipeline.run()"
  },
  {
    "objectID": "index.html#run-classification-task",
    "href": "index.html#run-classification-task",
    "title": "CLARIN Embeddings",
    "section": "Run classification task",
    "text": "Run classification task\nThe example with non-default arguments\npython evaluate_lightning_document_classification.py \\\n    --embedding-name-or-path allegro/herbert-base-cased \\\n    --dataset-name clarin-pl/polemo2-official \\\n    --input-columns-name text \\\n    --target-column-name target"
  },
  {
    "objectID": "index.html#run-sequence-labeling-task",
    "href": "index.html#run-sequence-labeling-task",
    "title": "CLARIN Embeddings",
    "section": "Run sequence labeling task",
    "text": "Run sequence labeling task\nThe example with default language model and dataset.\npython evaluate_lightning_sequence_labeling.py"
  },
  {
    "objectID": "index.html#run-pair-classification-task",
    "href": "index.html#run-pair-classification-task",
    "title": "CLARIN Embeddings",
    "section": "Run pair classification task",
    "text": "Run pair classification task\nThe example with static embedding model.\npython evaluate_document_pair_classification.py \\\n    --embedding-name-or-path clarin-pl/word2vec-kgr10"
  },
  {
    "objectID": "index.html#example-with-polemo2-dataset",
    "href": "index.html#example-with-polemo2-dataset",
    "title": "CLARIN Embeddings",
    "section": "Example with polemo2 dataset",
    "text": "Example with polemo2 dataset\n\nFlair pipeline\nfrom embeddings.pipeline.flair_classification import FlairClassificationPipeline\nfrom embeddings.config.flair_config import FlairTextClassificationAdvancedConfig\n\nconfig = FlairTextClassificationAdvancedConfig(\n    task_model_kwargs={\n        \"loss_weights\": {\n            \"plus\": 2.0,\n            \"minus\": 2.0\n        }\n    },\n    task_train_kwargs={\n        \"learning_rate\": 0.01,\n        \"max_epochs\": 20\n    }\n)\npipeline = FlairClassificationPipeline(\n    dataset_name=\"clarin-pl/polemo2-official\",\n    embedding_name=\"clarin-pl/word2vec-kgr10\",\n    input_column_name=\"text\",\n    target_column_name=\"target\",\n    output_path=\".\",\n    config=config\n)\n\nprint(pipeline.run())\n\n\nLightning pipeline\nfrom embeddings.config.lightning_config import LightningBasicConfig\nfrom embeddings.pipeline.lightning_classification import LightningClassificationPipeline\n\nconfig = LightningBasicConfig(\n    learning_rate=0.01, max_epochs=1, max_seq_length=128, finetune_last_n_layers=0,\n    accelerator=\"cpu\"\n)\n\npipeline = LightningClassificationPipeline(\n    embedding_name_or_path=\"allegro/herbert-base-cased\",\n    dataset_name_or_path=\"clarin-pl/polemo2-official\",\n    input_column_name=[\"text\"],\n    target_column_name=\"target\",\n    load_dataset_kwargs={\n        \"train_domains\": [\"hotels\", \"medicine\"],\n        \"dev_domains\": [\"hotels\", \"medicine\"],\n        \"test_domains\": [\"hotels\", \"medicine\"],\n        \"text_cfg\": \"text\",\n    },\n    output_path=\".\",\n    config=config\n)\nYou can also define an Advanced config with populated keyword arguments. In general, the keywords are passed to the object when constructing specific pipelines. We can identify and trace the keyword arguments to find the possible arguments that can be set in the config kwargs.\nfrom embeddings.config.lightning_config import LightningAdvancedConfig\n\nconfig = LightningAdvancedConfig(\n    finetune_last_n_layers=0,\n    task_train_kwargs={\n        \"max_epochs\": 1,\n        \"devices\": \"auto\",\n        \"accelerator\": \"cpu\",\n        \"deterministic\": True,\n    },\n    task_model_kwargs={\n        \"learning_rate\": 5e-4,\n        \"use_scheduler\": False,\n        \"optimizer\": \"AdamW\",\n        \"adam_epsilon\": 1e-8,\n        \"warmup_steps\": 100,\n        \"weight_decay\": 0.0,\n    },\n    datamodule_kwargs={\n        \"downsample_train\": 0.01,\n        \"downsample_val\": 0.01,\n        \"downsample_test\": 0.05,\n    },\n    dataloader_kwargs={\"num_workers\": 0},\n)"
  },
  {
    "objectID": "index.html#document-embeddings",
    "href": "index.html#document-embeddings",
    "title": "CLARIN Embeddings",
    "section": "Document embeddings",
    "text": "Document embeddings\nfrom flair.data import Sentence\n\nfrom embeddings.embedding.auto_flair import AutoFlairDocumentEmbedding\n\nsentence = Sentence(\"Myśl z duszy leci bystro, Nim się w słowach złamie.\")\n\nembedding = AutoFlairDocumentEmbedding.from_hub(\"clarin-pl/word2vec-kgr10\")\nembedding.embed([sentence])\n\nprint(sentence.embedding)"
  },
  {
    "objectID": "index.html#word-embeddings",
    "href": "index.html#word-embeddings",
    "title": "CLARIN Embeddings",
    "section": "Word embeddings",
    "text": "Word embeddings\nfrom flair.data import Sentence\n\nfrom embeddings.embedding.auto_flair import AutoFlairWordEmbedding\n\nsentence = Sentence(\"Myśl z duszy leci bystro, Nim się w słowach złamie.\")\n\nembedding = AutoFlairWordEmbedding.from_hub(\"clarin-pl/word2vec-kgr10\")\nembedding.embed([sentence])\n\nfor token in sentence:\n    print(token)\n    print(token.embedding)"
  },
  {
    "objectID": "index.html#transformers-embeddings",
    "href": "index.html#transformers-embeddings",
    "title": "CLARIN Embeddings",
    "section": "Transformers embeddings",
    "text": "Transformers embeddings\n\n\n\nTask\nOptimized Pipeline\n\n\n\n\nLightning Text Classification\nOptimizedLightingClassificationPipeline\n\n\nLightning Sequence Labeling\nOptimizedLightingSequenceLabelingPipeline"
  },
  {
    "objectID": "index.html#static-embeddings-1",
    "href": "index.html#static-embeddings-1",
    "title": "CLARIN Embeddings",
    "section": "Static embeddings",
    "text": "Static embeddings\n\n\n\nTask\nOptimized Pipeline\n\n\n\n\nFlair Text Classification\nOptimizedFlairClassificationPipeline\n\n\nFlair Pair Text Classification\nOptimizedFlairPairClassificationPipeline\n\n\nFlair Sequence Labeling\nOptimizedFlairSequenceLabelingPipeline"
  },
  {
    "objectID": "index.html#example-with-text-classification",
    "href": "index.html#example-with-text-classification",
    "title": "CLARIN Embeddings",
    "section": "Example with Text Classification",
    "text": "Example with Text Classification\nOptimized pipelines can be run via following snippet of code:\n\nfrom embeddings.config.lighting_config_space import LightingTextClassificationConfigSpace\nfrom embeddings.pipeline.lightning_hps_pipeline import OptimizedLightingClassificationPipeline\n\npipeline = OptimizedLightingClassificationPipeline(\n    config_space=LightingTextClassificationConfigSpace(\n        embedding_name_or_path=\"allegro/herbert-base-cased\"\n    ),\n    dataset_name_or_path=\"clarin-pl/polemo2-official\",\n    input_column_name=\"text\",\n    target_column_name=\"target\",\n).persisting(best_params_path=\"best_prams.yaml\", log_path=\"hps_log.pickle\")\ndf, metadata = pipeline.run()\n\nTraining model with obtained parameters\nAfter the parameters search process we can train model with best parameters found. But firstly we have to set output_path parameter, which is not automatically generated from OptimizedLightingClassificationPipeline.\nmetadata[\"output_path\"] = \".\"\nNow we are able to train the pipeline\nfrom embeddings.pipeline.lightning_classification import LightningClassificationPipeline\n\npipeline = LightningClassificationPipeline(**metadata)\nresults = pipeline.run()\n\n\nSelection of best embedding model.\nInstead of performing search with single embedding model we can search with multiple embedding models via passing them as list to ConfigSpace.\npipeline = OptimizedLightingClassificationPipeline(\n    config_space=LightingTextClassificationConfigSpace(\n        embedding_name_or_path=[\"allegro/herbert-base-cased\", \"clarin-pl/roberta-polish-kgr10\"]\n    ),\n    dataset_name_or_path=\"clarin-pl/polemo2-official\",\n    input_column_name=\"text\",\n    target_column_name=\"target\",\n).persisting(best_params_path=\"best_prams.yaml\", log_path=\"hps_log.pickle\")"
  },
  {
    "objectID": "lepiszcze/submission.html",
    "href": "lepiszcze/submission.html",
    "title": "Submission",
    "section": "",
    "text": "from pathlib import Path\n\nfrom embeddings.config.lightning_config import LightningBasicConfig, LightningAdvancedConfig\nfrom embeddings.pipeline.lightning_classification import LightningClassificationPipeline\n\n/opt/conda/envs/embeddings/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nimport datasets\nimport numpy as np\n\nfrom embeddings.evaluator.evaluation_results import Predictions\nfrom embeddings.evaluator.leaderboard import get_dataset_task\nfrom embeddings.evaluator.submission import AveragedSubmission\nfrom embeddings.utils.utils import get_installed_packages\n\n\nDATASET_NAME = \"clarin-pl/polemo2-official\"\nTARGET_COLUMN_NAME = \"target\"\n\nhparams = {\"hparam_name_1\": 0.2, \"hparam_name_2\": 0.1}  # put your hyperparameters here!\n\ndataset = datasets.load_dataset(DATASET_NAME)\ny_true = np.array(dataset[\"test\"][TARGET_COLUMN_NAME])\n# put your predictions from multiple runs below!\npredictions = [\n    Predictions(\n        y_true=y_true, y_pred=np.random.randint(low=0, high=4, size=len(y_true))\n    )\n    for _ in range(5)\n]\n\n# make sure you are running on a training env or put exported packages below!\npackages = get_installed_packages() \nsubmission = AveragedSubmission.from_predictions(\n    submission_name=\"your_submission_name\",  # put your submission here!\n    dataset_name=DATASET_NAME,\n    dataset_version=dataset[\"train\"].info.version.version_str,\n    embedding_name=\"your_embedding_model\",  # put your embedding name here!\n    predictions=predictions,\n    hparams=hparams,\n    packages=packages,\n    task=get_dataset_task(DATASET_NAME),\n)\n\nsubmission.save_json()\n\nNo config specified, defaulting to: polemo2-official/all_text\nFound cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n100%|██████████| 3/3 [00:00<00:00, 47.31it/s]\n/app/embeddings/metric/hugging_face_metric.py:27: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  datasets.load_metric(metric, **init_kwargs) if isinstance(metric, str) else metric"
  },
  {
    "objectID": "lepiszcze/lepiszcze.html",
    "href": "lepiszcze/lepiszcze.html",
    "title": "LEPISZCZE",
    "section": "",
    "text": "We recommend to read our NeurIPS paper (Augustyniak et al. 2022) where you can find our lessons learned from the process of designing and compiling LEPISZCZE benchmark.\n\n\nfrom pathlib import Path\n\nfrom embeddings.config.lightning_config import LightningBasicConfig, LightningAdvancedConfig\nfrom embeddings.pipeline.lightning_classification import LightningClassificationPipeline\n\n/opt/conda/envs/embeddings/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nWe will start with training a text classifier using embeddings.pipeline.lightning_classification.LightningClassificationPipeline\n\ndoc(LightningClassificationPipeline)\n\n\nLightningClassificationPipeline\nLightningClassificationPipeline(embedding_name_or_path:Union[str,pathlib.Path], dataset_name_or_path:Union[str,pathlib.Path], input_column_name:Union[str,Sequence[str]], target_column_name:str, output_path:Union[str,pathlib.Path], evaluation_filename:str='evaluation.json', config:Union[embeddings.config.lightning_config.LightningBasicConfig,embeddings.config.lightning_config.LightningAdvancedConfig]=LightningBasicConfig(use_scheduler=True, optimizer='Adam', warmup_steps=100, learning_rate=0.0001, adam_epsilon=1e-08, weight_decay=0.0, finetune_last_n_layers=-1, classifier_dropout=None, max_seq_length=None, batch_size=32, max_epochs=None, early_stopping_monitor='val/Loss', early_stopping_mode='min', early_stopping_patience=3, tokenizer_kwargs={}, batch_encoding_kwargs={}, dataloader_kwargs={}), devices:Union[List[int],str,int,NoneType]='auto', accelerator:Union[str,pytorch_lightning.accelerators.accelerator.Accelerator,NoneType]='auto', logging_config:embeddings.utils.loggers.LightningLoggingConfig=LightningLoggingConfig(loggers_names=[], tracking_project_name=None, wandb_entity=None, wandb_logger_kwargs={}), tokenizer_name_or_path:Union[str,pathlib.Path,NoneType]=None, predict_subset:embeddings.data.dataset.LightingDataModuleSubset=, load_dataset_kwargs:Optional[Dict[str,Any]]=None, model_checkpoint_kwargs:Optional[Dict[str,Any]]=None)Helper class that provides a standard way to create an ABC using\ninheritance.\n\n\n\nLEPISZCZE_SUBMISSIONS = Path(\"../lepiszcze-submissions\")\nLEPISZCZE_SUBMISSIONS.mkdir(exist_ok=True, parents=True)\n\n\nconfig = LightningBasicConfig(\n    learning_rate=0.01, max_epochs=1, max_seq_length=128, finetune_last_n_layers=0\n)\n\n\npipeline = LightningClassificationPipeline(\n    dataset_name_or_path=\"clarin-pl/polemo2-official\",\n    embedding_name_or_path=\"distilbert-base-uncased\",\n    input_column_name=\"text\",\n    target_column_name=\"target\",\n    output_path=\".\",\n    config=config\n)\n\nNo config specified, defaulting to: polemo2-official/all_text\nFound cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n100%|██████████| 3/3 [00:00<00:00, 76.32it/s]\n100%|██████████| 1/1 [00:02<00:00,  2.29s/ba]\n100%|██████████| 1/1 [00:00<00:00,  9.10ba/s]\n100%|██████████| 1/1 [00:00<00:00, 10.65ba/s]\nCasting the dataset: 100%|██████████| 7/7 [00:00<00:00,  9.94ba/s]\nCasting the dataset: 100%|██████████| 1/1 [00:00<00:00, 10.82ba/s]\nCasting the dataset: 100%|██████████| 1/1 [00:00<00:00, 11.31ba/s]\n\n\nIt took a couple of seconds but finally we have a pipeline objects ready and we need only run it.\n\nfrom embeddings.config.lightning_config import LightningAdvancedConfig\n\n\nresults = pipeline.run()\nprint(results)\n\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo config specified, defaulting to: polemo2-official/all_text\nFound cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n100%|██████████| 3/3 [00:00<00:00, 82.76it/s]\n/opt/conda/envs/embeddings/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:849: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(strategy=\"dp\"|\"ddp\"|\"ddp2\")`. Setting `strategy=\"ddp_spawn\"` for you.\n  rank_zero_warn(\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\n\n\nTypeError: __new__() missing 1 required positional argument: 'task'\n\n\n\nfrom embeddings.pipeline.flair_classification import FlairClassificationPipeline\nfrom embeddings.utils.utils import build_output_path\n\n\nroot = \".\"\nembedding_name_or_path = \"clarin-pl/word2vec-kgr10\"\ndataset_name = \"clarin-pl/polemo2-official\"\ninput_column_name = \"text\"\ntarget_column_name = \"target\"\n\n\noutput_path = build_output_path(root, embedding_name_or_path, dataset_name)\npipeline = FlairClassificationPipeline(\n    embedding_name=embedding_name_or_path,\n    dataset_name=dataset_name,\n    input_column_name=input_column_name,\n    target_column_name=target_column_name,\n    output_path=output_path,\n)\n\n2022-12-16 00:16:43,449 - embeddings.utils.utils - WARNING - String 'clarin-pl/word2vec-kgr10' contains '/'. Replacing it with '__'. Cleaned_text: clarin-pl__word2vec-kgr10.\n2022-12-16 00:16:43,450 - embeddings.utils.utils - WARNING - String 'clarin-pl/polemo2-official' contains '/'. Replacing it with '__'. Cleaned_text: clarin-pl__polemo2-official.\n2022-12-16 00:16:44,569 - embeddings.embedding.auto_flair - INFO - clarin-pl/word2vec-kgr10 not compatible with Transformers, trying to initialise as static embedding.\n/opt/conda/envs/embeddings/lib/python3.9/site-packages/huggingface_hub/file_download.py:594: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n  warnings.warn(\n\n\n\nresult = pipeline.run()\n\nNo config specified, defaulting to: polemo2-official/all_text\nFound cached dataset polemo2-official (/root/.cache/huggingface/datasets/clarin-pl___polemo2-official/all_text/0.0.0/2b75fdbe5def97538e81fb120f8752744b50729a4ce09bd75132bfc863a2fd70)\n100%|██████████| 3/3 [00:00<00:00, 86.55it/s]\n2022-12-16 00:18:30,059 - embeddings.transformation.flair_transformation.corpus_transformation - INFO - Info of ['train', 'validation', 'test']:\n{'builder_name': 'polemo2-official',\n 'citation': '\\n'\n             '@inproceedings{kocon-etal-2019-multi,\\n'\n             '    title = \"Multi-Level Sentiment Analysis of {P}ol{E}mo 2.0: '\n             'Extended Corpus of Multi-Domain Consumer Reviews\",\\n'\n             '    author = \"Koco{\\'n}, Jan  and\\n'\n             '      Mi{\\\\l}kowski, Piotr  and\\n'\n             '      Za{\\'s}ko-Zieli{\\'n}ska, Monika\",\\n'\n             '    booktitle = \"Proceedings of the 23rd Conference on '\n             'Computational Natural Language Learning (CoNLL)\",\\n'\n             '    month = nov,\\n'\n             '    year = \"2019\",\\n'\n             '    address = \"Hong Kong, China\",\\n'\n             '    publisher = \"Association for Computational Linguistics\",\\n'\n             '    url = \"https://www.aclweb.org/anthology/K19-1092\",\\n'\n             '    doi = \"10.18653/v1/K19-1092\",\\n'\n             '    pages = \"980--991\",}\\n',\n 'config_name': 'all_text',\n 'dataset_size': 6606544,\n 'description': 'PolEmo 2.0:  Corpus of Multi-Domain Consumer Reviews, '\n                'evaluation data for article presented at CoNLL.',\n 'download_checksums': {'https://huggingface.co/datasets/clarin-pl/polemo2-official/resolve/main/data/all.text.dev.txt': {'checksum': '1d92db1b0b76f7c4787cbc1492f68be55c762deae0d6c6a0e5359748de7f4c47',\n                                                                                                                          'num_bytes': 663454},\n                        'https://huggingface.co/datasets/clarin-pl/polemo2-official/resolve/main/data/all.text.test.txt': {'checksum': '153ca33789f9b5bc5c8a7661bb13861a5c714b915b4f9969ae9901bee116d4c7',\n                                                                                                                           'num_bytes': 648758},\n                        'https://huggingface.co/datasets/clarin-pl/polemo2-official/resolve/main/data/all.text.train.txt': {'checksum': '03b9908ba4662b81879c719ae7d19e72ff8c705eb7dc127450bc232810068bae',\n                                                                                                                            'num_bytes': 5372370}},\n 'download_size': 6684582,\n 'features': {'target': ClassLabel(names=['zero', 'minus', 'plus', 'amb'], id=None),\n              'text': Value(dtype='string', id=None)},\n 'homepage': 'https://clarin-pl.eu/dspace/handle/11321/710',\n 'license': 'CC-BY-4.0',\n 'post_processed': None,\n 'post_processing_size': None,\n 'size_in_bytes': 13291126,\n 'splits': {'test': SplitInfo(name='test', num_bytes=640863, num_examples=820, shard_lengths=None, dataset_name='polemo2-official'),\n            'train': SplitInfo(name='train', num_bytes=5310040, num_examples=6573, shard_lengths=None, dataset_name='polemo2-official'),\n            'validation': SplitInfo(name='validation', num_bytes=655641, num_examples=823, shard_lengths=None, dataset_name='polemo2-official')},\n 'supervised_keys': None,\n 'task_templates': None,\n 'version': 0.0.0}\n2022-12-16 00:18:30,059 - embeddings.transformation.flair_transformation.corpus_transformation - INFO - Schemas:    DatasetDict({\n    train: Dataset({\n        features: ['text', 'target'],\n        num_rows: 6573\n    })\n    validation: Dataset({\n        features: ['text', 'target'],\n        num_rows: 823\n    })\n    test: Dataset({\n        features: ['text', 'target'],\n        num_rows: 820\n    })\n})\n100%|██████████| 6573/6573 [00:00<00:00, 15093.71ex/s]\nCreating CSV from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 25.72ba/s]\n100%|██████████| 823/823 [00:00<00:00, 17241.72ex/s]\nCreating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 30.31ba/s]\n100%|██████████| 820/820 [00:00<00:00, 18486.84ex/s]\nCreating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 32.88ba/s]\n\n\n2022-12-16 00:18:42,849 Computing label dictionary. Progress:\n\n\n6573it [00:00, 56695.52it/s]\n\n\n2022-12-16 00:18:42,971 Dictionary created for label 'None' with 5 values: 1 (seen 2469 times), 2 (seen 1824 times), 3 (seen 1309 times), 0 (seen 971 times)\n\n\n\n\n\n2022-12-16 00:18:44,592 ----------------------------------------------------------------------------------------------------\n2022-12-16 00:18:44,594 Model: \"TextClassifier(\n  (decoder): Linear(in_features=300, out_features=5, bias=True)\n  (dropout): Dropout(p=0.0, inplace=False)\n  (locked_dropout): LockedDropout(p=0.0)\n  (word_dropout): WordDropout(p=0.0)\n  (loss_function): CrossEntropyLoss()\n  (document_embeddings): DocumentPoolEmbeddings(\n    fine_tune_mode=none, pooling=mean\n    (embeddings): StackedEmbeddings(\n      (list_embedding_0): WordEmbeddings(\n        '/root/.cache/huggingface/hub/894a9a0a7a7c9e5defa71b9ed26e5699b9394e25d3ebce51d39188935f15ac57.3b0d8f1d834bcf9f436953bd7051d5d9761aae9f482a26ed62e8c21283da012b'\n        (embedding): Embedding(2283378, 300)\n      )\n    )\n  )\n  (weights): None\n  (weight_tensor) None\n)\"\n2022-12-16 00:18:44,596 ----------------------------------------------------------------------------------------------------\n2022-12-16 00:18:44,597 Corpus: \"Corpus: 6573 train + 823 dev + 820 test sentences\"\n2022-12-16 00:18:44,598 ----------------------------------------------------------------------------------------------------\n2022-12-16 00:18:44,599 Parameters:\n2022-12-16 00:18:44,600  - learning_rate: \"0.001000\"\n2022-12-16 00:18:44,601  - mini_batch_size: \"32\"\n2022-12-16 00:18:44,602  - patience: \"3\"\n2022-12-16 00:18:44,603  - anneal_factor: \"0.5\"\n2022-12-16 00:18:44,604  - max_epochs: \"20\"\n2022-12-16 00:18:44,605  - shuffle: \"True\"\n2022-12-16 00:18:44,606  - train_with_dev: \"False\"\n2022-12-16 00:18:44,607  - batch_growth_annealing: \"False\"\n2022-12-16 00:18:44,608 ----------------------------------------------------------------------------------------------------\n2022-12-16 00:18:44,609 Model training base path: \"clarin-pl__word2vec-kgr10/clarin-pl__polemo2-official/20221216_001643\"\n2022-12-16 00:18:44,610 ----------------------------------------------------------------------------------------------------\n2022-12-16 00:18:44,611 Device: cuda:0\n2022-12-16 00:18:44,612 ----------------------------------------------------------------------------------------------------\n2022-12-16 00:18:44,613 Embeddings storage mode: cpu\n2022-12-16 00:18:44,614 ----------------------------------------------------------------------------------------------------\n2022-12-16 00:18:52,165 epoch 1 - iter 20/206 - loss 0.05042096 - samples/sec: 84.78 - lr: 0.001000\n2022-12-16 00:18:58,129 epoch 1 - iter 40/206 - loss 0.05039598 - samples/sec: 107.37 - lr: 0.001000\n2022-12-16 00:19:05,184 epoch 1 - iter 60/206 - loss 0.05036845 - samples/sec: 90.75 - lr: 0.001000\n2022-12-16 00:19:11,868 epoch 1 - iter 80/206 - loss 0.05034159 - samples/sec: 95.80 - lr: 0.001000\n2022-12-16 00:19:17,588 epoch 1 - iter 100/206 - loss 0.05031501 - samples/sec: 111.94 - lr: 0.001000\n2022-12-16 00:19:23,393 epoch 1 - iter 120/206 - loss 0.05028955 - samples/sec: 110.32 - lr: 0.001000\n\n\n: \n\n\n: \n\n\n\n\n\n\nReferences\n\nAugustyniak, Lukasz, Kamil Tagowski, Albert Sawczyn, Denis Janiak, Roman Bartusiak, Adrian Dominik Szymczak, Arkadiusz Janz, et al. 2022. “This Is the Way: Designing and Compiling LEPISZCZE, a Comprehensive NLP Benchmark for Polish.” In Thirty-Sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. https://openreview.net/forum?id=CZAd_6uiUx0."
  }
]