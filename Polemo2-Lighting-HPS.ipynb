{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import dataclasses\n",
    "import functools\n",
    "import inspect\n",
    "from abc import ABC\n",
    "from dataclasses import InitVar, dataclass, field\n",
    "from typing import Any, Dict, Final, List, Optional, Set, Tuple, Type, TypeVar, Union\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from embeddings.data.huggingface_datamodule import (\n",
    "    HuggingFaceDataset,\n",
    "    TextClassificationDataModule,\n",
    ")\n",
    "from embeddings.embedding.auto_flair import AutoFlairWordEmbedding\n",
    "from embeddings.embedding.flair_embedding import FlairTransformerEmbedding\n",
    "from embeddings.embedding.static.embedding import StaticEmbedding\n",
    "from embeddings.evaluator.text_classification_evaluator import (\n",
    "    TextClassificationEvaluator,\n",
    ")\n",
    "from embeddings.hyperparameter_search.configspace import (\n",
    "    BaseConfigSpace,\n",
    "    Parameter,\n",
    "    ParsedParameters,\n",
    "    SampledParameters,\n",
    ")\n",
    "from embeddings.hyperparameter_search.parameters import (\n",
    "    ConstantParameter,\n",
    "    SearchableParameter,\n",
    ")\n",
    "from embeddings.model.lightning.auto_lightning import AutoTransformer\n",
    "from embeddings.utils.utils import PrimitiveTypes\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "from torch.optim import AdamW, Optimizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import F1, Accuracy, MetricCollection, Precision, Recall\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(inspect.getsource(TextClassificationDataModule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(inspect.getsource(AutoTransformer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define necessary model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceClassificationModule(pl.LightningModule, abc.ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_labels: int,\n",
    "        metrics: Optional[MetricCollection] = None,\n",
    "        learning_rate: float = 1e-4,\n",
    "        adam_epsilon: float = 1e-8,\n",
    "        warmup_steps: int = 0,\n",
    "        weight_decay: float = 0.0,\n",
    "        train_batch_size: int = 32,\n",
    "        eval_batch_size: int = 32,\n",
    "        use_scheduler: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        if metrics is None:\n",
    "            metrics = self.get_default_metrics(num_labels=num_labels)\n",
    "        self.train_metrics = metrics.clone(prefix=\"train/\")\n",
    "        self.val_metrics = metrics.clone(prefix=\"val/\")\n",
    "        self.test_metrics = metrics.clone(prefix=\"test/\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_metrics(num_labels: int) -> MetricCollection:\n",
    "        if num_labels > 2:\n",
    "            metrics = MetricCollection(\n",
    "                [\n",
    "                    Accuracy(num_classes=num_labels),\n",
    "                    Precision(num_classes=num_labels, average=\"macro\"),\n",
    "                    Recall(num_classes=num_labels, average=\"macro\"),\n",
    "                    F1(num_classes=num_labels, average=\"macro\"),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            metrics = MetricCollection(\n",
    "                [\n",
    "                    Accuracy(num_classes=num_labels),\n",
    "                    Precision(num_classes=num_labels),\n",
    "                    Recall(num_classes=num_labels),\n",
    "                    F1(num_classes=num_labels),\n",
    "                ]\n",
    "            )\n",
    "        return metrics\n",
    "\n",
    "    def shared_step(self, **batch: Any) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        outputs = self.forward(**batch)\n",
    "        loss, logits = outputs[:2]\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        # loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        # loss = loss_fn(logits.view(-1, self.hparams.num_labels), batch[\"labels\"].view(-1))\n",
    "        return loss, logits\n",
    "\n",
    "    def training_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\n",
    "        batch, batch_idx = args\n",
    "        loss, preds = self.shared_step(**batch)\n",
    "        self.train_metrics(preds, batch[\"labels\"])\n",
    "        lr = (\n",
    "            self.lr_scheduler.get_last_lr()[-1]\n",
    "            if self.hparams.use_scheduler\n",
    "            else self.hparams.learning_rate\n",
    "        )\n",
    "        self.log(\"train/Loss\", loss, on_step=True, on_epoch=True)\n",
    "        self.log(\"train/LR\", lr, on_step=True, on_epoch=True)\n",
    "        return {\"loss\": loss, \"lr\": lr}\n",
    "\n",
    "    def validation_step(self, *args: Any, **kwargs: Any) -> Optional[STEP_OUTPUT]:\n",
    "        batch, batch_idx = args\n",
    "        loss, preds = self.shared_step(**batch)\n",
    "        self.val_metrics(preds, batch[\"labels\"])\n",
    "        self.log(\"val/Loss\", loss, on_epoch=True)\n",
    "        return None\n",
    "\n",
    "    def test_step(self, *args: Any, **kwargs: Any) -> Optional[STEP_OUTPUT]:\n",
    "        batch, batch_idx = args\n",
    "        loss, preds = self.shared_step(**batch)\n",
    "        if -1 not in batch[\"labels\"]:\n",
    "            self.test_metrics(preds, batch[\"labels\"])\n",
    "            self.log(\"test/Loss\", loss, on_epoch=True)\n",
    "        return None\n",
    "\n",
    "    def training_epoch_end(self, outputs: List[Any]) -> None:\n",
    "        self._aggregate_and_log_metrics(self.train_metrics)\n",
    "\n",
    "    def validation_epoch_end(self, outputs: List[Any]) -> None:\n",
    "        metric_values = self._aggregate_and_log_metrics(self.val_metrics, prog_bar=True)\n",
    "\n",
    "    #         self.log('val_f1', metric_values['val/F1'], prog_bar=True, logger=False)\n",
    "\n",
    "    def test_epoch_end(self, outputs: List[Any]) -> None:\n",
    "        self._aggregate_and_log_metrics(self.test_metrics)\n",
    "\n",
    "    def _aggregate_and_log_metrics(\n",
    "        self, metrics: MetricCollection, prog_bar: bool = False\n",
    "    ) -> Dict[str, float]:\n",
    "        metric_values = metrics.compute()\n",
    "        metrics.reset()\n",
    "        self.log_dict(metric_values, prog_bar=prog_bar)\n",
    "        return metric_values\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        if stage == \"fit\" and self.hparams.use_scheduler:\n",
    "            # Get dataloader by calling it - train_dataloader() is called after setup() by default\n",
    "            train_loader = self.trainer.datamodule.train_dataloader()\n",
    "\n",
    "            # Calculate total steps\n",
    "            tb_size = self.hparams.train_batch_size * max(1, self.trainer.gpus)\n",
    "            ab_size = self.trainer.accumulate_grad_batches * float(\n",
    "                self.trainer.max_epochs\n",
    "            )\n",
    "            self.total_steps = (len(train_loader.dataset) // tb_size) // ab_size\n",
    "\n",
    "    def configure_optimizers(self) -> Tuple[List[Optimizer], List[Any]]:\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=self.hparams.learning_rate,\n",
    "            eps=self.hparams.adam_epsilon,\n",
    "        )\n",
    "\n",
    "        if self.hparams.use_scheduler:\n",
    "\n",
    "            def lr_lambda(step: int) -> float:\n",
    "                if self.hparams.warmup_steps > 0 and step < self.hparams.warmup_steps:\n",
    "                    return step / self.hparams.warmup_steps\n",
    "                else:\n",
    "                    return (self.total_steps - step) / (\n",
    "                        self.total_steps - self.hparams.warmup_steps\n",
    "                    )\n",
    "\n",
    "            self.lr_scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "            lr_scheduler = [self.lr_scheduler]\n",
    "        else:\n",
    "            lr_scheduler = []\n",
    "\n",
    "        return [optimizer], lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoTransformerForSequenceClassification(\n",
    "    AutoTransformer, SequenceClassificationModule\n",
    "):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        num_labels: int,\n",
    "        unfreeze_from: Optional[int] = None,\n",
    "        **kwargs: Any\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model_name_or_path,\n",
    "            AutoModelForSequenceClassification,\n",
    "            num_labels,\n",
    "            unfreeze_from,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def predict(\n",
    "        self, dataloader: DataLoader[HuggingFaceDataset]\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        predictions = []\n",
    "        for batch in dataloader:\n",
    "            outputs = self.forward(**batch)\n",
    "            predictions.append(outputs.logits)\n",
    "        predictions = torch.argmax(torch.cat(predictions), dim=1).numpy()\n",
    "        ground_truth = torch.cat([x[\"labels\"] for x in dataloader]).numpy()\n",
    "        return {\"y_pred\": predictions, \"y_true\": ground_truth}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(inspect.getsource(AutoTransformer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test run (loss should decrease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_name = \"allegro/herbert-base-cased\"\n",
    "dataset_name = \"clarin-pl/polemo2-official\"\n",
    "input_column_name = [\"text\"]\n",
    "target_column_name = \"target\"\n",
    "output_path = None\n",
    "load_dataset_kwargs = {\n",
    "    \"train_domains\": [\"hotels\", \"medicine\"],\n",
    "    \"dev_domains\": [\"hotels\", \"medicine\"],\n",
    "    \"test_domains\": [\"hotels\", \"medicine\"],\n",
    "    \"text_cfg\": \"text\",\n",
    "}\n",
    "task_train_kwargs = {\"max_epochs\": 5, \"gpus\": 1}\n",
    "task_model_kwargs = {\"learning_rate\": 1e-3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = TextClassificationDataModule(\n",
    "    model_name_or_path=embedding_name,\n",
    "    dataset_name=dataset_name,\n",
    "    text_fields=input_column_name,\n",
    "    target_field=target_column_name,\n",
    "    load_dataset_kwargs=load_dataset_kwargs,\n",
    ")\n",
    "dm.initalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoTransformerForSequenceClassification(\n",
    "    model_name_or_path=embedding_name,\n",
    "    input_dim=dm.input_dim,\n",
    "    num_labels=dm.num_labels,\n",
    "    **task_model_kwargs,\n",
    ")\n",
    "trainer = pl.Trainer(default_root_dir=output_path, **task_train_kwargs)\n",
    "evaluator = TextClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup(\"fit\")\n",
    "trainer.fit(model, dm)\n",
    "trainer.test(datamodule=dm)\n",
    "model_result = model.predict(dataloader=dm.test_dataloader())\n",
    "metrics = evaluator.evaluate(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define config space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConfigSpace(BaseConfigSpace):\n",
    "    max_epochs: Parameter = SearchableParameter(\n",
    "        name=\"max_epochs\",\n",
    "        type=\"categorical\",\n",
    "        choices=[20, 25, 30],\n",
    "    )\n",
    "    mini_batch_size: Parameter = SearchableParameter(\n",
    "        name=\"batch_size\",\n",
    "        type=\"categorical\",\n",
    "        choices=[8, 16, 32, 48, 64],\n",
    "    )\n",
    "    max_seq_length: Parameter = SearchableParameter(\n",
    "        name=\"max_seq_length\",\n",
    "        type=\"categorical\",\n",
    "        choices=[128, 256],\n",
    "    )\n",
    "    optimizer: Parameter = SearchableParameter(\n",
    "        name=\"optimizer\",\n",
    "        type=\"categorical\",\n",
    "        choices=[\"Adam\", \"AdamW\"],\n",
    "    )\n",
    "    use_scheduler: Parameter = SearchableParameter(\n",
    "        name=\"use_scheduler\",\n",
    "        type=\"categorical\",\n",
    "        choices=[False, True],\n",
    "    )\n",
    "    warmup_steps: Parameter = SearchableParameter(\n",
    "        name=\"warmup_steps\", type=\"int_uniform\", low=0, high=200, step=10\n",
    "    )\n",
    "    learning_rate: Parameter = SearchableParameter(\n",
    "        name=\"learning_rate\", type=\"uniform\", low=1e-5, high=1e-3\n",
    "    )\n",
    "    adam_epsilon: Parameter = SearchableParameter(\n",
    "        name=\"adam_epsilon\", type=\"uniform\", low=1e-9, high=1e-6\n",
    "    )\n",
    "    weight_decay: Parameter = SearchableParameter(\n",
    "        name=\"weight_decay\", type=\"uniform\", low=0.0, high=1e-2\n",
    "    )\n",
    "    unfreeze_from: Parameter = SearchableParameter(\n",
    "        name=\"unfreeze_from\",\n",
    "        type=\"categorical\",\n",
    "        choices=[-1, 4, 7, 9, 11, None],\n",
    "    )\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_parameters(parameters: Dict[str, PrimitiveTypes]) -> SampledParameters:\n",
    "        dl_model_keys: Final = {\"batch_size\", \"max_seq_length\"}\n",
    "        task_model_keys: Final = {\n",
    "            \"learning_rate\",\n",
    "            \"unfreeze_from\",\n",
    "            \"optimizer\",\n",
    "            \"use_scheduler\",\n",
    "            \"warmup_steps\",\n",
    "            \"adam_epsilon\",\n",
    "            \"weight_decay\",\n",
    "        }\n",
    "        task_trainer_keys: Final = {\n",
    "            \"max_epochs\",\n",
    "        }\n",
    "        dl_model_kwargs = BaseConfigSpace._pop_parameters(\n",
    "            parameters=parameters, parameters_keys=dl_model_keys\n",
    "        )\n",
    "        task_model_kwargs = BaseConfigSpace._pop_parameters(\n",
    "            parameters=parameters, parameters_keys=task_model_keys\n",
    "        )\n",
    "        task_trainer_kwargs = BaseConfigSpace._pop_parameters(\n",
    "            parameters=parameters, parameters_keys=task_trainer_keys\n",
    "        )\n",
    "        task_trainer_kwargs = {}\n",
    "        task_model_kwargs[\"train_batch_size\"] = dl_model_kwargs[\"batch_size\"]\n",
    "        task_model_kwargs[\"eval_batch_size\"] = dl_model_kwargs[\"batch_size\"]\n",
    "\n",
    "        return dl_model_kwargs, task_model_kwargs, task_trainer_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_paramaters(study):\n",
    "    best_params = study.best_params\n",
    "    constant_params = study.best_trial.user_attrs\n",
    "    parsed_params = best_params | constant_params\n",
    "    return parsed_params\n",
    "\n",
    "\n",
    "def objective(trial, cs, dataset) -> float:\n",
    "    parameters = cs.sample_parameters(trial=trial)\n",
    "    dl_model_kwargs, task_model_kwargs, task_trainer_kwargs = cs.parse_parameters(\n",
    "        parameters\n",
    "    )\n",
    "    print(\"params\", dl_model_kwargs, task_model_kwargs, task_trainer_kwargs)\n",
    "\n",
    "    dm = TextClassificationDataModule(\n",
    "        model_name_or_path=\"allegro/herbert-base-cased\",\n",
    "        dataset_name=\"clarin-pl/polemo2-official\",\n",
    "        text_fields=[\"text\"],\n",
    "        target_field=\"target\",\n",
    "        load_dataset_kwargs={\n",
    "            \"train_domains\": [\"hotels\", \"medicine\"],\n",
    "            \"dev_domains\": [\"hotels\", \"medicine\"],\n",
    "            \"test_domains\": [\"hotels\", \"medicine\"],\n",
    "            \"text_cfg\": \"text\",\n",
    "        },\n",
    "        ignore_test_split=True,\n",
    "        **dl_model_kwargs,\n",
    "    )\n",
    "    dm.initalize()\n",
    "\n",
    "    model = AutoTransformerForSequenceClassification(\n",
    "        model_name_or_path=\"allegro/herbert-base-cased\",\n",
    "        input_dim=dm.input_dim,\n",
    "        num_labels=dm.num_labels,\n",
    "        **task_model_kwargs,\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=1,\n",
    "        callbacks=[EarlyStopping(monitor=\"val/F1\", verbose=True, patience=5, min_delta=0.01, mode=\"max\")],\n",
    "        max_epochs=20,\n",
    "        **task_trainer_kwargs,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        trainer.fit(model, dm)\n",
    "    except Exception as e:\n",
    "        del model\n",
    "        del dm\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        raise e\n",
    "    evaluator = TextClassificationEvaluator()\n",
    "\n",
    "    model_result = model.predict(dataloader=dm.val_dataloader())\n",
    "    metrics = self.evaluator.evaluate(model_result)\n",
    "    del dm\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    return metrics[\"f1__average_macro\"][\"f1\"]\n",
    "\n",
    "\n",
    "def run(n_trials):\n",
    "    dataset = dataset = datasets.load_dataset(\n",
    "        \"clarin-pl/polemo2-official\",\n",
    "        **{\n",
    "            \"train_domains\": [\"hotels\", \"medicine\"],\n",
    "            \"dev_domains\": [\"hotels\", \"medicine\"],\n",
    "            \"test_domains\": [\"hotels\", \"medicine\"],\n",
    "            \"text_cfg\": \"text\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    cs = ConfigSpace()\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=optuna.samplers.TPESampler(),\n",
    "        pruner=optuna.pruners.MedianPruner(),\n",
    "    )\n",
    "\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, cs, dataset),\n",
    "        n_trials=n_trials,\n",
    "        show_progress_bar=True,\n",
    "        catch=(Exception,),\n",
    "    )\n",
    "\n",
    "    return study.trials_dataframe(), get_best_paramaters(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, metadata = run(n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allegro default config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42)\n",
    "\n",
    "dm = TextClassificationDataModule(\n",
    "    model_name_or_path=\"allegro/herbert-base-cased\",\n",
    "    dataset_name=\"clarin-pl/polemo2-official\",\n",
    "    text_fields=[\"text\"],\n",
    "    target_field=\"target\",\n",
    "    load_dataset_kwargs={\n",
    "        \"train_domains\": [\"hotels\", \"medicine\"],\n",
    "        \"dev_domains\": [\"hotels\", \"medicine\"],\n",
    "        \"test_domains\": [\"hotels\", \"medicine\"],\n",
    "        \"text_cfg\": \"text\",\n",
    "    },\n",
    "    ignore_test_split=False,\n",
    "    batch_size=16,\n",
    "    max_seq_length=256,\n",
    ")\n",
    "dm.initalize()\n",
    "\n",
    "model = AutoTransformerForSequenceClassification(\n",
    "    model_name_or_path=\"allegro/herbert-base-cased\",\n",
    "    input_dim=dm.input_dim,\n",
    "    num_labels=dm.num_labels,\n",
    "    **{\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"unfreeze_from\": -1,  # change to -1\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"use_scheduler\": True,\n",
    "        \"warmup_steps\": 100,\n",
    "    },\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    **{\"max_epochs\": 4, \"accumulate_grad_batches\": 2},\n",
    ")\n",
    "\n",
    "trainer.fit(model, dm)\n",
    "evaluator = TextClassificationEvaluator()\n",
    "model_result = model.predict(dataloader=dm.val_dataloader())\n",
    "metrics = self.evaluator.evaluate(model_result)\n",
    "\n",
    "pl.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## klejbenchmark-baselines data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Move notebook or folder or change imports here in order to run following cells!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from klejbenchmark_baselines.config import Config\n",
    "from klejbenchmark_baselines.dataset import Datasets\n",
    "from klejbenchmark_baselines.model import KlejTransformer\n",
    "from klejbenchmark_baselines.task import TASKS\n",
    "from klejbenchmark_baselines.trainer import TrainerWithPredictor\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    \"task_name\": \"polemo2.0-in\",\n",
    "    \"run_id\": None,    # input run_id\n",
    "    \"task_path\": None,    # input task_path\n",
    "    \"predict_path\": None,   # input predict_path\n",
    "    \"logger_path\": \"output/tb/\",\n",
    "    \"checkpoint_path\": \"output/checkpoints/\",\n",
    "    \"tokenizer_name_or_path\": \"allegro/herbert-base-cased\",\n",
    "    \"max_seq_length\": 256,\n",
    "    \"do_lower_case\": None,\n",
    "    \"model_name_or_path\": \"allegro/herbert-base-cased\",\n",
    "    \"learning_rate\": None,\n",
    "    \"adam_epsilon\": None,\n",
    "    \"warmup_steps\": None,\n",
    "    \"batch_size\": 16,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"num_train_epochs\": None,\n",
    "    \"weight_decay\": None,\n",
    "    \"max_grad_norm\": None,\n",
    "    \"seed\": None,\n",
    "    \"num_workers\": None,\n",
    "    \"num_gpu\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int, num_gpu: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if num_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config.from_argparse(args_dict)\n",
    "task = TASKS[args_dict[\"task_name\"]](config)\n",
    "datasets = Datasets(task)\n",
    "\n",
    "set_seed(config.seed, config.num_gpu)\n",
    "model = KlejTransformer(task, datasets)\n",
    "\n",
    "# train\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=config.logger_path,\n",
    "    name=config.run_id,\n",
    "    version=config.task_name,\n",
    ")\n",
    "\n",
    "trainer = TrainerWithPredictor(\n",
    "    weights_summary=None,\n",
    "    logger=logger,\n",
    "    accumulate_grad_batches=config.gradient_accumulation_steps,\n",
    "    gradient_clip_val=config.max_grad_norm,\n",
    "    max_epochs=config.num_train_epochs,\n",
    "    gpus=config.num_gpu,\n",
    "    **({\"distributed_backend\": \"ddp\"} if config.num_gpu > 1 else {}),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda-clarin-embeddings]",
   "language": "python",
   "name": "conda-env-conda-clarin-embeddings-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
