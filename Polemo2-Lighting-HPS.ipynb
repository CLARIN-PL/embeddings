{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0154dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, TypeVar, Union\n",
    "\n",
    "import datasets\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, BatchEncoding\n",
    "\n",
    "from embeddings.utils.loggers import get_logger\n",
    "\n",
    "HuggingFaceDataset = TypeVar(\"HuggingFaceDataset\")\n",
    "\n",
    "_logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "class TextClassificationDataModule(pl.LightningDataModule):\n",
    "    loader_columns = [\n",
    "        \"datasets_idx\",\n",
    "        \"input_ids\",\n",
    "        \"token_type_ids\",\n",
    "        \"attention_mask\",\n",
    "        \"start_positions\",\n",
    "        \"end_positions\",\n",
    "        \"labels\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        dataset_name: str,\n",
    "        text_fields: Union[str, List[str]],\n",
    "        target_field: str,\n",
    "        max_seq_length: int = 128,\n",
    "        batch_size: int = 128,\n",
    "        load_dataset_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        ignore_test_split: bool = True,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        if isinstance(text_fields, str):\n",
    "            text_fields = [text_fields]\n",
    "        assert 1 <= len(text_fields) <= 2\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.dataset_name = dataset_name\n",
    "        self.text_fields = text_fields\n",
    "        self.target_field = target_field\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.train_batch_size = batch_size\n",
    "        self.eval_batch_size = batch_size\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path, use_fast=True)\n",
    "        self.initialized = False\n",
    "        if load_dataset_kwargs is None:\n",
    "            self.load_dataset_kwargs = {}\n",
    "        else:\n",
    "            self.load_dataset_kwargs = load_dataset_kwargs\n",
    "        self.ignore_test_split = ignore_test_split\n",
    "\n",
    "            \n",
    "        self.dataset = None\n",
    "    @property\n",
    "    def input_dim(self) -> int:\n",
    "        # TODO: hardcoded for now because text pairs are encoded together\n",
    "        return 768\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> Optional[int]:\n",
    "        if not self.initialized:\n",
    "            _logger.warning(\"Datamodule not initialized. Returning None.\")\n",
    "            return None\n",
    "        else:\n",
    "            return self.num_labels\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        if not self.initialized:\n",
    "            self.initalize()\n",
    "        if stage == \"fit\":\n",
    "            for split in self.dataset.keys():\n",
    "                if split == \"test\" and self.ignore_test_split:\n",
    "                    continue\n",
    "                \n",
    "                self.dataset[split] = self.dataset[split].map(\n",
    "                    self.convert_to_features,\n",
    "                    batched=True,\n",
    "                    remove_columns=[self.target_field],\n",
    "                )\n",
    "                self.columns = [\n",
    "                    c for c in self.dataset[split].column_names if c in self.loader_columns\n",
    "                ]\n",
    "                self.dataset[split].set_format(type=\"torch\", columns=self.columns)\n",
    "\n",
    "    def initalize(self) -> None:\n",
    "        self.dataset = datasets.load_dataset(self.dataset_name, **self.load_dataset_kwargs)\n",
    "        self.num_labels = len(set(ex[self.target_field] for ex in self.dataset[\"train\"]))\n",
    "        self.initialized = True\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader[HuggingFaceDataset]:\n",
    "        return DataLoader(self.dataset[\"train\"], batch_size=self.train_batch_size)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader[HuggingFaceDataset]:\n",
    "        if \"validation\" in self.dataset:\n",
    "            return DataLoader(self.dataset[\"validation\"], batch_size=self.eval_batch_size)\n",
    "        # else:\n",
    "        #     raise AttributeError(\"Validation dataset not available\")\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader[HuggingFaceDataset]:\n",
    "        if \"test\" in self.dataset and self.ignore_test_split == False:\n",
    "            return DataLoader(self.dataset[\"test\"], batch_size=self.eval_batch_size)\n",
    "        else:\n",
    "            raise AttributeError(\"Test dataset not available\")\n",
    "\n",
    "    def convert_to_features(\n",
    "        self, example_batch: Dict[str, Any], indices: Optional[List[int]] = None\n",
    "    ) -> BatchEncoding:\n",
    "        # Either encode single sentence or sentence pairs\n",
    "        if len(self.text_fields) > 1:\n",
    "            texts_or_text_pairs = list(\n",
    "                zip(example_batch[self.text_fields[0]], example_batch[self.text_fields[1]])\n",
    "            )\n",
    "        else:\n",
    "            texts_or_text_pairs = example_batch[self.text_fields[0]]\n",
    "\n",
    "        # Tokenize the text/text pairs\n",
    "        features = self.tokenizer.batch_encode_plus(\n",
    "            texts_or_text_pairs,\n",
    "            max_length=self.max_seq_length,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        # Rename label to labels to make it easier to pass to model forward\n",
    "        features[\"labels\"] = example_batch[self.target_field]\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ee1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "from torch.optim import AdamW, Optimizer\n",
    "from torchmetrics import F1, Accuracy, MetricCollection, Precision, Recall\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "class TextClassificationTransformer(pl.LightningModule, abc.ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_labels: int,\n",
    "        metrics: Optional[MetricCollection] = None,\n",
    "        learning_rate: float = 1e-4,\n",
    "        adam_epsilon: float = 1e-8,\n",
    "        warmup_steps: int = 0,\n",
    "        weight_decay: float = 0.0,\n",
    "        train_batch_size: int = 64,\n",
    "        eval_batch_size: int = 32,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        if metrics is None:\n",
    "            metrics = self.get_default_metrics(num_labels=num_labels)\n",
    "        self.train_metrics = metrics.clone(prefix=\"train/\")\n",
    "        self.val_metrics = metrics.clone(prefix=\"val/\")\n",
    "        self.test_metrics = metrics.clone(prefix=\"test/\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_metrics(num_labels: int) -> MetricCollection:\n",
    "        if num_labels > 2:\n",
    "            metrics = MetricCollection(\n",
    "                [\n",
    "                    Accuracy(num_classes=num_labels),\n",
    "                    Precision(num_classes=num_labels, average=\"macro\"),\n",
    "                    Recall(num_classes=num_labels, average=\"macro\"),\n",
    "                    F1(num_classes=num_labels, average=\"macro\"),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            metrics = MetricCollection(\n",
    "                [\n",
    "                    Accuracy(num_classes=num_labels),\n",
    "                    Precision(num_classes=num_labels),\n",
    "                    Recall(num_classes=num_labels),\n",
    "                    F1(num_classes=num_labels),\n",
    "                ]\n",
    "            )\n",
    "        return metrics\n",
    "\n",
    "    def shared_step(self, **batch: Any) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        logits = self.forward(**batch)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(logits.view(-1, self.hparams.num_labels), batch[\"labels\"].view(-1))\n",
    "        return loss, logits\n",
    "\n",
    "    def training_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\n",
    "        batch, batch_idx = args\n",
    "        loss, preds = self.shared_step(**batch)\n",
    "        self.train_metrics(preds, batch[\"labels\"])\n",
    "        self.log(\"train/Loss\", loss, on_step=True, on_epoch=True)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, *args: Any, **kwargs: Any) -> Optional[STEP_OUTPUT]:\n",
    "        batch, batch_idx = args\n",
    "        loss, preds = self.shared_step(**batch)\n",
    "        self.val_metrics(preds, batch[\"labels\"])\n",
    "        self.log(\"val/Loss\", loss, on_epoch=True)\n",
    "        return None\n",
    "\n",
    "    def test_step(self, *args: Any, **kwargs: Any) -> Optional[STEP_OUTPUT]:\n",
    "        batch, batch_idx = args\n",
    "        loss, preds = self.shared_step(**batch)\n",
    "        if -1 not in batch[\"labels\"]:\n",
    "            self.test_metrics(preds, batch[\"labels\"])\n",
    "            self.log(\"test/Loss\", loss, on_epoch=True)\n",
    "        return None\n",
    "\n",
    "    def training_epoch_end(self, outputs: List[Any]) -> None:\n",
    "        self._aggregate_and_log_metrics(self.train_metrics)\n",
    "\n",
    "    def validation_epoch_end(self, outputs: List[Any]) -> None:\n",
    "        self._aggregate_and_log_metrics(self.val_metrics, prog_bar=True)\n",
    "\n",
    "    def test_epoch_end(self, outputs: List[Any]) -> None:\n",
    "        self._aggregate_and_log_metrics(self.test_metrics)\n",
    "\n",
    "    def _aggregate_and_log_metrics(\n",
    "        self, metrics: MetricCollection, prog_bar: bool = False\n",
    "    ) -> Dict[str, float]:\n",
    "        metric_values = metrics.compute()\n",
    "        metrics.reset()\n",
    "        self.log_dict(metric_values, prog_bar=prog_bar)\n",
    "        return metric_values\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        if stage == \"fit\":\n",
    "            # Get dataloader by calling it - train_dataloader() is called after setup() by default\n",
    "            train_loader = self.trainer.datamodule.train_dataloader()\n",
    "\n",
    "            # Calculate total steps\n",
    "            tb_size = self.hparams.train_batch_size * max(1, self.trainer.gpus)\n",
    "            ab_size = self.trainer.accumulate_grad_batches * float(self.trainer.max_epochs)\n",
    "            self.train_steps = (len(train_loader.dataset) // tb_size) // ab_size\n",
    "\n",
    "    def configure_optimizers(self) -> Tuple[List[Optimizer], List[Any]]:\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=self.hparams.learning_rate,\n",
    "            eps=self.hparams.adam_epsilon,\n",
    "        )\n",
    "\n",
    "        return [optimizer], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0daa41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import ChainMap\n",
    "from typing import Any, Literal\n",
    "\n",
    "from torch import nn\n",
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "from embeddings.embedding.document_embedding import DocumentPoolEmbedding\n",
    "\n",
    "\n",
    "class AutoTransformerForSequenceClassification(TextClassificationTransformer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        input_dim: int,\n",
    "        num_labels: int,\n",
    "        pool_strategy: Literal[\"cls\", \"mean\", \"max\"] = \"cls\",\n",
    "        dropout_rate: float = 0.5,\n",
    "        freeze_transformer: bool = True,\n",
    "        **kwargs: Any\n",
    "    ):\n",
    "        super().__init__(num_labels=num_labels, **kwargs)\n",
    "        self.config = AutoConfig.from_pretrained(model_name_or_path, num_labels=num_labels)\n",
    "        self.model = AutoModel.from_pretrained(model_name_or_path, config=self.config)\n",
    "        self.doc_embedder = DocumentPoolEmbedding(strategy=pool_strategy)\n",
    "        if freeze_transformer:\n",
    "            self.freeze_transformer()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(input_dim, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(self, *args: Any, **kwargs: Any) -> Any:\n",
    "        assert not (args and kwargs)\n",
    "        assert args or kwargs\n",
    "        inputs = kwargs if kwargs else args\n",
    "        if isinstance(inputs, tuple):\n",
    "            inputs = dict(ChainMap(*inputs))\n",
    "        inputs.pop(\"labels\", None)\n",
    "\n",
    "        outputs = self.model(**inputs)\n",
    "        pooled_output = self.doc_embedder(outputs.last_hidden_state)\n",
    "        # pooled_output = outputs.last_hidden_state[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        logits = self.layers(pooled_output)\n",
    "        return logits\n",
    "\n",
    "    def unfreeze_transformer(self, unfreeze_from: int = -1) -> None:\n",
    "        if unfreeze_from == -1:\n",
    "            for param in self.model.base_model.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            requires_grad = False\n",
    "            for name, param in self.model.base_model.named_parameters():\n",
    "                if not requires_grad:\n",
    "                    if name.startswith(\"encoder.layer\"):\n",
    "                        no_layer = int(name.split(\".\")[2])\n",
    "                        if no_layer >= unfreeze_from:\n",
    "                            requires_grad = True\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def freeze_transformer(self) -> None:\n",
    "        for param in self.model.base_model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d785ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f7cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import dataclasses\n",
    "import functools\n",
    "from abc import ABC\n",
    "from dataclasses import InitVar, dataclass, field\n",
    "from typing import Any, Dict, Final, List, Set, Tuple, Type, TypeVar, Union\n",
    "\n",
    "import optuna\n",
    "\n",
    "from embeddings.embedding.auto_flair import AutoFlairWordEmbedding\n",
    "from embeddings.embedding.flair_embedding import FlairTransformerEmbedding\n",
    "from embeddings.embedding.static.embedding import StaticEmbedding\n",
    "from embeddings.hyperparameter_search.parameters import (\n",
    "    ConstantParameter,\n",
    "    SearchableParameter,\n",
    ")\n",
    "from embeddings.utils.utils import PrimitiveTypes\n",
    "\n",
    "Parameter = Union[SearchableParameter, ConstantParameter]\n",
    "ParsedParameters = TypeVar(\"ParsedParameters\")\n",
    "SampledParameters = Dict[str, Union[PrimitiveTypes, Dict[str, PrimitiveTypes]]]\n",
    "from embeddings.hyperparameter_search.configspace import BaseConfigSpace\n",
    "\n",
    "@dataclass\n",
    "class ConfigSpace(BaseConfigSpace):\n",
    "    max_epochs: Parameter = SearchableParameter(\n",
    "        name=\"max_epochs\",\n",
    "        type=\"categorical\",\n",
    "        choices=[1, 2, 5, 10, 25, 30],\n",
    "    )\n",
    "    mini_batch_size: Parameter = SearchableParameter(\n",
    "        name=\"batch_size\",\n",
    "        type=\"categorical\",\n",
    "        choices=[8, 16, 32, 48, 64],\n",
    "    )\n",
    "    learning_rate: Parameter = SearchableParameter(\n",
    "        name=\"learning_rate\",\n",
    "        type=\"categorical\",\n",
    "        choices=[1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    )\n",
    "    pool_strategy: Parameter = SearchableParameter(\n",
    "        name=\"pool_strategy\",\n",
    "        type=\"categorical\",\n",
    "        choices=[\"cls\", \"mean\", \"max\"],\n",
    "    )\n",
    "    dropout: Parameter = SearchableParameter(\n",
    "        name=\"dropout\", type=\"discrete_uniform\", low=0.0, high=0.5, q=0.05\n",
    "    )\n",
    "    freeze_transformer: Parameter = SearchableParameter(\n",
    "        name=\"freeze_transformer\",\n",
    "        type=\"categorical\",\n",
    "        choices=[False, True],\n",
    "    )\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_parameters(parameters: Dict[str, PrimitiveTypes]) -> SampledParameters:\n",
    "        dl_model_keys: Final = {\n",
    "            \"batch_size\",\n",
    "        }\n",
    "        task_model_keys: Final = {\n",
    "            \"learning_rate\",\n",
    "            \"pool_strategy\",\n",
    "            \"dropout\",\n",
    "            \"freeze_transformer\",\n",
    "        }\n",
    "        task_trainer_keys: Final = {\n",
    "            \"max_epochs\",\n",
    "        }\n",
    "        dl_model_kwargs = BaseConfigSpace._pop_parameters(\n",
    "            parameters=parameters, parameters_keys=dl_model_keys\n",
    "        )\n",
    "        task_model_kwargs = BaseConfigSpace._pop_parameters(\n",
    "            parameters=parameters, parameters_keys=task_model_keys\n",
    "        )\n",
    "        task_trainer_kwargs = BaseConfigSpace._pop_parameters(\n",
    "            parameters=parameters, parameters_keys=task_trainer_keys\n",
    "        )\n",
    "        task_model_kwargs[\"train_batch_size\"] = dl_model_kwargs[\"batch_size\"]\n",
    "        task_model_kwargs[\"eval_batch_size\"] = dl_model_kwargs[\"batch_size\"]\n",
    "\n",
    "        return dl_model_kwargs, task_model_kwargs, task_trainer_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f628ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de10f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.evaluator.text_classification_evaluator import (\n",
    "    TextClassificationEvaluator,\n",
    ")\n",
    "\n",
    "\n",
    "def get_best_paramaters(study):\n",
    "    best_params = study.best_params\n",
    "    constant_params = study.best_trial.user_attrs\n",
    "    parsed_params = best_params | constant_params\n",
    "    return parsed_params\n",
    "\n",
    "\n",
    "def objective(trial, cs, dataset) -> float:\n",
    "    parameters = cs.sample_parameters(trial=trial)\n",
    "    dl_model_kwargs, task_model_kwargs, task_trainer_kwargs = cs.parse_parameters(\n",
    "        parameters\n",
    "    )\n",
    "    print(\"params\", dl_model_kwargs, task_model_kwargs, task_trainer_kwargs)\n",
    "\n",
    "    dm = TextClassificationDataModule(\n",
    "        model_name_or_path=\"allegro/herbert-base-cased\",\n",
    "        dataset_name=\"clarin-pl/polemo2-official\",\n",
    "        text_fields=[\"text\"],\n",
    "        target_field=\"target\",\n",
    "        load_dataset_kwargs={\n",
    "            \"train_domains\": [\"hotels\", \"medicine\"],\n",
    "            \"dev_domains\": [\"hotels\", \"medicine\"],\n",
    "            \"test_domains\": [\"hotels\", \"medicine\"],\n",
    "            \"text_cfg\": \"text\",\n",
    "        },\n",
    "        ignore_test_split=True,\n",
    "        **dl_model_kwargs,\n",
    "    )\n",
    "    dm.initalize()\n",
    "\n",
    "    model = AutoTransformerForSequenceClassification(\n",
    "        model_name_or_path=\"allegro/herbert-base-cased\",\n",
    "        input_dim=dm.input_dim,\n",
    "        num_labels=dm.num_labels,\n",
    "        **task_model_kwargs,\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=1,\n",
    "        callbacks=[EarlyStopping(monitor=\"val/Loss\", verbose=True, patience=5)],\n",
    "        **task_trainer_kwargs,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        trainer.fit(model, dm)\n",
    "    except Exception as e:\n",
    "        del model\n",
    "        del dm\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        raise e\n",
    "    evaluator = TextClassificationEvaluator()\n",
    "    predictions = trainer.predict(\n",
    "        dataloaders=dm.val_dataloader(), return_predictions=True\n",
    "    )\n",
    "    predictions = torch.argmax(torch.cat(predictions), dim=1).numpy()\n",
    "    ground_truth = torch.cat([x[\"labels\"] for x in list(dm.val_dataloader())]).numpy()\n",
    "    metrics = evaluator.evaluate({\"y_pred\": predictions, \"y_true\": ground_truth})\n",
    "    \n",
    "    del dm\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    return metrics[\"f1__average_macro\"][\"f1\"]\n",
    "\n",
    "\n",
    "def run(n_trials):\n",
    "    dataset = dataset = datasets.load_dataset(\n",
    "        \"clarin-pl/polemo2-official\",\n",
    "        **{\n",
    "            \"train_domains\": [\"hotels\", \"medicine\"],\n",
    "            \"dev_domains\": [\"hotels\", \"medicine\"],\n",
    "            \"test_domains\": [\"hotels\", \"medicine\"],\n",
    "            \"text_cfg\": \"text\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    cs = ConfigSpace()\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=optuna.samplers.TPESampler(),\n",
    "        pruner=optuna.pruners.MedianPruner(),\n",
    "    )\n",
    "\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, cs, dataset),\n",
    "        n_trials=n_trials,\n",
    "        show_progress_bar=True,\n",
    "        catch=(Exception,),\n",
    "    )\n",
    "\n",
    "    return study.trials_dataframe(), get_best_paramaters(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7b9482",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df, metadata = run(n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e056ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e1d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = TextClassificationDataModule(\n",
    "    model_name_or_path=\"allegro/herbert-base-cased\",\n",
    "    dataset_name=\"clarin-pl/polemo2-official\",\n",
    "    text_fields=[\"text\"],\n",
    "    target_field=\"target\",\n",
    "    load_dataset_kwargs={\n",
    "        \"train_domains\": [\"hotels\", \"medicine\"],\n",
    "        \"dev_domains\": [\"hotels\", \"medicine\"],\n",
    "        \"test_domains\": [\"hotels\", \"medicine\"],\n",
    "        \"text_cfg\": \"text\",\n",
    "    },\n",
    "    ignore_test_split=False,\n",
    "    batch_size=8,\n",
    ")\n",
    "dm.initalize()\n",
    "\n",
    "model = AutoTransformerForSequenceClassification(\n",
    "    model_name_or_path=\"allegro/herbert-base-cased\",\n",
    "    input_dim=dm.input_dim,\n",
    "    num_labels=dm.num_labels,\n",
    "    **{\n",
    "        \"learning_rate\": 1e-05,\n",
    "        \"pool_strategy\": \"cls\",\n",
    "        \"dropout\": 0.35,\n",
    "        \"freeze_transformer\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    callbacks=[EarlyStopping(monitor=\"val/Loss\", verbose=True, patience=5)],\n",
    "    **{\"max_epochs\": 10},\n",
    ")\n",
    "\n",
    "try:\n",
    "    trainer.fit(model, dm)\n",
    "except Exception as e:\n",
    "    del model\n",
    "    del dm\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    raise e\n",
    "evaluator = TextClassificationEvaluator()\n",
    "predictions = trainer.predict(dataloaders=dm.test_dataloader(), return_predictions=True)\n",
    "predictions = torch.argmax(torch.cat(predictions), dim=1).numpy()\n",
    "ground_truth = torch.cat([x[\"labels\"] for x in list(dm.test_dataloader())]).numpy()\n",
    "metrics = evaluator.evaluate({\"y_pred\": predictions, \"y_true\": ground_truth})\n",
    "\n",
    "del dm\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d808c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline = TorchClassificationPipeline(\n",
    "    embedding_name=\"allegro/herbert-base-cased\",\n",
    "    dataset_name=\"clarin-pl/polemo2-official\",\n",
    "    input_column_name=[\"text\"],\n",
    "    target_column_name=\"target\",\n",
    "    load_dataset_kwargs={\n",
    "        \"train_domains\": [\"hotels\", \"medicine\"],\n",
    "        \"dev_domains\": [\"hotels\", \"medicine\"],\n",
    "        \"test_domains\": [\"hotels\", \"medicine\"],\n",
    "        \"text_cfg\": \"text\",\n",
    "    },\n",
    "    task_train_kwargs={\"max_epochs\": 10, \"gpus\": 1},\n",
    "    task_model_kwargs={\"pool_strategy\": \"cls\", \"learning_rate\": 5e-4}\n",
    ")\n",
    "result = pipeline.run()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
