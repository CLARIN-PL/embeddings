{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ed8e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")\n",
    "\n",
    "from embeddings.config.lightning_config import (\n",
    "    LightningAdvancedConfig,\n",
    "    LightningBasicConfig,\n",
    ")\n",
    "from embeddings.defaults import RESULTS_PATH\n",
    "from embeddings.pipeline.lightning_classification import LightningClassificationPipeline\n",
    "from embeddings.utils.utils import build_output_path, format_eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d5b6e",
   "metadata": {},
   "source": [
    "# How to use our configs? \n",
    "\n",
    "Two types of config are defined in our library: `BasicConfig` and `AdvancedConfig`.\n",
    "`BasicConfig` allows for easy use of the most common parameters in the pipeline. However, the objects defined in our pipelines are constructed in a way that they can be further paramatrized with keyword arguments. These arguments can be utilized by constructing the `AdvancedConfig`.   \n",
    "In summary, the `BasicConfig` takes arguments and automatically assign them into proper keyword group, while the `AdvancedConfig` takes as the input keyword groups that should be already correctly mapped.  \n",
    "\n",
    "The keywords arguments will depend on the type of the pipelines for the Flair pipeline (that are used for static embeddings), and thus there are config defined for type of the task.\n",
    "\n",
    "The list of available config can be found below.\n",
    "\n",
    "\n",
    "### **Flair**:  \n",
    "   - FlairBasicConfig\n",
    "   - FlairSequenceLabelingBasicConfig\n",
    "   - FlairTextClassificationBasicConfig\n",
    "   - FlairSequenceLabelingAdvancedConfig\n",
    "   - FlairTextClassificationAdvancedConfig\n",
    "   \n",
    "### **Lightning**:\n",
    "   - LightningBasicConfig\n",
    "   - LightningAdvancedConfig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## What are the available advanced config keyword arguments and where to find them?\n",
    "\n",
    "In general, the keywords are passed to the object when constructing specific pipelines. Take for example the fragment of `LightningClassificationPipeline`:\n",
    "\n",
    "```\n",
    "datamodule = TextClassificationDataModule(\n",
    "    tokenizer_name_or_path=tokenizer_name_or_path,\n",
    "    dataset_name_or_path=dataset_name_or_path,\n",
    "    text_fields=input_column_name,\n",
    "    target_field=target_column_name,\n",
    "    train_batch_size=config_space.train_batch_size,\n",
    "    eval_batch_size=config_space.eval_batch_size,\n",
    "    tokenizer_kwargs=config_space.tokenizer_kwargs,\n",
    "    batch_encoding_kwargs=config_space.batch_encoding_kwargs,\n",
    "    load_dataset_kwargs=load_dataset_kwargs,\n",
    "    **config_space.datamodule_kwargs\n",
    ")\n",
    "task = TextClassificationTask(\n",
    "    model_name_or_path=embedding_name_or_path,\n",
    "    output_path=output_path,\n",
    "    finetune_last_n_layers=config_space.finetune_last_n_layers,\n",
    "    model_config_kwargs=config_space.model_config_kwargs,\n",
    "    task_model_kwargs=config_space.task_model_kwargs,\n",
    "    task_train_kwargs=config_space.task_train_kwargs,\n",
    "    early_stopping_kwargs=config_space.early_stopping_kwargs,\n",
    ")\n",
    "```\n",
    "\n",
    "We can identify and trace the keyword arguments to find the possible arguments that can be set in the config kwargs.\n",
    "\n",
    "Let's see an example of the process of defininf the parameters in our `LightningAdvancedConfig`. \n",
    "Tracing back different kwargs we can find: \n",
    "\n",
    "1. `task_train_kwargs`\n",
    "https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-flags - parameters that are passed to the `Lightning Trainer` object.\n",
    "\n",
    "2. `task_model_kwargs`\n",
    "https://github.com/CLARIN-PL/embeddings/blob/4292d110691c6c67695fefab74c927dbae9acff7/embeddings/model/lightning_module/lightning_module.py#L19 - parameters that are passed to the `Lightning module` object (we use `TextClassificationModule` which inherits from `HuggingFaceLightningModule` and `HuggingFaceLightningModule`).\n",
    "\n",
    "3. `datamodule_kwargs` - https://github.com/CLARIN-PL/embeddings/blob/main/embeddings/data/datamodule.py#L35 - parameters passed to the datamodule classes, currently `HuggingFaceDataModule` takes several arguments (such as max_seq_length, processing_batch_size or downsamples args) as an input\n",
    "\n",
    "4. `batch_encoding_kwargs` https://github.com/huggingface/transformers/blob/db7d6a80e82d66127b2a44b6e3382969fdc8b207/src/transformers/tokenization_utils_base.py#L2359 - parameters that are defined in `__call__` method\n",
    "\n",
    "5. `tokenizer_kwargs` https://github.com/huggingface/transformers/blob/074645e32acda6498f16203a8459bb597610f623/src/transformers/models/auto/tokenization_auto.py#L351\n",
    "This is a generic configuration class of the hugginface model's tokenizer, possible parameters depends on the tokenizer that is used. For example for bert uncased tokenizer these parameters are present here: https://huggingface.co/bert-base-uncased/blob/main/tokenizer_config.json\n",
    "\n",
    "6. `load_dataset_kwargs`\n",
    "https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html#DataLoader - dataloader kwargs\n",
    "\n",
    "7. `model_config_kwargs`\n",
    "https://github.com/huggingface/transformers/blob/074645e32acda6498f16203a8459bb597610f623/src/transformers/models/auto/configuration_auto.py#L515\n",
    "This is a generic configuration class of the hugginface model, possible parameters depends on the model that is used. For example for bert uncased these parameters are present here: https://huggingface.co/bert-base-uncased/blob/main/config.json\n",
    "\n",
    "8. `early_stopping_kwargs`  \n",
    "https://github.com/PyTorchLightning/pytorch-lightning/blob/5d2d9b09df5359226fea6ad2722592839ac0ebc4/pytorch_lightning/callbacks/early_stopping.py#L35 - params that are defined in `__init__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986cddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_name_or_path = \"allegro/herbert-base-cased\"\n",
    "dataset_name = \"clarin-pl/polemo2-official\"\n",
    "input_columns_name = \"text\"\n",
    "target_column_name = \"target\"\n",
    "root = RESULTS_PATH.joinpath(\"lightning_sequence_classification\")\n",
    "\n",
    "output_path = build_output_path(root, embedding_name_or_path, dataset_name)\n",
    "output_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f70e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_space = LightningBasicConfig(\n",
    "    learning_rate=0.01, max_epochs=1, max_seq_length=128, finetune_last_n_layers=0, accelerator=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556574a3-8aa5-4cbd-9a41-eae4e3376c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_config_space = LightningAdvancedConfig(\n",
    "    finetune_last_n_layers=0,\n",
    "    datamodule_kwargs={\n",
    "        \"max_seq_length\": 128,\n",
    "        \"downsample_train\": 0.01,\n",
    "        \"downsample_val\": 0.1,\n",
    "        \"downsample_test\": 0.1,\n",
    "    },\n",
    "    task_model_kwargs={\n",
    "        \"use_scheduler\": False,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"weight_decay\": 1e-3,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "    },\n",
    "    model_config_kwargs={\"classifier_dropout\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcecbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = LightningClassificationPipeline(\n",
    "    embedding_name_or_path=embedding_name_or_path,\n",
    "    dataset_name_or_path=dataset_name,\n",
    "    input_column_name=input_columns_name,\n",
    "    target_column_name=target_column_name,\n",
    "    output_path=output_path,\n",
    "    config_space=config_space,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e85282",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:embeddings]",
   "language": "python",
   "name": "conda-env-embeddings-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
